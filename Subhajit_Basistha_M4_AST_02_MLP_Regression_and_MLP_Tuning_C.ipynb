{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subho99/Computational-Data-Science/blob/main/Subhajit_Basistha_M4_AST_02_MLP_Regression_and_MLP_Tuning_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HklD4MlOgCTy"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 2: MLP Regression and MLP Tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doENGez0k03C"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbFv4sGHk7VW"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "- understand the concept of MLPs for regression\n",
        "- know the hyperparameters of neural network and their tuning \n",
        "- understand batch normalization using Keras\n",
        "- understand the concept of optimizers\n",
        "- understand the time-based learning rate method through an example\n",
        "- understand the different regularization methods to avoid the overfitting of neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-9AxpS_WMr9"
      },
      "source": [
        "### Introduction to Regression MLPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btCQw9drWTH_"
      },
      "source": [
        "First, MLPs can be used for regression tasks. If we want to predict a single value (e.g., the  price  of  a  house  given  many  of  its  features), then  we  just  need  a  single  output neuron:  its  output  is  the  predicted  value.  For  multivariate  regression  (i.e.,  to  predict multiple  values  at  once),  we  need  one  output  neuron  per  output  dimension.  For example, to locate the center of an object on an image, we need to predict 2D coordinates,  so  we  need  two  output  neurons.  If  we  also  want  to  place  a  bounding  box around the object, then we need two more numbers: the width and the height of the object. So we end up with 4 output neurons.\n",
        "\n",
        "In general, when building an MLP for regression, we do not want to use any activation  function  for  the  output  neurons,  so  they  are  free  to  output  any  range  of  values. However,  if  we  want  to  guarantee  that  the  output  will  always  be  positive,  then  we can use the ReLU activation function or the softplus activation function in the output layer.  Finally,  if  we  want  to  guarantee  that  the  predictions  will  fall  within  a  given range of values, then we can use the logistic function or the hyperbolic tangent and scale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for the hyperbolic tangent.\n",
        "\n",
        "The loss function to use during training is typically the mean squared error, but if we have  a  lot  of  outliers  in  the  training  set,  we  may  prefer  to  use  the  mean  absolute error  instead.  Alternatively,  we  can  use  the  Huber  loss,  which  is  a  combination  of both.\n",
        "\n",
        "**Note:** The Huber loss is quadratic when the error is smaller than a threshold $δ$ (typically 1), but linear when the error is larger than $δ$. This makes it less sensitive to outliers than the mean squared error, and\n",
        "it  is  often  more  precise  and  converges  faster  than  the  mean  absolute error.\n",
        "\n",
        "To know more about Multi Layer Perceptron (MLP), click [here](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L05%20Multilayer%20Perceptrons.pdf).\n",
        "\n",
        "**Implementation Using Keras**\n",
        "\n",
        "Keras is a high-level Deep Learning API that allows us to easily build train, evaluate and execute all sorts of neural networks. To know more about the documentation of Keras, click [here](https://keras.io/).\n",
        "\n",
        "**Implementation of MLP regression Using sklearn**\n",
        "\n",
        "The very popular machine learning library Scikit-Learn is also capable of basic deep learning modeling.\n",
        "\n",
        "Salient points of Multilayer Perceptron (MLP) in Scikit-learn:\n",
        "\n",
        "* There is no activation function in the output layer.\n",
        "* For regression scenarios, the square error is the loss function, and cross-entropy is the loss function for the classification\n",
        "* It can work with single as well as multiple target values regression.\n",
        "* Unlike other popular packages, likes Keras the implementation of MLP in Scikit doesn’t support GPU.\n",
        "\n",
        "To know more about Scikit-Learn MLP regressor, click [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html).\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx6BJLwYZBxw"
      },
      "source": [
        "### Typical MLP Regressor Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH2a3AvwZNlT"
      },
      "source": [
        "\n",
        "Hyperparameter             | Typical Value \n",
        "---------------------------|------------------\n",
        "# input neurons            | One per input feature (e.g., 28 x 28 = 784 for MNIST)\n",
        "# hidden layers            | Depends on the problem. Typically 1 to 5. \n",
        "# neurons per hidden layer | Depends on the problem. Typically 10 to 100.\n",
        "# output neurons           | 1 per prediction dimension\n",
        "Hidden activation          | ReLU\n",
        "Output activation          | None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\n",
        "Loss function              | MSE or MAE/Huber (if outliers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2236624\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"8240187807\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "300822fe-c298-4d2a-f4fb-4cd56082e592"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M4_AST_02_MLP_Regression_and_MLP_Tuning_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    \n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2236624&recordId=5249\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAu6v-CnA8zj"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SRN62EfayXM"
      },
      "source": [
        "# install livelossplot package to visualize epoch by epoch loss and accuracy curve\n",
        "!pip -qq install livelossplot"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw83tjrgdqNO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error             \n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler              # scaling functions from sklearn\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split                  # search on hyperparameters\n",
        "from functools import partial                                                             # partial functions\n",
        "import tensorflow as tf                                                                   # importing tensorflow library\n",
        "from tensorflow import keras                                                              # importing keras package\n",
        "from scipy.stats import reciprocal \n",
        "from sklearn.neural_network import MLPRegressor                                           # importing MLP regressor            \n",
        "from tensorflow.keras.optimizers import SGD                                               # stochastic Gradient Descent\n",
        "from tensorflow.keras.utils import to_categorical                                         # converting a class to categorical data type\n",
        "from keras.datasets import mnist                                                          # load mnist dataset\n",
        "import livelossplot                                                                       # visualize loss and accuracy \n",
        "from keras.models import Sequential                                                       # using keras importing Sequential Model\n",
        "from keras.layers import Activation, Dense, Input, Flatten, Dropout, BatchNormalization   # using keras importing layers                                    \n",
        "from keras.callbacks import EarlyStopping                                                 # to stop the training process"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLVYkKdrdqNV"
      },
      "source": [
        "### Building a Regression MLP "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5YpbtWMdqNX"
      },
      "source": [
        "Here, in this implementation, we will be using California housing problem and tackle it using a regression neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE9ZZHrjdqNY"
      },
      "source": [
        "#### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2NZQqhkdqNZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "905f5065-9d99-44f4-cd82-9ee042830d5f"
      },
      "source": [
        "# train dataset\n",
        "df_train = pd.read_csv('/content/sample_data/california_housing_train.csv')\n",
        "df_train.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "0    -114.31     34.19                15.0       5612.0          1283.0   \n",
              "1    -114.47     34.40                19.0       7650.0          1901.0   \n",
              "2    -114.56     33.69                17.0        720.0           174.0   \n",
              "3    -114.57     33.64                14.0       1501.0           337.0   \n",
              "4    -114.57     33.57                20.0       1454.0           326.0   \n",
              "\n",
              "   population  households  median_income  median_house_value  \n",
              "0      1015.0       472.0         1.4936             66900.0  \n",
              "1      1129.0       463.0         1.8200             80100.0  \n",
              "2       333.0       117.0         1.6509             85700.0  \n",
              "3       515.0       226.0         3.1917             73400.0  \n",
              "4       624.0       262.0         1.9250             65500.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-46dde616-1174-4142-adf1-a44b0a33e406\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-114.31</td>\n",
              "      <td>34.19</td>\n",
              "      <td>15.0</td>\n",
              "      <td>5612.0</td>\n",
              "      <td>1283.0</td>\n",
              "      <td>1015.0</td>\n",
              "      <td>472.0</td>\n",
              "      <td>1.4936</td>\n",
              "      <td>66900.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-114.47</td>\n",
              "      <td>34.40</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7650.0</td>\n",
              "      <td>1901.0</td>\n",
              "      <td>1129.0</td>\n",
              "      <td>463.0</td>\n",
              "      <td>1.8200</td>\n",
              "      <td>80100.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-114.56</td>\n",
              "      <td>33.69</td>\n",
              "      <td>17.0</td>\n",
              "      <td>720.0</td>\n",
              "      <td>174.0</td>\n",
              "      <td>333.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>1.6509</td>\n",
              "      <td>85700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.64</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1501.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>226.0</td>\n",
              "      <td>3.1917</td>\n",
              "      <td>73400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-114.57</td>\n",
              "      <td>33.57</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1454.0</td>\n",
              "      <td>326.0</td>\n",
              "      <td>624.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>1.9250</td>\n",
              "      <td>65500.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46dde616-1174-4142-adf1-a44b0a33e406')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-46dde616-1174-4142-adf1-a44b0a33e406 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-46dde616-1174-4142-adf1-a44b0a33e406');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aToiAvNdqNb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9cc0f964-f4db-4d18-a9a5-09b885ca5fd6"
      },
      "source": [
        "# test dataset\n",
        "df_test = pd.read_csv('/content/sample_data/california_housing_test.csv')\n",
        "df_test.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
              "0    -122.05     37.37                27.0       3885.0           661.0   \n",
              "1    -118.30     34.26                43.0       1510.0           310.0   \n",
              "2    -117.81     33.78                27.0       3589.0           507.0   \n",
              "3    -118.36     33.82                28.0         67.0            15.0   \n",
              "4    -119.67     36.33                19.0       1241.0           244.0   \n",
              "\n",
              "   population  households  median_income  median_house_value  \n",
              "0      1537.0       606.0         6.6085            344700.0  \n",
              "1       809.0       277.0         3.5990            176500.0  \n",
              "2      1484.0       495.0         5.7934            270500.0  \n",
              "3        49.0        11.0         6.1359            330000.0  \n",
              "4       850.0       237.0         2.9375             81700.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae9986aa-b9ff-4e2c-aa13-8086ebe9cabb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.05</td>\n",
              "      <td>37.37</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3885.0</td>\n",
              "      <td>661.0</td>\n",
              "      <td>1537.0</td>\n",
              "      <td>606.0</td>\n",
              "      <td>6.6085</td>\n",
              "      <td>344700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-118.30</td>\n",
              "      <td>34.26</td>\n",
              "      <td>43.0</td>\n",
              "      <td>1510.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>809.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>3.5990</td>\n",
              "      <td>176500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-117.81</td>\n",
              "      <td>33.78</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3589.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>1484.0</td>\n",
              "      <td>495.0</td>\n",
              "      <td>5.7934</td>\n",
              "      <td>270500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-118.36</td>\n",
              "      <td>33.82</td>\n",
              "      <td>28.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.1359</td>\n",
              "      <td>330000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-119.67</td>\n",
              "      <td>36.33</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>2.9375</td>\n",
              "      <td>81700.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae9986aa-b9ff-4e2c-aa13-8086ebe9cabb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ae9986aa-b9ff-4e2c-aa13-8086ebe9cabb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ae9986aa-b9ff-4e2c-aa13-8086ebe9cabb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyyGRVgVdqNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e29ff70-397f-4634-c8ca-e52efd0a31ac"
      },
      "source": [
        "# printing train dataset information\n",
        "df_train.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17000 entries, 0 to 16999\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           17000 non-null  float64\n",
            " 1   latitude            17000 non-null  float64\n",
            " 2   housing_median_age  17000 non-null  float64\n",
            " 3   total_rooms         17000 non-null  float64\n",
            " 4   total_bedrooms      17000 non-null  float64\n",
            " 5   population          17000 non-null  float64\n",
            " 6   households          17000 non-null  float64\n",
            " 7   median_income       17000 non-null  float64\n",
            " 8   median_house_value  17000 non-null  float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 1.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV-lUMrIdqNd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c100cd-344b-49a9-9d06-33273cb68c01"
      },
      "source": [
        "# printing test dataset information\n",
        "df_test.info()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           3000 non-null   float64\n",
            " 1   latitude            3000 non-null   float64\n",
            " 2   housing_median_age  3000 non-null   float64\n",
            " 3   total_rooms         3000 non-null   float64\n",
            " 4   total_bedrooms      3000 non-null   float64\n",
            " 5   population          3000 non-null   float64\n",
            " 6   households          3000 non-null   float64\n",
            " 7   median_income       3000 non-null   float64\n",
            " 8   median_house_value  3000 non-null   float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 211.1 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Vwj6P4dqNe"
      },
      "source": [
        "#### Train and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wISS_1e2dqNf"
      },
      "source": [
        "X_train = df_train.drop('median_house_value',axis=1)\n",
        "y_train = df_train['median_house_value']\n",
        "X_test = df_test.drop('median_house_value',axis=1)\n",
        "y_test = df_test['median_house_value']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Zhd27edqNg"
      },
      "source": [
        "#### Scaling Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLnjSOd6dqNh"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train= scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDSuABdjdqNi"
      },
      "source": [
        "There are two ways to implement MLP regressor one is using keras and the other way is using Scikit-Learn. In this section, we will discuss both two ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fnY7eWpdqNj"
      },
      "source": [
        "#### 1. Using Keras API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzTaNnmudqNk"
      },
      "source": [
        "Building, training, evaluating, and using a regression MLP using the Sequential API to make  predictions  is  quite  similar  to  what  we  did  for classification.  The  main  differences  are  the  fact  that  the  output  layer  has  a  single  neuron  (since  we  only  want  to predict  a  single  value)  and  uses  no  activation  function,  and  the  loss  function  is  the mean squared error. Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdUBydS0dqNl"
      },
      "source": [
        "# create a model with two layers\n",
        "model = Sequential([\n",
        "                    Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "                    Dense(1)\n",
        "                    ])\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glssNoESfQC_"
      },
      "source": [
        "Keras supports the early stopping of training via a callback called EarlyStopping.\n",
        "\n",
        "This callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process.\n",
        "\n",
        "The EarlyStopping callback is configured when instantiated via arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7MdewIYdqNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc744cb-9253-431f-91d4-dd8503713e80"
      },
      "source": [
        "# defining early stop \n",
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "# fitting the model\n",
        "model.fit(x=X_train,y=y_train.values,\n",
        "          validation_data=(X_test,y_test.values),\n",
        "          batch_size=128,epochs=400, callbacks=[early_stop])\n",
        "          "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "133/133 [==============================] - 1s 3ms/step - loss: 56424697856.0000 - val_loss: 55163518976.0000\n",
            "Epoch 2/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 56423145472.0000 - val_loss: 55161327616.0000\n",
            "Epoch 3/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 56420204544.0000 - val_loss: 55157665792.0000\n",
            "Epoch 4/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56415825920.0000 - val_loss: 55152648192.0000\n",
            "Epoch 5/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 56410013696.0000 - val_loss: 55146086400.0000\n",
            "Epoch 6/400\n",
            "133/133 [==============================] - 1s 7ms/step - loss: 56402567168.0000 - val_loss: 55138009088.0000\n",
            "Epoch 7/400\n",
            "133/133 [==============================] - 1s 9ms/step - loss: 56393736192.0000 - val_loss: 55128551424.0000\n",
            "Epoch 8/400\n",
            "133/133 [==============================] - 1s 8ms/step - loss: 56383569920.0000 - val_loss: 55117848576.0000\n",
            "Epoch 9/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 56372154368.0000 - val_loss: 55105961984.0000\n",
            "Epoch 10/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 56359555072.0000 - val_loss: 55092916224.0000\n",
            "Epoch 11/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56345870336.0000 - val_loss: 55078821888.0000\n",
            "Epoch 12/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56331071488.0000 - val_loss: 55063650304.0000\n",
            "Epoch 13/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 56315256832.0000 - val_loss: 55047487488.0000\n",
            "Epoch 14/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56298434560.0000 - val_loss: 55030337536.0000\n",
            "Epoch 15/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 56280608768.0000 - val_loss: 55012241408.0000\n",
            "Epoch 16/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56261865472.0000 - val_loss: 54993190912.0000\n",
            "Epoch 17/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 56242208768.0000 - val_loss: 54973292544.0000\n",
            "Epoch 18/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56221667328.0000 - val_loss: 54952501248.0000\n",
            "Epoch 19/400\n",
            "133/133 [==============================] - 0s 4ms/step - loss: 56200237056.0000 - val_loss: 54930903040.0000\n",
            "Epoch 20/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 56177991680.0000 - val_loss: 54908444672.0000\n",
            "Epoch 21/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 56154923008.0000 - val_loss: 54885203968.0000\n",
            "Epoch 22/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56131002368.0000 - val_loss: 54861115392.0000\n",
            "Epoch 23/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56106315776.0000 - val_loss: 54836314112.0000\n",
            "Epoch 24/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 56080846848.0000 - val_loss: 54810685440.0000\n",
            "Epoch 25/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 56054628352.0000 - val_loss: 54784376832.0000\n",
            "Epoch 26/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 56027660288.0000 - val_loss: 54757261312.0000\n",
            "Epoch 27/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 55999930368.0000 - val_loss: 54729494528.0000\n",
            "Epoch 28/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 55971475456.0000 - val_loss: 54700929024.0000\n",
            "Epoch 29/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 55942291456.0000 - val_loss: 54671728640.0000\n",
            "Epoch 30/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 55912415232.0000 - val_loss: 54641766400.0000\n",
            "Epoch 31/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 55881818112.0000 - val_loss: 54611140608.0000\n",
            "Epoch 32/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55850541056.0000 - val_loss: 54579773440.0000\n",
            "Epoch 33/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55818575872.0000 - val_loss: 54547841024.0000\n",
            "Epoch 34/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55785914368.0000 - val_loss: 54515118080.0000\n",
            "Epoch 35/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55752568832.0000 - val_loss: 54481817600.0000\n",
            "Epoch 36/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55718547456.0000 - val_loss: 54447759360.0000\n",
            "Epoch 37/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55683854336.0000 - val_loss: 54413123584.0000\n",
            "Epoch 38/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55648526336.0000 - val_loss: 54377746432.0000\n",
            "Epoch 39/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55612506112.0000 - val_loss: 54341828608.0000\n",
            "Epoch 40/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55575814144.0000 - val_loss: 54305218560.0000\n",
            "Epoch 41/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55538499584.0000 - val_loss: 54267924480.0000\n",
            "Epoch 42/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55500533760.0000 - val_loss: 54230024192.0000\n",
            "Epoch 43/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55461933056.0000 - val_loss: 54191505408.0000\n",
            "Epoch 44/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55422693376.0000 - val_loss: 54152302592.0000\n",
            "Epoch 45/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55382806528.0000 - val_loss: 54112509952.0000\n",
            "Epoch 46/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55342292992.0000 - val_loss: 54072090624.0000\n",
            "Epoch 47/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55301128192.0000 - val_loss: 54031052800.0000\n",
            "Epoch 48/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55259373568.0000 - val_loss: 53989384192.0000\n",
            "Epoch 49/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55216967680.0000 - val_loss: 53947072512.0000\n",
            "Epoch 50/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55173943296.0000 - val_loss: 53904166912.0000\n",
            "Epoch 51/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 55130296320.0000 - val_loss: 53860614144.0000\n",
            "Epoch 52/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 55086026752.0000 - val_loss: 53816516608.0000\n",
            "Epoch 53/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 55041122304.0000 - val_loss: 53771907072.0000\n",
            "Epoch 54/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54995628032.0000 - val_loss: 53726457856.0000\n",
            "Epoch 55/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54949507072.0000 - val_loss: 53680476160.0000\n",
            "Epoch 56/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54902759424.0000 - val_loss: 53633896448.0000\n",
            "Epoch 57/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54855405568.0000 - val_loss: 53586763776.0000\n",
            "Epoch 58/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54807478272.0000 - val_loss: 53538983936.0000\n",
            "Epoch 59/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54758928384.0000 - val_loss: 53490647040.0000\n",
            "Epoch 60/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54709788672.0000 - val_loss: 53441646592.0000\n",
            "Epoch 61/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54660022272.0000 - val_loss: 53392154624.0000\n",
            "Epoch 62/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54609698816.0000 - val_loss: 53342011392.0000\n",
            "Epoch 63/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54558797824.0000 - val_loss: 53291261952.0000\n",
            "Epoch 64/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54507257856.0000 - val_loss: 53240041472.0000\n",
            "Epoch 65/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54455136256.0000 - val_loss: 53188169728.0000\n",
            "Epoch 66/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54402404352.0000 - val_loss: 53135667200.0000\n",
            "Epoch 67/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54349111296.0000 - val_loss: 53082562560.0000\n",
            "Epoch 68/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54295224320.0000 - val_loss: 53028966400.0000\n",
            "Epoch 69/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54240763904.0000 - val_loss: 52974735360.0000\n",
            "Epoch 70/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 54185725952.0000 - val_loss: 52919898112.0000\n",
            "Epoch 71/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54130069504.0000 - val_loss: 52864606208.0000\n",
            "Epoch 72/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54073839616.0000 - val_loss: 52808675328.0000\n",
            "Epoch 73/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 54017064960.0000 - val_loss: 52752068608.0000\n",
            "Epoch 74/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53959692288.0000 - val_loss: 52695179264.0000\n",
            "Epoch 75/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53901762560.0000 - val_loss: 52637564928.0000\n",
            "Epoch 76/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53843247104.0000 - val_loss: 52579274752.0000\n",
            "Epoch 77/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53784158208.0000 - val_loss: 52520497152.0000\n",
            "Epoch 78/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53724532736.0000 - val_loss: 52461117440.0000\n",
            "Epoch 79/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53664321536.0000 - val_loss: 52401307648.0000\n",
            "Epoch 80/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53603540992.0000 - val_loss: 52340838400.0000\n",
            "Epoch 81/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53542187008.0000 - val_loss: 52279906304.0000\n",
            "Epoch 82/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53480321024.0000 - val_loss: 52218232832.0000\n",
            "Epoch 83/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53417844736.0000 - val_loss: 52156211200.0000\n",
            "Epoch 84/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53354860544.0000 - val_loss: 52093530112.0000\n",
            "Epoch 85/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53291298816.0000 - val_loss: 52030402560.0000\n",
            "Epoch 86/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53227188224.0000 - val_loss: 51966627840.0000\n",
            "Epoch 87/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53162524672.0000 - val_loss: 51902373888.0000\n",
            "Epoch 88/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53097316352.0000 - val_loss: 51837583360.0000\n",
            "Epoch 89/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 53031530496.0000 - val_loss: 51772239872.0000\n",
            "Epoch 90/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52965203968.0000 - val_loss: 51706232832.0000\n",
            "Epoch 91/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52898373632.0000 - val_loss: 51639824384.0000\n",
            "Epoch 92/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52830973952.0000 - val_loss: 51572903936.0000\n",
            "Epoch 93/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52763013120.0000 - val_loss: 51505422336.0000\n",
            "Epoch 94/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52694552576.0000 - val_loss: 51437338624.0000\n",
            "Epoch 95/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52625567744.0000 - val_loss: 51368755200.0000\n",
            "Epoch 96/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 52556005376.0000 - val_loss: 51299614720.0000\n",
            "Epoch 97/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52485939200.0000 - val_loss: 51230040064.0000\n",
            "Epoch 98/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52415328256.0000 - val_loss: 51159957504.0000\n",
            "Epoch 99/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 52344205312.0000 - val_loss: 51089174528.0000\n",
            "Epoch 100/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52272549888.0000 - val_loss: 51017994240.0000\n",
            "Epoch 101/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 52200386560.0000 - val_loss: 50946265088.0000\n",
            "Epoch 102/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 52127690752.0000 - val_loss: 50874089472.0000\n",
            "Epoch 103/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 52054450176.0000 - val_loss: 50801442816.0000\n",
            "Epoch 104/400\n",
            "133/133 [==============================] - 0s 4ms/step - loss: 51980693504.0000 - val_loss: 50728091648.0000\n",
            "Epoch 105/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 51906416640.0000 - val_loss: 50654392320.0000\n",
            "Epoch 106/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 51831660544.0000 - val_loss: 50580140032.0000\n",
            "Epoch 107/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 51756363776.0000 - val_loss: 50505371648.0000\n",
            "Epoch 108/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 51680571392.0000 - val_loss: 50430148608.0000\n",
            "Epoch 109/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51604303872.0000 - val_loss: 50354348032.0000\n",
            "Epoch 110/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51527520256.0000 - val_loss: 50278068224.0000\n",
            "Epoch 111/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51450216448.0000 - val_loss: 50201407488.0000\n",
            "Epoch 112/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51372462080.0000 - val_loss: 50124132352.0000\n",
            "Epoch 113/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51294126080.0000 - val_loss: 50046455808.0000\n",
            "Epoch 114/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51215343616.0000 - val_loss: 49968168960.0000\n",
            "Epoch 115/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51136053248.0000 - val_loss: 49889398784.0000\n",
            "Epoch 116/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 51056267264.0000 - val_loss: 49810309120.0000\n",
            "Epoch 117/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50976018432.0000 - val_loss: 49730519040.0000\n",
            "Epoch 118/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50895253504.0000 - val_loss: 49650364416.0000\n",
            "Epoch 119/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50813997056.0000 - val_loss: 49569742848.0000\n",
            "Epoch 120/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50732269568.0000 - val_loss: 49488605184.0000\n",
            "Epoch 121/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50650046464.0000 - val_loss: 49406988288.0000\n",
            "Epoch 122/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50567327744.0000 - val_loss: 49324953600.0000\n",
            "Epoch 123/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50484158464.0000 - val_loss: 49242382336.0000\n",
            "Epoch 124/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50400505856.0000 - val_loss: 49159417856.0000\n",
            "Epoch 125/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50316390400.0000 - val_loss: 49075847168.0000\n",
            "Epoch 126/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50231795712.0000 - val_loss: 48991862784.0000\n",
            "Epoch 127/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50146734080.0000 - val_loss: 48907542528.0000\n",
            "Epoch 128/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 50061213696.0000 - val_loss: 48822652928.0000\n",
            "Epoch 129/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49975250944.0000 - val_loss: 48737333248.0000\n",
            "Epoch 130/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49888808960.0000 - val_loss: 48651472896.0000\n",
            "Epoch 131/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49801912320.0000 - val_loss: 48565264384.0000\n",
            "Epoch 132/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49714540544.0000 - val_loss: 48478642176.0000\n",
            "Epoch 133/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49626705920.0000 - val_loss: 48391581696.0000\n",
            "Epoch 134/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49538428928.0000 - val_loss: 48303919104.0000\n",
            "Epoch 135/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49449701376.0000 - val_loss: 48215924736.0000\n",
            "Epoch 136/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49360535552.0000 - val_loss: 48127434752.0000\n",
            "Epoch 137/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49270894592.0000 - val_loss: 48038637568.0000\n",
            "Epoch 138/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49180844032.0000 - val_loss: 47949127680.0000\n",
            "Epoch 139/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 49090330624.0000 - val_loss: 47859363840.0000\n",
            "Epoch 140/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 48999399424.0000 - val_loss: 47769346048.0000\n",
            "Epoch 141/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 48908029952.0000 - val_loss: 47678480384.0000\n",
            "Epoch 142/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 48816197632.0000 - val_loss: 47587622912.0000\n",
            "Epoch 143/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 48723922944.0000 - val_loss: 47496101888.0000\n",
            "Epoch 144/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 48631238656.0000 - val_loss: 47404011520.0000\n",
            "Epoch 145/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 48538120192.0000 - val_loss: 47311671296.0000\n",
            "Epoch 146/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 48444624896.0000 - val_loss: 47218913280.0000\n",
            "Epoch 147/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 48350707712.0000 - val_loss: 47125778432.0000\n",
            "Epoch 148/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48256339968.0000 - val_loss: 47032320000.0000\n",
            "Epoch 149/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48161579008.0000 - val_loss: 46938243072.0000\n",
            "Epoch 150/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 48066342912.0000 - val_loss: 46843899904.0000\n",
            "Epoch 151/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 47970746368.0000 - val_loss: 46749057024.0000\n",
            "Epoch 152/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 47874740224.0000 - val_loss: 46653894656.0000\n",
            "Epoch 153/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47778336768.0000 - val_loss: 46558126080.0000\n",
            "Epoch 154/400\n",
            "133/133 [==============================] - 1s 6ms/step - loss: 47681454080.0000 - val_loss: 46462263296.0000\n",
            "Epoch 155/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47584239616.0000 - val_loss: 46365790208.0000\n",
            "Epoch 156/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 47486607360.0000 - val_loss: 46268964864.0000\n",
            "Epoch 157/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 47388545024.0000 - val_loss: 46171852800.0000\n",
            "Epoch 158/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47290105856.0000 - val_loss: 46074195968.0000\n",
            "Epoch 159/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47191248896.0000 - val_loss: 45976338432.0000\n",
            "Epoch 160/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 47092019200.0000 - val_loss: 45877846016.0000\n",
            "Epoch 161/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46992404480.0000 - val_loss: 45779001344.0000\n",
            "Epoch 162/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46892392448.0000 - val_loss: 45679964160.0000\n",
            "Epoch 163/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46791995392.0000 - val_loss: 45580484608.0000\n",
            "Epoch 164/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46691229696.0000 - val_loss: 45480550400.0000\n",
            "Epoch 165/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46590058496.0000 - val_loss: 45380214784.0000\n",
            "Epoch 166/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46488502272.0000 - val_loss: 45279608832.0000\n",
            "Epoch 167/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46386634752.0000 - val_loss: 45178609664.0000\n",
            "Epoch 168/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46284369920.0000 - val_loss: 45077291008.0000\n",
            "Epoch 169/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46181748736.0000 - val_loss: 44975718400.0000\n",
            "Epoch 170/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 46078763008.0000 - val_loss: 44873543680.0000\n",
            "Epoch 171/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45975379968.0000 - val_loss: 44771090432.0000\n",
            "Epoch 172/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45871644672.0000 - val_loss: 44668293120.0000\n",
            "Epoch 173/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45767598080.0000 - val_loss: 44565110784.0000\n",
            "Epoch 174/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 45663170560.0000 - val_loss: 44461637632.0000\n",
            "Epoch 175/400\n",
            "133/133 [==============================] - 0s 4ms/step - loss: 45558382592.0000 - val_loss: 44357951488.0000\n",
            "Epoch 176/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 45453197312.0000 - val_loss: 44253810688.0000\n",
            "Epoch 177/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 45347729408.0000 - val_loss: 44149002240.0000\n",
            "Epoch 178/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 45241937920.0000 - val_loss: 44044230656.0000\n",
            "Epoch 179/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 45135790080.0000 - val_loss: 43939074048.0000\n",
            "Epoch 180/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 45029314560.0000 - val_loss: 43833667584.0000\n",
            "Epoch 181/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 44922527744.0000 - val_loss: 43727835136.0000\n",
            "Epoch 182/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44815380480.0000 - val_loss: 43621601280.0000\n",
            "Epoch 183/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44707901440.0000 - val_loss: 43515359232.0000\n",
            "Epoch 184/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44600086528.0000 - val_loss: 43408326656.0000\n",
            "Epoch 185/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44491919360.0000 - val_loss: 43301310464.0000\n",
            "Epoch 186/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44383469568.0000 - val_loss: 43193683968.0000\n",
            "Epoch 187/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44274696192.0000 - val_loss: 43086028800.0000\n",
            "Epoch 188/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44165619712.0000 - val_loss: 42978045952.0000\n",
            "Epoch 189/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 44056240128.0000 - val_loss: 42869469184.0000\n",
            "Epoch 190/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43946528768.0000 - val_loss: 42760794112.0000\n",
            "Epoch 191/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43836518400.0000 - val_loss: 42652033024.0000\n",
            "Epoch 192/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43726213120.0000 - val_loss: 42542714880.0000\n",
            "Epoch 193/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43615571968.0000 - val_loss: 42433310720.0000\n",
            "Epoch 194/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43504603136.0000 - val_loss: 42323288064.0000\n",
            "Epoch 195/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43393339392.0000 - val_loss: 42213138432.0000\n",
            "Epoch 196/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43281817600.0000 - val_loss: 42102599680.0000\n",
            "Epoch 197/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43170025472.0000 - val_loss: 41991929856.0000\n",
            "Epoch 198/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 43057909760.0000 - val_loss: 41880793088.0000\n",
            "Epoch 199/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42945495040.0000 - val_loss: 41769725952.0000\n",
            "Epoch 200/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42832826368.0000 - val_loss: 41658077184.0000\n",
            "Epoch 201/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42719870976.0000 - val_loss: 41546108928.0000\n",
            "Epoch 202/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42606649344.0000 - val_loss: 41434038272.0000\n",
            "Epoch 203/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42493153280.0000 - val_loss: 41321644032.0000\n",
            "Epoch 204/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42379407360.0000 - val_loss: 41209102336.0000\n",
            "Epoch 205/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42265399296.0000 - val_loss: 41096196096.0000\n",
            "Epoch 206/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42151108608.0000 - val_loss: 40983023616.0000\n",
            "Epoch 207/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 42036592640.0000 - val_loss: 40869502976.0000\n",
            "Epoch 208/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41921789952.0000 - val_loss: 40755986432.0000\n",
            "Epoch 209/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41806749696.0000 - val_loss: 40642105344.0000\n",
            "Epoch 210/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41691455488.0000 - val_loss: 40527872000.0000\n",
            "Epoch 211/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41575895040.0000 - val_loss: 40413523968.0000\n",
            "Epoch 212/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41460047872.0000 - val_loss: 40298971136.0000\n",
            "Epoch 213/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 41343987712.0000 - val_loss: 40184061952.0000\n",
            "Epoch 214/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 41227718656.0000 - val_loss: 40068837376.0000\n",
            "Epoch 215/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 41111207936.0000 - val_loss: 39953473536.0000\n",
            "Epoch 216/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 40994451456.0000 - val_loss: 39837954048.0000\n",
            "Epoch 217/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 40877494272.0000 - val_loss: 39722160128.0000\n",
            "Epoch 218/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 40760262656.0000 - val_loss: 39606226944.0000\n",
            "Epoch 219/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 40642826240.0000 - val_loss: 39489957888.0000\n",
            "Epoch 220/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 40525201408.0000 - val_loss: 39373520896.0000\n",
            "Epoch 221/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 40407379968.0000 - val_loss: 39256739840.0000\n",
            "Epoch 222/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40289251328.0000 - val_loss: 39140216832.0000\n",
            "Epoch 223/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40170950656.0000 - val_loss: 39022907392.0000\n",
            "Epoch 224/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 40052404224.0000 - val_loss: 38905815040.0000\n",
            "Epoch 225/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39933726720.0000 - val_loss: 38788145152.0000\n",
            "Epoch 226/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 39814795264.0000 - val_loss: 38670487552.0000\n",
            "Epoch 227/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39695712256.0000 - val_loss: 38552723456.0000\n",
            "Epoch 228/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39576399872.0000 - val_loss: 38434574336.0000\n",
            "Epoch 229/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39456878592.0000 - val_loss: 38316453888.0000\n",
            "Epoch 230/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39337238528.0000 - val_loss: 38198009856.0000\n",
            "Epoch 231/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39217348608.0000 - val_loss: 38079545344.0000\n",
            "Epoch 232/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 39097307136.0000 - val_loss: 37960630272.0000\n",
            "Epoch 233/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 38977085440.0000 - val_loss: 37841649664.0000\n",
            "Epoch 234/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38856699904.0000 - val_loss: 37722574848.0000\n",
            "Epoch 235/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 38736158720.0000 - val_loss: 37603119104.0000\n",
            "Epoch 236/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38615392256.0000 - val_loss: 37483933696.0000\n",
            "Epoch 237/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38494502912.0000 - val_loss: 37364441088.0000\n",
            "Epoch 238/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38373478400.0000 - val_loss: 37244497920.0000\n",
            "Epoch 239/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38252253184.0000 - val_loss: 37124620288.0000\n",
            "Epoch 240/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38130835456.0000 - val_loss: 37004632064.0000\n",
            "Epoch 241/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 38009286656.0000 - val_loss: 36884131840.0000\n",
            "Epoch 242/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37887598592.0000 - val_loss: 36764090368.0000\n",
            "Epoch 243/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37765836800.0000 - val_loss: 36643377152.0000\n",
            "Epoch 244/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37643841536.0000 - val_loss: 36522946560.0000\n",
            "Epoch 245/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37521780736.0000 - val_loss: 36401999872.0000\n",
            "Epoch 246/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37399580672.0000 - val_loss: 36281221120.0000\n",
            "Epoch 247/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37277270016.0000 - val_loss: 36160208896.0000\n",
            "Epoch 248/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37154799616.0000 - val_loss: 36039028736.0000\n",
            "Epoch 249/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 37032128512.0000 - val_loss: 35918139392.0000\n",
            "Epoch 250/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36909420544.0000 - val_loss: 35796561920.0000\n",
            "Epoch 251/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36786565120.0000 - val_loss: 35675156480.0000\n",
            "Epoch 252/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36663586816.0000 - val_loss: 35553468416.0000\n",
            "Epoch 253/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 36540485632.0000 - val_loss: 35431800832.0000\n",
            "Epoch 254/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 36417302528.0000 - val_loss: 35310030848.0000\n",
            "Epoch 255/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 36294053888.0000 - val_loss: 35188170752.0000\n",
            "Epoch 256/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 36170674176.0000 - val_loss: 35066269696.0000\n",
            "Epoch 257/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 36047187968.0000 - val_loss: 34944118784.0000\n",
            "Epoch 258/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 35923607552.0000 - val_loss: 34821849088.0000\n",
            "Epoch 259/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 35799904256.0000 - val_loss: 34699702272.0000\n",
            "Epoch 260/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35676192768.0000 - val_loss: 34577117184.0000\n",
            "Epoch 261/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 35552350208.0000 - val_loss: 34454704128.0000\n",
            "Epoch 262/400\n",
            "133/133 [==============================] - 1s 4ms/step - loss: 35428474880.0000 - val_loss: 34332397568.0000\n",
            "Epoch 263/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35304542208.0000 - val_loss: 34209900544.0000\n",
            "Epoch 264/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 35180453888.0000 - val_loss: 34087528448.0000\n",
            "Epoch 265/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 35056308224.0000 - val_loss: 33964660736.0000\n",
            "Epoch 266/400\n",
            "133/133 [==============================] - 1s 5ms/step - loss: 34932125696.0000 - val_loss: 33841805312.0000\n",
            "Epoch 267/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34807885824.0000 - val_loss: 33719156736.0000\n",
            "Epoch 268/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34683625472.0000 - val_loss: 33596227584.0000\n",
            "Epoch 269/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 34559246336.0000 - val_loss: 33473331200.0000\n",
            "Epoch 270/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 34434850816.0000 - val_loss: 33350402048.0000\n",
            "Epoch 271/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34310440960.0000 - val_loss: 33227384832.0000\n",
            "Epoch 272/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34185947136.0000 - val_loss: 33104322560.0000\n",
            "Epoch 273/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 34061375488.0000 - val_loss: 32981547008.0000\n",
            "Epoch 274/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33936805888.0000 - val_loss: 32858458112.0000\n",
            "Epoch 275/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33812219904.0000 - val_loss: 32735186944.0000\n",
            "Epoch 276/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 33687584768.0000 - val_loss: 32612128768.0000\n",
            "Epoch 277/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33562982400.0000 - val_loss: 32488933376.0000\n",
            "Epoch 278/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33438332928.0000 - val_loss: 32365821952.0000\n",
            "Epoch 279/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 33313630208.0000 - val_loss: 32242845696.0000\n",
            "Epoch 280/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33188956160.0000 - val_loss: 32119400448.0000\n",
            "Epoch 281/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 33064249344.0000 - val_loss: 31996280832.0000\n",
            "Epoch 282/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32939558912.0000 - val_loss: 31873081344.0000\n",
            "Epoch 283/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32814905344.0000 - val_loss: 31749892096.0000\n",
            "Epoch 284/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32690219008.0000 - val_loss: 31626749952.0000\n",
            "Epoch 285/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32565508096.0000 - val_loss: 31503923200.0000\n",
            "Epoch 286/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32440844288.0000 - val_loss: 31380502528.0000\n",
            "Epoch 287/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 32316160000.0000 - val_loss: 31257647104.0000\n",
            "Epoch 288/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 32191539200.0000 - val_loss: 31134152704.0000\n",
            "Epoch 289/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 32066910208.0000 - val_loss: 31011170304.0000\n",
            "Epoch 290/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 31942281216.0000 - val_loss: 30888562688.0000\n",
            "Epoch 291/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 31817754624.0000 - val_loss: 30765258752.0000\n",
            "Epoch 292/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 31693219840.0000 - val_loss: 30642419712.0000\n",
            "Epoch 293/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 31568756736.0000 - val_loss: 30519257088.0000\n",
            "Epoch 294/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 31444310016.0000 - val_loss: 30396440576.0000\n",
            "Epoch 295/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 31319930880.0000 - val_loss: 30273714176.0000\n",
            "Epoch 296/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 31195570176.0000 - val_loss: 30151157760.0000\n",
            "Epoch 297/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 31071262720.0000 - val_loss: 30028378112.0000\n",
            "Epoch 298/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30947031040.0000 - val_loss: 29905543168.0000\n",
            "Epoch 299/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30822754304.0000 - val_loss: 29783336960.0000\n",
            "Epoch 300/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30698610688.0000 - val_loss: 29660387328.0000\n",
            "Epoch 301/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30574546944.0000 - val_loss: 29537972224.0000\n",
            "Epoch 302/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30450548736.0000 - val_loss: 29415677952.0000\n",
            "Epoch 303/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30326640640.0000 - val_loss: 29293101056.0000\n",
            "Epoch 304/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30202804224.0000 - val_loss: 29171126272.0000\n",
            "Epoch 305/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 30079070208.0000 - val_loss: 29049081856.0000\n",
            "Epoch 306/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29955414016.0000 - val_loss: 28926984192.0000\n",
            "Epoch 307/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29831782400.0000 - val_loss: 28804796416.0000\n",
            "Epoch 308/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29708300288.0000 - val_loss: 28683214848.0000\n",
            "Epoch 309/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29584941056.0000 - val_loss: 28561451008.0000\n",
            "Epoch 310/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29461696512.0000 - val_loss: 28439638016.0000\n",
            "Epoch 311/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29338527744.0000 - val_loss: 28318318592.0000\n",
            "Epoch 312/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 29215531008.0000 - val_loss: 28197050368.0000\n",
            "Epoch 313/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 29092628480.0000 - val_loss: 28075585536.0000\n",
            "Epoch 314/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28969789440.0000 - val_loss: 27954515968.0000\n",
            "Epoch 315/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28847038464.0000 - val_loss: 27833636864.0000\n",
            "Epoch 316/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28724496384.0000 - val_loss: 27712561152.0000\n",
            "Epoch 317/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28602085376.0000 - val_loss: 27591768064.0000\n",
            "Epoch 318/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28479795200.0000 - val_loss: 27471169536.0000\n",
            "Epoch 319/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28357718016.0000 - val_loss: 27350671360.0000\n",
            "Epoch 320/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 28235769856.0000 - val_loss: 27230158848.0000\n",
            "Epoch 321/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 28113948672.0000 - val_loss: 27110420480.0000\n",
            "Epoch 322/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 27992295424.0000 - val_loss: 26990180352.0000\n",
            "Epoch 323/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27870771200.0000 - val_loss: 26870423552.0000\n",
            "Epoch 324/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 27749371904.0000 - val_loss: 26751201280.0000\n",
            "Epoch 325/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 27628191744.0000 - val_loss: 26631464960.0000\n",
            "Epoch 326/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 27507187712.0000 - val_loss: 26512080896.0000\n",
            "Epoch 327/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 27386363904.0000 - val_loss: 26393092096.0000\n",
            "Epoch 328/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 27265658880.0000 - val_loss: 26273964032.0000\n",
            "Epoch 329/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 27145127936.0000 - val_loss: 26155253760.0000\n",
            "Epoch 330/400\n",
            "133/133 [==============================] - 0s 4ms/step - loss: 27024824320.0000 - val_loss: 26036494336.0000\n",
            "Epoch 331/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 26904705024.0000 - val_loss: 25918134272.0000\n",
            "Epoch 332/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 26784747520.0000 - val_loss: 25800173568.0000\n",
            "Epoch 333/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 26665000960.0000 - val_loss: 25682192384.0000\n",
            "Epoch 334/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26545420288.0000 - val_loss: 25564297216.0000\n",
            "Epoch 335/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26426105856.0000 - val_loss: 25446430720.0000\n",
            "Epoch 336/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26306961408.0000 - val_loss: 25329319936.0000\n",
            "Epoch 337/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 26188060672.0000 - val_loss: 25212196864.0000\n",
            "Epoch 338/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 26069350400.0000 - val_loss: 25095372800.0000\n",
            "Epoch 339/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25950820352.0000 - val_loss: 24978575360.0000\n",
            "Epoch 340/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25832538112.0000 - val_loss: 24861796352.0000\n",
            "Epoch 341/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 25714575360.0000 - val_loss: 24745580544.0000\n",
            "Epoch 342/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25596878848.0000 - val_loss: 24629610496.0000\n",
            "Epoch 343/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25479432192.0000 - val_loss: 24513849344.0000\n",
            "Epoch 344/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25362245632.0000 - val_loss: 24398106624.0000\n",
            "Epoch 345/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25245261824.0000 - val_loss: 24283357184.0000\n",
            "Epoch 346/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25128583168.0000 - val_loss: 24168308736.0000\n",
            "Epoch 347/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 25012133888.0000 - val_loss: 24053534720.0000\n",
            "Epoch 348/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24895819776.0000 - val_loss: 23939563520.0000\n",
            "Epoch 349/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24779870208.0000 - val_loss: 23824850944.0000\n",
            "Epoch 350/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24664137728.0000 - val_loss: 23711174656.0000\n",
            "Epoch 351/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 24548712448.0000 - val_loss: 23597662208.0000\n",
            "Epoch 352/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24433545216.0000 - val_loss: 23484266496.0000\n",
            "Epoch 353/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24318709760.0000 - val_loss: 23370983424.0000\n",
            "Epoch 354/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 24204146688.0000 - val_loss: 23258294272.0000\n",
            "Epoch 355/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 24089905152.0000 - val_loss: 23145920512.0000\n",
            "Epoch 356/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23975954432.0000 - val_loss: 23033794560.0000\n",
            "Epoch 357/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23862286336.0000 - val_loss: 22921803776.0000\n",
            "Epoch 358/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23748884480.0000 - val_loss: 22810750976.0000\n",
            "Epoch 359/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 23635826688.0000 - val_loss: 22699124736.0000\n",
            "Epoch 360/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23523078144.0000 - val_loss: 22588200960.0000\n",
            "Epoch 361/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23410681856.0000 - val_loss: 22477355008.0000\n",
            "Epoch 362/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23298553856.0000 - val_loss: 22367393792.0000\n",
            "Epoch 363/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23186853888.0000 - val_loss: 22257328128.0000\n",
            "Epoch 364/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 23075457024.0000 - val_loss: 22147725312.0000\n",
            "Epoch 365/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 22964353024.0000 - val_loss: 22038652928.0000\n",
            "Epoch 366/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 22853677056.0000 - val_loss: 21929496576.0000\n",
            "Epoch 367/400\n",
            "133/133 [==============================] - 0s 4ms/step - loss: 22743300096.0000 - val_loss: 21821247488.0000\n",
            "Epoch 368/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 22633293824.0000 - val_loss: 21712773120.0000\n",
            "Epoch 369/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 22523564032.0000 - val_loss: 21604988928.0000\n",
            "Epoch 370/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 22414217216.0000 - val_loss: 21497524224.0000\n",
            "Epoch 371/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 22305241088.0000 - val_loss: 21390573568.0000\n",
            "Epoch 372/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 22196684800.0000 - val_loss: 21283768320.0000\n",
            "Epoch 373/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 22088480768.0000 - val_loss: 21177272320.0000\n",
            "Epoch 374/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21980588032.0000 - val_loss: 21071603712.0000\n",
            "Epoch 375/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21873090560.0000 - val_loss: 20966217728.0000\n",
            "Epoch 376/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21766023168.0000 - val_loss: 20860639232.0000\n",
            "Epoch 377/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21659326464.0000 - val_loss: 20755730432.0000\n",
            "Epoch 378/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21553012736.0000 - val_loss: 20651487232.0000\n",
            "Epoch 379/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21447084032.0000 - val_loss: 20547461120.0000\n",
            "Epoch 380/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21341546496.0000 - val_loss: 20443592704.0000\n",
            "Epoch 381/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21236420608.0000 - val_loss: 20340359168.0000\n",
            "Epoch 382/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21131720704.0000 - val_loss: 20237479936.0000\n",
            "Epoch 383/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 21027436544.0000 - val_loss: 20135014400.0000\n",
            "Epoch 384/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20923582464.0000 - val_loss: 20033011712.0000\n",
            "Epoch 385/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20820205568.0000 - val_loss: 19931668480.0000\n",
            "Epoch 386/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20717234176.0000 - val_loss: 19830429696.0000\n",
            "Epoch 387/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 20614639616.0000 - val_loss: 19730102272.0000\n",
            "Epoch 388/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20512473088.0000 - val_loss: 19629778944.0000\n",
            "Epoch 389/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20410836992.0000 - val_loss: 19529725952.0000\n",
            "Epoch 390/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20309565440.0000 - val_loss: 19430576128.0000\n",
            "Epoch 391/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 20208816128.0000 - val_loss: 19331454976.0000\n",
            "Epoch 392/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 20108529664.0000 - val_loss: 19233021952.0000\n",
            "Epoch 393/400\n",
            "133/133 [==============================] - 0s 3ms/step - loss: 20008683520.0000 - val_loss: 19135232000.0000\n",
            "Epoch 394/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19909265408.0000 - val_loss: 19037845504.0000\n",
            "Epoch 395/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19810322432.0000 - val_loss: 18940667904.0000\n",
            "Epoch 396/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19711815680.0000 - val_loss: 18844127232.0000\n",
            "Epoch 397/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19613788160.0000 - val_loss: 18748108800.0000\n",
            "Epoch 398/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19516284928.0000 - val_loss: 18652350464.0000\n",
            "Epoch 399/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19419219968.0000 - val_loss: 18557399040.0000\n",
            "Epoch 400/400\n",
            "133/133 [==============================] - 0s 2ms/step - loss: 19322753024.0000 - val_loss: 18462631936.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff7e65018d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzYPgOOEdqNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21ee636-5256-4ca8-bdff-dcd3cea588a3"
      },
      "source": [
        "# Sequential Model Summary \n",
        "model.summary() "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 30)                270       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 31        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 301\n",
            "Trainable params: 301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0IzyLyTdqNo"
      },
      "source": [
        "##### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_0If9N3dqNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e38734e-e9bd-4b39-e8e3-9a3c98eb0031"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94/94 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[133420.56],\n",
              "       [131649.8 ],\n",
              "       [130553.46],\n",
              "       ...,\n",
              "       [100472.87],\n",
              "       [125982.59],\n",
              "       [140582.5 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osGmwIw7dqNq"
      },
      "source": [
        "##### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eou7_DeldqNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b5739c2-8690-4a7f-8b9e-c6a597143daa"
      },
      "source": [
        "np.sqrt(mean_squared_error(y_test,y_pred))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "135877.26911238875"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52QacZPvdqNs"
      },
      "source": [
        "#### 2. Using Sci-kit Learn API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgmVeddndqNs"
      },
      "source": [
        "Using the same dataset, we will implement MLP regressor using sci-kit learn API. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUVZ9KE8dqNt"
      },
      "source": [
        "In the below code, one hidden layer is modeled with 32 neurons. Considering\n",
        "the input and output layer, we have a total of 5 layers in the model. In case any optimizer is not mentioned then “Adam” is the default optimizer and it can manage a pretty large dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkBzCoekdqNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b8069d-cbeb-40b4-9438-1aea763edd4c"
      },
      "source": [
        "# implementing MLPregressor\n",
        "regr = MLPRegressor(hidden_layer_sizes=(32), activation=\"relu\", random_state=1, max_iter=500).fit(X_train, y_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rruum2FKdqNv"
      },
      "source": [
        "##### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yPBpSl0dqN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9520e7e0-9c08-44fb-ec6b-f3191558a5a6"
      },
      "source": [
        "y_pred = regr.predict(X_test)\n",
        "y_pred"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([106943.7296704 , 105433.71377299, 104575.22478819, ...,\n",
              "        80490.94502437, 100875.35533722, 112639.58639599])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-HUagWGdqN2"
      },
      "source": [
        "##### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKmNxwzodqN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af57247-7e0e-4b8c-8d4d-9fcbed1c56a3"
      },
      "source": [
        "np.sqrt(mean_squared_error(y_test,y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "152367.3177159458"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehdUQLWSdqN4"
      },
      "source": [
        "Now, let us look at the tuning of the neural network hyperparameters or hyperparameter regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYgkmzLEdqN5"
      },
      "source": [
        "### Fine-Tuning Neural Network Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEYREMR3dqN6"
      },
      "source": [
        "The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters  to  tweak.  Not  only  can  we  use  any  imaginable  network  architecture, but even in a simple MLP we can change the number of layers, the number of neurons per layer, the type of activation function to use in each layer, the weight initialization  logic,  and  much  more. \n",
        "\n",
        "One  option  is  to  simply  try  many  combinations  of  hyperparameters  and  see  which one works best on the validation set (or using K-fold cross-validation). For this, one approach  is  simply to use  GridSearchCV  or  RandomizedSearchCV  to  explore  the  hyperparameter space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WawLE5TndqN6"
      },
      "source": [
        "The first step is to create a function that will build and compile a Keras model, given a set of hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYsW5K_VdqN7"
      },
      "source": [
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "    model = Sequential()\n",
        "    options = {\"input_shape\": input_shape}\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(Dense(n_neurons, activation=\"relu\", **options))\n",
        "        options = {}\n",
        "    model.add(Dense(1, **options))\n",
        "    optimizer = SGD(learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBH4CYF5dqN8"
      },
      "source": [
        "This function creates a simple Sequential model for univariate regression (only one output  neuron),  with  the  given  input  shape  and  the  given  number  of  hidden  layers and  neurons,  and  it  compiles  it  using  an  SGD  optimizer  configured  with  the  given learning rate.\n",
        "\n",
        "Next, let’s create a KerasRegressor based on this build_model() function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLNf-9oFdqN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4821b2be-423e-431f-affa-ebf25ba8eef6"
      },
      "source": [
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-651c14c6d32f>:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUo6pP6JdqN-"
      },
      "source": [
        "We want to train hundreds of variants and see which one performs best on the validation set. Since there are many hyperparameters, it is preferable to use a randomized search rather than grid search. \n",
        "\n",
        "Let’s try to explore the number of hidden layers, the number of neurons, and the learning rate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKMjD2XHdqN_"
      },
      "source": [
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(1, 100),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl4MtAGwdqOA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28c64561-553a-435c-aa5f-bea15429e0fe"
      },
      "source": [
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
        "                  validation_data=(X_test,y_test.values),\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])   "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9481531869498441728.0000 - val_loss: 491635309805568.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 78661436833792.0000 - val_loss: 917590048768.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 156325003264.0000 - val_loss: 14689317888.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13042178048.0000 - val_loss: 12849015808.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786084864.0000 - val_loss: 12798257152.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785166336.0000 - val_loss: 12806476800.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784065536.0000 - val_loss: 12809383936.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784755712.0000 - val_loss: 12816465920.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12783903744.0000 - val_loss: 12801881088.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784860160.0000 - val_loss: 12798279680.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785627136.0000 - val_loss: 12807249920.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784129024.0000 - val_loss: 12815282176.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12784833536.0000 - val_loss: 12810153984.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12784890880.0000 - val_loss: 12804763648.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12784902144.0000 - val_loss: 12802086912.0000\n",
            "178/178 [==============================] - 0s 2ms/step - loss: 14854683648.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 724022013533177188756488192.0000 - val_loss: 135162023009444071211008.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 99779132249037579747328.0000 - val_loss: 70933121772167188447232.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 52364199544166702645248.0000 - val_loss: 37225817771122026872832.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 27480798293028553883648.0000 - val_loss: 19536153564571406172160.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14421971662320714121216.0000 - val_loss: 10252616814394681065472.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7568674661415050543104.0000 - val_loss: 5380577701509004787712.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3972045674326467280896.0000 - val_loss: 2823734448363164139520.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2084533730987405737984.0000 - val_loss: 1481900168712030257152.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1093964600409963102208.0000 - val_loss: 777701638746427883520.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 574114763925656436736.0000 - val_loss: 408139314364776710144.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 301295951989411151872.0000 - val_loss: 214191708451136077824.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 158120257017071271936.0000 - val_loss: 112407937946981761024.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 82981752233277784064.0000 - val_loss: 58991873064693661696.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 43548981563928608768.0000 - val_loss: 30958975291567898624.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 22854538854905413632.0000 - val_loss: 16247319496413413376.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11994101756431695872.0000 - val_loss: 8526593376391266304.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6294512204238553088.0000 - val_loss: 4474758062984396800.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 3303365862542540800.0000 - val_loss: 2348353427278397440.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 1733609466802208768.0000 - val_loss: 1232413996271796224.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 909798661704122368.0000 - val_loss: 646770122853187584.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 477463886434402304.0000 - val_loss: 339423912418869248.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 250573734987956224.0000 - val_loss: 178128786439536640.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 131501315804102656.0000 - val_loss: 93481286047367168.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 69012054203170816.0000 - val_loss: 49058460779675648.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 36217595191361536.0000 - val_loss: 25745419099176960.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 19007019138678784.0000 - val_loss: 13510894445133824.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9974913368588288.0000 - val_loss: 7090298979090432.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5234858611703808.0000 - val_loss: 3720827610595328.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2747269656870912.0000 - val_loss: 1952575213535232.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1441779853295616.0000 - val_loss: 1024627732119552.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 756656937893888.0000 - val_loss: 537663434129408.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 397102207205376.0000 - val_loss: 282124288524288.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 208406979805184.0000 - val_loss: 148028866428928.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 109379252649984.0000 - val_loss: 77666422095872.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 57409213038592.0000 - val_loss: 40745901752320.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 30135336042496.0000 - val_loss: 21375553110016.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 15821738868736.0000 - val_loss: 11214282293248.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8310521593856.0000 - val_loss: 5883743436800.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4368208494592.0000 - val_loss: 3088638017536.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2299627044864.0000 - val_loss: 1623263412224.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1214006427648.0000 - val_loss: 855336681472.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 644341760000.0000 - val_loss: 453079007232.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 345417252864.0000 - val_loss: 242423840768.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 188465840128.0000 - val_loss: 132328267776.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 106134355968.0000 - val_loss: 74766876672.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 62884577280.0000 - val_loss: 44817973248.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 40221278208.0000 - val_loss: 29223921664.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 28309649408.0000 - val_loss: 21127084032.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 22040883200.0000 - val_loss: 16980971520.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 18766469120.0000 - val_loss: 14849712128.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 17042857984.0000 - val_loss: 13775028224.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 16139813888.0000 - val_loss: 13244813312.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 15667469312.0000 - val_loss: 12986068992.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 15420115968.0000 - val_loss: 12866878464.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15289923584.0000 - val_loss: 12815185920.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15221780480.0000 - val_loss: 12796041216.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15186213888.0000 - val_loss: 12791799808.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15166696448.0000 - val_loss: 12794007552.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15156542464.0000 - val_loss: 12798831616.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151104000.0000 - val_loss: 12802790400.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15148804096.0000 - val_loss: 12806763520.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15147357184.0000 - val_loss: 12809783296.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146750976.0000 - val_loss: 12812951552.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146188800.0000 - val_loss: 12814382080.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146025984.0000 - val_loss: 12816831488.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15145930752.0000 - val_loss: 12817973248.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15145783296.0000 - val_loss: 12818482176.0000\n",
            "178/178 [==============================] - 0s 2ms/step - loss: 10105098240.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 2s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 0s 1ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2851620987600896.0000 - val_loss: 669624827904.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 492960743424.0000 - val_loss: 358835290112.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 264802156544.0000 - val_loss: 195312369664.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 145046110208.0000 - val_loss: 109253189632.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 82202624000.0000 - val_loss: 63900393472.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 49200807936.0000 - val_loss: 39969984512.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 31896913920.0000 - val_loss: 27309221888.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 22802546688.0000 - val_loss: 20597137408.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 18042466304.0000 - val_loss: 17027577856.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15542284288.0000 - val_loss: 15122399232.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14234504192.0000 - val_loss: 14092160000.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13543969792.0000 - val_loss: 13531096064.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13181769728.0000 - val_loss: 13223259136.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12991416320.0000 - val_loss: 13052204032.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12892217344.0000 - val_loss: 12953580544.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12839211008.0000 - val_loss: 12896865280.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12811756544.0000 - val_loss: 12863856640.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12797895680.0000 - val_loss: 12845388800.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12790797312.0000 - val_loss: 12832644096.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12786723840.0000 - val_loss: 12824358912.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784687104.0000 - val_loss: 12818318336.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12783398912.0000 - val_loss: 12815300608.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782996480.0000 - val_loss: 12813438976.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782774272.0000 - val_loss: 12812098560.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782566400.0000 - val_loss: 12810157056.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782511104.0000 - val_loss: 12809916416.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782474240.0000 - val_loss: 12809832448.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782420992.0000 - val_loss: 12808619008.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782360576.0000 - val_loss: 12808780800.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782352384.0000 - val_loss: 12808464384.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782368768.0000 - val_loss: 12808297472.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782345216.0000 - val_loss: 12808398848.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782354432.0000 - val_loss: 12808478720.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782391296.0000 - val_loss: 12808594432.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782331904.0000 - val_loss: 12807891968.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782370816.0000 - val_loss: 12807142400.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782370816.0000 - val_loss: 12807250944.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782382080.0000 - val_loss: 12806832128.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782403584.0000 - val_loss: 12807220224.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782354432.0000 - val_loss: 12807126016.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782373888.0000 - val_loss: 12807627776.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782384128.0000 - val_loss: 12808122368.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782360576.0000 - val_loss: 12807872512.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782307328.0000 - val_loss: 12808008704.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782371840.0000 - val_loss: 12808053760.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782352384.0000 - val_loss: 12807682048.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782327808.0000 - val_loss: 12807686144.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782365696.0000 - val_loss: 12808172544.0000\n",
            "178/178 [==============================] - 0s 2ms/step - loss: 14881316864.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 31407683584.0000 - val_loss: 15683811328.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15845799936.0000 - val_loss: 12387433472.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14325812224.0000 - val_loss: 12027687936.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13992496128.0000 - val_loss: 11863852032.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13761551360.0000 - val_loss: 11718338560.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13550676992.0000 - val_loss: 11563331584.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13346309120.0000 - val_loss: 11412697088.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13149770752.0000 - val_loss: 11265930240.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12960191488.0000 - val_loss: 11133565952.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12776508416.0000 - val_loss: 11004084224.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12599743488.0000 - val_loss: 10863885312.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12429129728.0000 - val_loss: 10743422976.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12263655424.0000 - val_loss: 10631245824.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12103760896.0000 - val_loss: 10507098112.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11948225536.0000 - val_loss: 10390574080.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11797659648.0000 - val_loss: 10296821760.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11652576256.0000 - val_loss: 10183760896.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11510939648.0000 - val_loss: 10069477376.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 11374824448.0000 - val_loss: 9981648896.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 11242075136.0000 - val_loss: 9881986048.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11113634816.0000 - val_loss: 9800118272.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10988527616.0000 - val_loss: 9702381568.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10867223552.0000 - val_loss: 9611621376.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10748756992.0000 - val_loss: 9521670144.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10635117568.0000 - val_loss: 9446368256.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10523489280.0000 - val_loss: 9367075840.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10415866880.0000 - val_loss: 9285569536.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10310805504.0000 - val_loss: 9203592192.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10208988160.0000 - val_loss: 9129648128.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10109409280.0000 - val_loss: 9056250880.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10013045760.0000 - val_loss: 8980667392.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9919134720.0000 - val_loss: 8912939008.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9827641344.0000 - val_loss: 8850749440.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9738411008.0000 - val_loss: 8789532672.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9651940352.0000 - val_loss: 8715384832.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9567416320.0000 - val_loss: 8651101184.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9484992512.0000 - val_loss: 8597645312.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9404850176.0000 - val_loss: 8531803136.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 9327110144.0000 - val_loss: 8479207936.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9251595264.0000 - val_loss: 8420624384.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9177234432.0000 - val_loss: 8370402816.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9105545216.0000 - val_loss: 8319883776.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9035374592.0000 - val_loss: 8244572672.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8967408640.0000 - val_loss: 8199763456.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8900636672.0000 - val_loss: 8147862016.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8835742720.0000 - val_loss: 8104786944.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8772974592.0000 - val_loss: 8049546752.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8711431168.0000 - val_loss: 8011581440.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8650744832.0000 - val_loss: 7950828032.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8593341440.0000 - val_loss: 7912485888.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8536253440.0000 - val_loss: 7874193920.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8480751104.0000 - val_loss: 7825057792.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8426774528.0000 - val_loss: 7784383488.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8374036480.0000 - val_loss: 7742528000.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8322641408.0000 - val_loss: 7711306240.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8272598528.0000 - val_loss: 7660543488.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 8223739392.0000 - val_loss: 7627744256.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8176134656.0000 - val_loss: 7583755776.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 8129833472.0000 - val_loss: 7548534272.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8084544000.0000 - val_loss: 7516564480.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8040298496.0000 - val_loss: 7483191808.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7997334016.0000 - val_loss: 7436527616.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7954966016.0000 - val_loss: 7417745408.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7914614272.0000 - val_loss: 7377616384.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7874165760.0000 - val_loss: 7350205440.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7835224064.0000 - val_loss: 7316136448.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7797215232.0000 - val_loss: 7280808448.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7760018432.0000 - val_loss: 7255472128.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7724070912.0000 - val_loss: 7229229056.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7688534016.0000 - val_loss: 7198953984.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7654126080.0000 - val_loss: 7169332224.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7620291072.0000 - val_loss: 7148419584.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7587698176.0000 - val_loss: 7112444416.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7555640832.0000 - val_loss: 7091761152.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7524331008.0000 - val_loss: 7067943936.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7493909504.0000 - val_loss: 7041499648.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7464215040.0000 - val_loss: 7014186496.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7434983936.0000 - val_loss: 6981257728.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7406021120.0000 - val_loss: 6952529408.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7378343424.0000 - val_loss: 6945089536.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7351771648.0000 - val_loss: 6922415616.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7325144576.0000 - val_loss: 6889531392.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7299515904.0000 - val_loss: 6872949248.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7274173440.0000 - val_loss: 6849007104.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7249565696.0000 - val_loss: 6826074112.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7225421312.0000 - val_loss: 6811727360.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7202202624.0000 - val_loss: 6786643968.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7179291136.0000 - val_loss: 6764750336.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7157165568.0000 - val_loss: 6749419008.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7135404032.0000 - val_loss: 6733629440.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7113911808.0000 - val_loss: 6713699328.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7092850688.0000 - val_loss: 6706308096.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7072737792.0000 - val_loss: 6692291584.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7052921344.0000 - val_loss: 6667387904.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7033531904.0000 - val_loss: 6644202496.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7014558720.0000 - val_loss: 6632258560.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6995597312.0000 - val_loss: 6605980672.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6978328576.0000 - val_loss: 6596655104.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6960307712.0000 - val_loss: 6591827456.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6942842368.0000 - val_loss: 6572217856.0000\n",
            "178/178 [==============================] - 1s 1ms/step - loss: 6086749696.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 28616275968.0000 - val_loss: 15852128256.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13309813760.0000 - val_loss: 12417377280.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11809284096.0000 - val_loss: 12051241984.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11543996416.0000 - val_loss: 11906235392.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11383225344.0000 - val_loss: 11771610112.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11236401152.0000 - val_loss: 11638236160.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11094033408.0000 - val_loss: 11506862080.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10955662336.0000 - val_loss: 11376753664.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10820200448.0000 - val_loss: 11250856960.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10688558080.0000 - val_loss: 11129793536.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10559228928.0000 - val_loss: 11006809088.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10434477056.0000 - val_loss: 10891061248.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10312157184.0000 - val_loss: 10777337856.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 10192189440.0000 - val_loss: 10668435456.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10076308480.0000 - val_loss: 10558635008.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9962419200.0000 - val_loss: 10452543488.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9851592704.0000 - val_loss: 10348864512.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9743320064.0000 - val_loss: 10247725056.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9637307392.0000 - val_loss: 10148073472.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9533916160.0000 - val_loss: 10052235264.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9433010176.0000 - val_loss: 9957766144.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9334788096.0000 - val_loss: 9865378816.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9238694912.0000 - val_loss: 9775278080.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9144563712.0000 - val_loss: 9687224320.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9052534784.0000 - val_loss: 9600659456.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8962886656.0000 - val_loss: 9516541952.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8875828224.0000 - val_loss: 9434811392.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8790522880.0000 - val_loss: 9354682368.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8706829312.0000 - val_loss: 9276761088.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 8625178624.0000 - val_loss: 9200702464.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8545781248.0000 - val_loss: 9126720512.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 8468667904.0000 - val_loss: 9053218816.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8392354304.0000 - val_loss: 8981959680.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8318154752.0000 - val_loss: 8912110592.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8245498880.0000 - val_loss: 8844186624.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8174659584.0000 - val_loss: 8777738240.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8105617408.0000 - val_loss: 8712780800.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8037915648.0000 - val_loss: 8649085952.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7971709952.0000 - val_loss: 8587247616.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7907123712.0000 - val_loss: 8526164480.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7843886080.0000 - val_loss: 8466567680.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7781861376.0000 - val_loss: 8408863232.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7721709568.0000 - val_loss: 8352425984.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7662927872.0000 - val_loss: 8297316352.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7605318144.0000 - val_loss: 8243472384.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7549245440.0000 - val_loss: 8190761984.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7493460480.0000 - val_loss: 8138464768.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7440497664.0000 - val_loss: 8088071680.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7387684352.0000 - val_loss: 8038710272.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7335962624.0000 - val_loss: 7990305792.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7286012928.0000 - val_loss: 7943692288.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7236892160.0000 - val_loss: 7897617408.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7189041664.0000 - val_loss: 7852603904.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7141936128.0000 - val_loss: 7808626176.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7096221184.0000 - val_loss: 7764912128.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7051160576.0000 - val_loss: 7722801152.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7007146496.0000 - val_loss: 7681416192.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6964301312.0000 - val_loss: 7641848320.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6922398720.0000 - val_loss: 7602901504.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6881230848.0000 - val_loss: 7563346432.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6841239552.0000 - val_loss: 7527458816.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6801729024.0000 - val_loss: 7488258560.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6763401728.0000 - val_loss: 7452807680.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6725891072.0000 - val_loss: 7418050048.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6689238528.0000 - val_loss: 7383619072.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6653514752.0000 - val_loss: 7349595136.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6618158080.0000 - val_loss: 7315465216.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 2s 7ms/step - loss: 6584143360.0000 - val_loss: 7284842496.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6550428672.0000 - val_loss: 7252961280.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6517624832.0000 - val_loss: 7222319616.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6485476352.0000 - val_loss: 7193556480.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6454066176.0000 - val_loss: 7163616768.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6423229440.0000 - val_loss: 7134359552.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6393195520.0000 - val_loss: 7107736576.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6364029952.0000 - val_loss: 7077279232.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6335160832.0000 - val_loss: 7050190848.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6307016192.0000 - val_loss: 7024117248.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6279334400.0000 - val_loss: 6999633408.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6252336640.0000 - val_loss: 6975952896.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6226361856.0000 - val_loss: 6950232064.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6200541184.0000 - val_loss: 6925120512.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6175267840.0000 - val_loss: 6902152192.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6150644736.0000 - val_loss: 6876574208.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6126074880.0000 - val_loss: 6856752640.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6102766592.0000 - val_loss: 6835297280.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6079499264.0000 - val_loss: 6811130368.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6056684032.0000 - val_loss: 6788354048.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6034774016.0000 - val_loss: 6770534912.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6012791296.0000 - val_loss: 6747973632.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5991972352.0000 - val_loss: 6729463808.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5970939904.0000 - val_loss: 6708838912.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5950800896.0000 - val_loss: 6689226240.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5930899968.0000 - val_loss: 6670514176.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5911479296.0000 - val_loss: 6653902336.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5892193280.0000 - val_loss: 6637429248.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5873582080.0000 - val_loss: 6620456448.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5855330304.0000 - val_loss: 6602755072.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5837541888.0000 - val_loss: 6585758208.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5819935744.0000 - val_loss: 6571184128.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5802885120.0000 - val_loss: 6552710656.0000\n",
            "178/178 [==============================] - 0s 2ms/step - loss: 8399391744.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 27387172864.0000 - val_loss: 16615571456.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13262791680.0000 - val_loss: 12839060480.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11980139520.0000 - val_loss: 12289700864.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11739597824.0000 - val_loss: 12081730560.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 11588237312.0000 - val_loss: 11946398720.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11448806400.0000 - val_loss: 11828508672.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11314396160.0000 - val_loss: 11715125248.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11183301632.0000 - val_loss: 11593977856.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11056808960.0000 - val_loss: 11494019072.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10933371904.0000 - val_loss: 11396709376.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10813077504.0000 - val_loss: 11305660416.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10696416256.0000 - val_loss: 11194238976.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10583346176.0000 - val_loss: 11107170304.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10472028160.0000 - val_loss: 11010041856.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10365621248.0000 - val_loss: 10931208192.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10261335040.0000 - val_loss: 10852566016.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10159339520.0000 - val_loss: 10767608832.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10060763136.0000 - val_loss: 10676817920.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9963881472.0000 - val_loss: 10591095808.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9870918656.0000 - val_loss: 10539538432.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9779407872.0000 - val_loss: 10464821248.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9691179008.0000 - val_loss: 10394301440.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9604986880.0000 - val_loss: 10320447488.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 9520921600.0000 - val_loss: 10267620352.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9439522816.0000 - val_loss: 10190581760.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9359263744.0000 - val_loss: 10138845184.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9281567744.0000 - val_loss: 10071059456.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9205961728.0000 - val_loss: 10011362304.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9132221440.0000 - val_loss: 9955039232.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9060646912.0000 - val_loss: 9878691840.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8990666752.0000 - val_loss: 9833476096.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8922786816.0000 - val_loss: 9766903808.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8856308736.0000 - val_loss: 9711310848.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8791436288.0000 - val_loss: 9661543424.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8727531520.0000 - val_loss: 9624108032.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8666748928.0000 - val_loss: 9553536000.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8606486528.0000 - val_loss: 9492819968.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8547807232.0000 - val_loss: 9434982400.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 8490953728.0000 - val_loss: 9403462656.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8435080704.0000 - val_loss: 9360785408.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8380613120.0000 - val_loss: 9316801536.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8327454720.0000 - val_loss: 9278799872.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8275568128.0000 - val_loss: 9221876736.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8224859136.0000 - val_loss: 9178786816.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8175456256.0000 - val_loss: 9121444864.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8127109632.0000 - val_loss: 9090380800.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8079859200.0000 - val_loss: 9053758464.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8033818112.0000 - val_loss: 9017111552.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7989124096.0000 - val_loss: 8956261376.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7944878592.0000 - val_loss: 8909570048.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7902379008.0000 - val_loss: 8872072192.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7860364800.0000 - val_loss: 8854316032.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7819541504.0000 - val_loss: 8816467968.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7779689472.0000 - val_loss: 8780151808.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7740868096.0000 - val_loss: 8729312256.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7702861312.0000 - val_loss: 8694262784.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7665348608.0000 - val_loss: 8658111488.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7629010432.0000 - val_loss: 8615604224.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7593042944.0000 - val_loss: 8603813888.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7558598144.0000 - val_loss: 8572048896.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7524772864.0000 - val_loss: 8542848000.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7491344384.0000 - val_loss: 8489517568.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7459238400.0000 - val_loss: 8469155328.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7427715584.0000 - val_loss: 8430386176.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7396603904.0000 - val_loss: 8393055232.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7366428672.0000 - val_loss: 8370185728.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7336668160.0000 - val_loss: 8330968576.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7307917312.0000 - val_loss: 8307691008.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7279303680.0000 - val_loss: 8287594496.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7251833344.0000 - val_loss: 8249522688.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7224705536.0000 - val_loss: 8219142144.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7198224384.0000 - val_loss: 8197901312.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7172231680.0000 - val_loss: 8162995712.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7146971136.0000 - val_loss: 8134670336.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7122163712.0000 - val_loss: 8110287360.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7098067968.0000 - val_loss: 8086104064.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7074357248.0000 - val_loss: 8067690496.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7051176960.0000 - val_loss: 8038492672.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7028427264.0000 - val_loss: 8012228608.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7006050304.0000 - val_loss: 7977055744.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6984600576.0000 - val_loss: 7956201984.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6963297280.0000 - val_loss: 7939328000.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6942473216.0000 - val_loss: 7905596928.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6922101760.0000 - val_loss: 7899365376.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6902446080.0000 - val_loss: 7875671040.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6882668544.0000 - val_loss: 7857536512.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6863884288.0000 - val_loss: 7823115264.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6844979200.0000 - val_loss: 7793430016.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6826772480.0000 - val_loss: 7764164096.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6809066496.0000 - val_loss: 7743797248.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6791299584.0000 - val_loss: 7734474240.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6774346240.0000 - val_loss: 7716457472.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6757706752.0000 - val_loss: 7697071616.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6741168128.0000 - val_loss: 7678845952.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6725055488.0000 - val_loss: 7653050880.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6709294592.0000 - val_loss: 7628005888.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6693586432.0000 - val_loss: 7622742528.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6678824960.0000 - val_loss: 7603618816.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6663978496.0000 - val_loss: 7578713088.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6649421824.0000 - val_loss: 7552718336.0000\n",
            "178/178 [==============================] - 0s 2ms/step - loss: 9534153728.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16958525398700785664.0000 - val_loss: 5335169183514624.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3291849498296320.0000 - val_loss: 1839519594708992.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1135120530538496.0000 - val_loss: 634211715252224.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 391430166020096.0000 - val_loss: 218633515040768.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 134984287387648.0000 - val_loss: 75362121809920.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 46557189636096.0000 - val_loss: 25974324330496.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16064586973184.0000 - val_loss: 8951939727360.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5549929791488.0000 - val_loss: 3088448749568.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1924040228864.0000 - val_loss: 1068713115648.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 673341833216.0000 - val_loss: 374375219200.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 242150244352.0000 - val_loss: 135875059712.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 93388095488.0000 - val_loss: 54322065408.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 42132426752.0000 - val_loss: 26606788608.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 24456435712.0000 - val_loss: 17254010880.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 18353995776.0000 - val_loss: 14165791744.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16252040192.0000 - val_loss: 13174417408.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15528640512.0000 - val_loss: 12879513600.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15277068288.0000 - val_loss: 12801362944.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15189741568.0000 - val_loss: 12791961600.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15161241600.0000 - val_loss: 12797283328.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15151502336.0000 - val_loss: 12804288512.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15148119040.0000 - val_loss: 12809335808.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146782720.0000 - val_loss: 12813756416.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146339328.0000 - val_loss: 12816055296.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146179584.0000 - val_loss: 12817868800.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 15146093568.0000 - val_loss: 12818716672.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 15146104832.0000 - val_loss: 12818873344.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 15146002432.0000 - val_loss: 12818884608.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146139648.0000 - val_loss: 12818275328.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 10104637440.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 287940650439746256896.0000 - val_loss: 88400253836853248.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 54536936378859520.0000 - val_loss: 30482409396895744.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 18805842870534144.0000 - val_loss: 10510931729580032.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6484776772960256.0000 - val_loss: 3624314058309632.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 2236137275916288.0000 - val_loss: 1249685159280640.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 771090645254144.0000 - val_loss: 430886252183552.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 265902264156160.0000 - val_loss: 148561090052096.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 91699623755776.0000 - val_loss: 51218944622592.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 31628824936448.0000 - val_loss: 17659882110976.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10914685255680.0000 - val_loss: 6091859034112.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3771856060416.0000 - val_loss: 2105536151552.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1308962586624.0000 - val_loss: 732558065664.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 459555766272.0000 - val_loss: 259806740480.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 166615580672.0000 - val_loss: 97268170752.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 65578741760.0000 - val_loss: 41505865728.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 30733553664.0000 - val_loss: 22472316928.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 18716635136.0000 - val_loss: 15987833856.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14561292288.0000 - val_loss: 13803187200.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13127834624.0000 - val_loss: 13096318976.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12638666752.0000 - val_loss: 12873068544.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12470296576.0000 - val_loss: 12807912448.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12411218944.0000 - val_loss: 12792830976.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12391286784.0000 - val_loss: 12792090624.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12384871424.0000 - val_loss: 12793985024.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382581760.0000 - val_loss: 12796404736.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381582336.0000 - val_loss: 12798580736.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381333504.0000 - val_loss: 12799946752.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381191168.0000 - val_loss: 12801305600.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381092864.0000 - val_loss: 12800637952.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381115392.0000 - val_loss: 12800060416.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381194240.0000 - val_loss: 12799543296.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381255680.0000 - val_loss: 12799979520.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381177856.0000 - val_loss: 12799925248.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 15598688256.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 2s 4ms/step - loss: 102029104190062592.0000 - val_loss: 29402459013120.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 18117008818176.0000 - val_loss: 10158227849216.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6256189243392.0000 - val_loss: 3517546758144.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2165984460800.0000 - val_loss: 1225483485184.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 755491930112.0000 - val_loss: 432974233600.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 268907053056.0000 - val_loss: 158949457920.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 101094514688.0000 - val_loss: 63941492736.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 43250565120.0000 - val_loss: 30839386112.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 23277316096.0000 - val_loss: 19273291776.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16406419456.0000 - val_loss: 15199396864.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14042134528.0000 - val_loss: 13723104256.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13218920448.0000 - val_loss: 13170852864.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12930835456.0000 - val_loss: 12959740928.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12832606208.0000 - val_loss: 12876305408.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12799936512.0000 - val_loss: 12841113600.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12788108288.0000 - val_loss: 12822870016.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12784175104.0000 - val_loss: 12816120832.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12783142912.0000 - val_loss: 12812530688.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782803968.0000 - val_loss: 12810682368.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782503936.0000 - val_loss: 12807849984.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782653440.0000 - val_loss: 12807283712.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782617600.0000 - val_loss: 12807094272.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782570496.0000 - val_loss: 12808075264.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782627840.0000 - val_loss: 12808477696.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782530560.0000 - val_loss: 12807834624.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782546944.0000 - val_loss: 12807960576.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782475264.0000 - val_loss: 12808922112.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782574592.0000 - val_loss: 12809186304.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782553088.0000 - val_loss: 12807627776.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782630912.0000 - val_loss: 12807651328.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782594048.0000 - val_loss: 12807851008.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782530560.0000 - val_loss: 12807716864.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 14879467520.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 2s 3ms/step - loss: 6728326316032.0000 - val_loss: 13421175808.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12507506688.0000 - val_loss: 12793553920.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384546816.0000 - val_loss: 12798039040.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382799872.0000 - val_loss: 12794755072.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12383363072.0000 - val_loss: 12796933120.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382650368.0000 - val_loss: 12798106624.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12383800320.0000 - val_loss: 12806629376.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384113664.0000 - val_loss: 12803184640.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382349312.0000 - val_loss: 12814471168.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382278656.0000 - val_loss: 12798159872.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382628864.0000 - val_loss: 12798138368.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12383235072.0000 - val_loss: 12801278976.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 15600770048.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 2s 3ms/step - loss: 46494459991431130906624.0000 - val_loss: 1632470065087512576.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 236677832158216192.0000 - val_loss: 1526268436676608.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 221358856339456.0000 - val_loss: 1433373114368.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 219715223552.0000 - val_loss: 13763264512.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12963833856.0000 - val_loss: 12801843200.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785129472.0000 - val_loss: 12805580800.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785261568.0000 - val_loss: 12805864448.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784458752.0000 - val_loss: 12801750016.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784896000.0000 - val_loss: 12817341440.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784833536.0000 - val_loss: 12802925568.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12783930368.0000 - val_loss: 12817903616.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785259520.0000 - val_loss: 12809744384.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12784687104.0000 - val_loss: 12804050944.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12785144832.0000 - val_loss: 12811382784.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12785520640.0000 - val_loss: 12806162432.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784312320.0000 - val_loss: 12819076096.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785531904.0000 - val_loss: 12802407424.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785564672.0000 - val_loss: 12804811776.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 14867196928.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 29167988736.0000 - val_loss: 14306412544.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15148493824.0000 - val_loss: 12194724864.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14145614848.0000 - val_loss: 11936013312.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13841257472.0000 - val_loss: 11746388992.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13586287616.0000 - val_loss: 11566606336.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13343144960.0000 - val_loss: 11392344064.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13110699008.0000 - val_loss: 11218927616.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12887581696.0000 - val_loss: 11068338176.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12673965056.0000 - val_loss: 10914157568.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12469041152.0000 - val_loss: 10769439744.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12272716800.0000 - val_loss: 10616515584.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12082409472.0000 - val_loss: 10479427584.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11899133952.0000 - val_loss: 10341276672.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11723954176.0000 - val_loss: 10216799232.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11553951744.0000 - val_loss: 10110051328.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 11390718976.0000 - val_loss: 9980953600.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11233064960.0000 - val_loss: 9872454656.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11081746432.0000 - val_loss: 9763492864.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10934968320.0000 - val_loss: 9658962944.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10791802880.0000 - val_loss: 9564198912.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10655408128.0000 - val_loss: 9462654976.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10522738688.0000 - val_loss: 9351576576.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10395017216.0000 - val_loss: 9257740288.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10271670272.0000 - val_loss: 9166460928.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10151861248.0000 - val_loss: 9077620736.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10036417536.0000 - val_loss: 8995771392.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9923859456.0000 - val_loss: 8924007424.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9815685120.0000 - val_loss: 8843024384.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9710731264.0000 - val_loss: 8757650432.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9608811520.0000 - val_loss: 8667847680.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9510664192.0000 - val_loss: 8592937984.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 9415460864.0000 - val_loss: 8536135168.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 9323244544.0000 - val_loss: 8466182144.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9234209792.0000 - val_loss: 8405546496.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9147071488.0000 - val_loss: 8323785728.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9063117824.0000 - val_loss: 8279511552.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8981700608.0000 - val_loss: 8210057728.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8902336512.0000 - val_loss: 8134415872.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8826143744.0000 - val_loss: 8078851584.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8751793152.0000 - val_loss: 8030831616.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8679592960.0000 - val_loss: 7969753088.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8609102848.0000 - val_loss: 7939764224.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8541992960.0000 - val_loss: 7881891328.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8476265472.0000 - val_loss: 7815877120.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8411937280.0000 - val_loss: 7756333568.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8350185472.0000 - val_loss: 7725718528.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8289785344.0000 - val_loss: 7667375104.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8231984128.0000 - val_loss: 7623083008.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8175240192.0000 - val_loss: 7580000256.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8120139264.0000 - val_loss: 7542800896.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 8066810368.0000 - val_loss: 7504835072.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8015006720.0000 - val_loss: 7465819648.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7964904960.0000 - val_loss: 7417862144.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7915308544.0000 - val_loss: 7370735616.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7868034048.0000 - val_loss: 7345230336.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7821987840.0000 - val_loss: 7299959808.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7777325568.0000 - val_loss: 7273497600.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7733919744.0000 - val_loss: 7232711168.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7691950080.0000 - val_loss: 7195066880.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7650734592.0000 - val_loss: 7171298816.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7611253760.0000 - val_loss: 7136584704.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7572616704.0000 - val_loss: 7099632128.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7535225856.0000 - val_loss: 7078655488.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7498823168.0000 - val_loss: 7034794496.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7463213568.0000 - val_loss: 7014560256.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7428833792.0000 - val_loss: 6983482368.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7395295744.0000 - val_loss: 6958242816.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7362643456.0000 - val_loss: 6930281984.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7331102208.0000 - val_loss: 6893189632.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7300442112.0000 - val_loss: 6871433216.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7270522880.0000 - val_loss: 6847027712.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7241213440.0000 - val_loss: 6819442688.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7213108736.0000 - val_loss: 6798788608.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7185334784.0000 - val_loss: 6764227584.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7159241216.0000 - val_loss: 6747035648.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7133180416.0000 - val_loss: 6741705216.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7108009472.0000 - val_loss: 6719021056.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7083606016.0000 - val_loss: 6696082432.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7059481600.0000 - val_loss: 6677023232.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7036477440.0000 - val_loss: 6657908224.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7013986816.0000 - val_loss: 6641190912.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6991847936.0000 - val_loss: 6614316544.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6970250240.0000 - val_loss: 6606728704.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6949744640.0000 - val_loss: 6589744640.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6929531904.0000 - val_loss: 6564120576.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6909977600.0000 - val_loss: 6549097472.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6890397184.0000 - val_loss: 6518679040.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6872568832.0000 - val_loss: 6510063104.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6854151680.0000 - val_loss: 6501719040.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6836631040.0000 - val_loss: 6482296832.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6819440640.0000 - val_loss: 6475356672.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6802784256.0000 - val_loss: 6452120064.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6786707968.0000 - val_loss: 6437621248.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6770491904.0000 - val_loss: 6422359040.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6755575808.0000 - val_loss: 6409697792.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6740523520.0000 - val_loss: 6402357760.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6725840384.0000 - val_loss: 6390657536.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6711596032.0000 - val_loss: 6378540032.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6697954304.0000 - val_loss: 6362210304.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6684463616.0000 - val_loss: 6357164032.0000\n",
            "178/178 [==============================] - 0s 2ms/step - loss: 5912014336.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 26432579584.0000 - val_loss: 14451810304.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12612590592.0000 - val_loss: 12205905920.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11661556736.0000 - val_loss: 11967540224.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11438006272.0000 - val_loss: 11805684736.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11261129728.0000 - val_loss: 11649753088.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11091834880.0000 - val_loss: 11494804480.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10927840256.0000 - val_loss: 11341674496.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 10768571392.0000 - val_loss: 11191276544.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10614059008.0000 - val_loss: 11046908928.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 10464400384.0000 - val_loss: 10907823104.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 10318489600.0000 - val_loss: 10772429824.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10177026048.0000 - val_loss: 10641060864.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10039892992.0000 - val_loss: 10513561600.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9906596864.0000 - val_loss: 10389996544.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9776530432.0000 - val_loss: 10270316544.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9650789376.0000 - val_loss: 10151108608.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9528512512.0000 - val_loss: 10036768768.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9409497088.0000 - val_loss: 9925977088.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9293715456.0000 - val_loss: 9817780224.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9181040640.0000 - val_loss: 9713052672.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9071424512.0000 - val_loss: 9610504192.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8965356544.0000 - val_loss: 9510233088.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8861635584.0000 - val_loss: 9414112256.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8760985600.0000 - val_loss: 9320268800.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8663441408.0000 - val_loss: 9228549120.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8568144896.0000 - val_loss: 9139418112.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8475061248.0000 - val_loss: 9052342272.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 8385110528.0000 - val_loss: 8967962624.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8297288704.0000 - val_loss: 8886154240.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8212327936.0000 - val_loss: 8806319104.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8129104384.0000 - val_loss: 8728626176.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8048076800.0000 - val_loss: 8652741632.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7969977856.0000 - val_loss: 8579373568.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7893520384.0000 - val_loss: 8507340288.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7819034624.0000 - val_loss: 8437518336.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7746522112.0000 - val_loss: 8369780224.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7675894784.0000 - val_loss: 8303744512.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7607095296.0000 - val_loss: 8239076864.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7540101632.0000 - val_loss: 8176344064.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7475040256.0000 - val_loss: 8115816960.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7411801088.0000 - val_loss: 8056222208.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7350504960.0000 - val_loss: 7998852096.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7290522624.0000 - val_loss: 7942494208.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7232424960.0000 - val_loss: 7887798784.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7175910400.0000 - val_loss: 7835188224.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7120262144.0000 - val_loss: 7785410560.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7066926592.0000 - val_loss: 7734055936.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7014246912.0000 - val_loss: 7683747328.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6963236864.0000 - val_loss: 7635738624.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6913545216.0000 - val_loss: 7588915200.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6865140224.0000 - val_loss: 7545190912.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6818210816.0000 - val_loss: 7500768768.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6771620864.0000 - val_loss: 7460255232.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6727616000.0000 - val_loss: 7415586304.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6684448768.0000 - val_loss: 7376006656.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6642013696.0000 - val_loss: 7335576064.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6600782848.0000 - val_loss: 7298370048.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6560584192.0000 - val_loss: 7258728448.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6521394176.0000 - val_loss: 7222147584.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6483089920.0000 - val_loss: 7188273152.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6445970432.0000 - val_loss: 7153513984.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6409916416.0000 - val_loss: 7120014336.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6374473728.0000 - val_loss: 7086177280.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6340230144.0000 - val_loss: 7051698176.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6306648576.0000 - val_loss: 7022898688.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6274204160.0000 - val_loss: 6992087040.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6242496000.0000 - val_loss: 6959783424.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6211632128.0000 - val_loss: 6932388352.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6181374976.0000 - val_loss: 6902308864.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6151901696.0000 - val_loss: 6875642368.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6123218944.0000 - val_loss: 6850117632.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6095301120.0000 - val_loss: 6822760960.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6068020736.0000 - val_loss: 6799764992.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6041725440.0000 - val_loss: 6774824960.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6015826944.0000 - val_loss: 6749061632.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5990718976.0000 - val_loss: 6726180864.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5966055424.0000 - val_loss: 6704881664.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5941833216.0000 - val_loss: 6683629568.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5918850048.0000 - val_loss: 6661285888.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5896011776.0000 - val_loss: 6637821440.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5873822208.0000 - val_loss: 6617694720.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5852745728.0000 - val_loss: 6599620608.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5831759360.0000 - val_loss: 6580299776.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5810800128.0000 - val_loss: 6557505024.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5791030784.0000 - val_loss: 6539516928.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5771226624.0000 - val_loss: 6520748032.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5752206848.0000 - val_loss: 6504249344.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5733597696.0000 - val_loss: 6487571968.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5715340288.0000 - val_loss: 6470934528.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5697781760.0000 - val_loss: 6452112384.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5680232448.0000 - val_loss: 6437819904.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5663438336.0000 - val_loss: 6420838400.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5646608384.0000 - val_loss: 6403604480.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5630640640.0000 - val_loss: 6389028864.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5614984192.0000 - val_loss: 6373964288.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5599323136.0000 - val_loss: 6357361664.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5584966144.0000 - val_loss: 6344951296.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5570232832.0000 - val_loss: 6331548672.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5556110336.0000 - val_loss: 6318881280.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5541903360.0000 - val_loss: 6309227520.0000\n",
            "178/178 [==============================] - 0s 2ms/step - loss: 8112569344.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 25467097088.0000 - val_loss: 15205843968.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12680832000.0000 - val_loss: 12542688256.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11843298304.0000 - val_loss: 12146339840.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11636941824.0000 - val_loss: 11972938752.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11469965312.0000 - val_loss: 11825456128.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11310159872.0000 - val_loss: 11691681792.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11155881984.0000 - val_loss: 11567547392.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 11006731264.0000 - val_loss: 11448290304.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10862387200.0000 - val_loss: 11336429568.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10723023872.0000 - val_loss: 11225099264.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10588185600.0000 - val_loss: 11115725824.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10457404416.0000 - val_loss: 11000834048.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 10331190272.0000 - val_loss: 10888965120.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10209067008.0000 - val_loss: 10806797312.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 10090917888.0000 - val_loss: 10721861632.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 9976320000.0000 - val_loss: 10630602752.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9865508864.0000 - val_loss: 10510612480.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9758119936.0000 - val_loss: 10427689984.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9654139904.0000 - val_loss: 10354283520.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9552858112.0000 - val_loss: 10275648512.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9455495168.0000 - val_loss: 10190931968.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9360302080.0000 - val_loss: 10119541760.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9268083712.0000 - val_loss: 10036728832.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9178782720.0000 - val_loss: 9980079104.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9092297728.0000 - val_loss: 9910817792.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9008498688.0000 - val_loss: 9836979200.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8927476736.0000 - val_loss: 9760547840.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8848069632.0000 - val_loss: 9702374400.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8771391488.0000 - val_loss: 9642567680.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 8696908800.0000 - val_loss: 9563783168.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8624283648.0000 - val_loss: 9527363584.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 8554654720.0000 - val_loss: 9463098368.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 8486645248.0000 - val_loss: 9389243392.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8420346880.0000 - val_loss: 9350027264.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8356304384.0000 - val_loss: 9291778048.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8294336512.0000 - val_loss: 9233215488.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8233830400.0000 - val_loss: 9173208064.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8174048768.0000 - val_loss: 9144389632.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8117526016.0000 - val_loss: 9065586688.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8062003712.0000 - val_loss: 9022134272.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8007527424.0000 - val_loss: 8965800960.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7955446784.0000 - val_loss: 8926570496.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7904412160.0000 - val_loss: 8874550272.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7854331904.0000 - val_loss: 8842737664.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7805446656.0000 - val_loss: 8816098304.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7759246848.0000 - val_loss: 8762183680.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7713615872.0000 - val_loss: 8722377728.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7669410304.0000 - val_loss: 8673052672.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7626430976.0000 - val_loss: 8632948736.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7584092672.0000 - val_loss: 8573894144.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7543695872.0000 - val_loss: 8541854720.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7503485440.0000 - val_loss: 8507372032.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7465456128.0000 - val_loss: 8465776128.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7427436544.0000 - val_loss: 8433252864.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7391049216.0000 - val_loss: 8399657984.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7355238912.0000 - val_loss: 8363866624.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7320610816.0000 - val_loss: 8324623872.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7287073792.0000 - val_loss: 8280472576.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7254204416.0000 - val_loss: 8247922176.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7222136832.0000 - val_loss: 8223657984.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7190834688.0000 - val_loss: 8180445184.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7160563712.0000 - val_loss: 8140467200.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7130778112.0000 - val_loss: 8124961280.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7101783552.0000 - val_loss: 8085654016.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7073817088.0000 - val_loss: 8062071808.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 7046304256.0000 - val_loss: 8044807680.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7019482112.0000 - val_loss: 8017169408.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6993587200.0000 - val_loss: 7966896640.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6968055808.0000 - val_loss: 7945504256.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6943171072.0000 - val_loss: 7940125696.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6919543296.0000 - val_loss: 7892329472.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6896027648.0000 - val_loss: 7857426432.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6873032192.0000 - val_loss: 7821717504.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6850521088.0000 - val_loss: 7804763648.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6828924416.0000 - val_loss: 7775647744.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6807696384.0000 - val_loss: 7742046208.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6786767872.0000 - val_loss: 7722523136.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6766709760.0000 - val_loss: 7691262464.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6746632192.0000 - val_loss: 7663058944.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6727667200.0000 - val_loss: 7660551680.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6708859904.0000 - val_loss: 7637375488.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6690533376.0000 - val_loss: 7601106944.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6672412160.0000 - val_loss: 7594650624.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6655422464.0000 - val_loss: 7560219136.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6638303744.0000 - val_loss: 7530124800.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6621575168.0000 - val_loss: 7503031808.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6605209600.0000 - val_loss: 7484242432.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6589199872.0000 - val_loss: 7468834816.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6574078464.0000 - val_loss: 7437384704.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6558979584.0000 - val_loss: 7420835328.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6544443392.0000 - val_loss: 7397920256.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6530011648.0000 - val_loss: 7394655744.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6516147712.0000 - val_loss: 7371527168.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6502441472.0000 - val_loss: 7340402688.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6488558592.0000 - val_loss: 7340998656.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6475718144.0000 - val_loss: 7328821248.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6463143936.0000 - val_loss: 7291254272.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6450457600.0000 - val_loss: 7250845184.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6438358528.0000 - val_loss: 7234577408.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6426134016.0000 - val_loss: 7209584128.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 8866098176.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 2s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 2s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 2s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 2s 4ms/step - loss: nan - val_loss: nan\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: nan - val_loss: nan\n",
            "178/178 [==============================] - 1s 2ms/step - loss: nan\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 14409616652469534720.0000 - val_loss: 2868428536807424.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2133648081944576.0000 - val_loss: 1530437641961472.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1138482416189440.0000 - val_loss: 816531801899008.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 607476080705536.0000 - val_loss: 435632459481088.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 324145443241984.0000 - val_loss: 232406317531136.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 172964288921600.0000 - val_loss: 123982653161472.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 92297815392256.0000 - val_loss: 66136645304320.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 49254856916992.0000 - val_loss: 35277494026240.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: 26287963897856.0000 - val_loss: 18816841023488.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 14034067783680.0000 - val_loss: 10037570306048.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7495533723648.0000 - val_loss: 5355032018944.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4006363791360.0000 - val_loss: 2858329833472.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 2144677920768.0000 - val_loss: 1527505616896.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 1151367774208.0000 - val_loss: 818383355904.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 621345112064.0000 - val_loss: 440727339008.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 338569101312.0000 - val_loss: 239678685184.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 187656978432.0000 - val_loss: 132791214080.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 107141644288.0000 - val_loss: 76055879680.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 64196833280.0000 - val_loss: 46025732096.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 41332076544.0000 - val_loss: 30142343168.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 29111615488.0000 - val_loss: 21758429184.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 22580170752.0000 - val_loss: 17370953728.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 19108966400.0000 - val_loss: 15095919616.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 17260410880.0000 - val_loss: 13924676608.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 16274663424.0000 - val_loss: 13325579264.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15746547712.0000 - val_loss: 13032204288.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15468893184.0000 - val_loss: 12890163200.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15319410688.0000 - val_loss: 12826625024.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15239420928.0000 - val_loss: 12800510976.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15196315648.0000 - val_loss: 12792221696.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15172559872.0000 - val_loss: 12792581120.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15160437760.0000 - val_loss: 12796033024.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15153691648.0000 - val_loss: 12800642048.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15149851648.0000 - val_loss: 12805106688.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15147888640.0000 - val_loss: 12808230912.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15147028480.0000 - val_loss: 12811788288.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15146308608.0000 - val_loss: 12814621696.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 15145997312.0000 - val_loss: 12815234048.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 15145942016.0000 - val_loss: 12816446464.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 15145873408.0000 - val_loss: 12817954816.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 10103923712.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 1697026135818240.0000 - val_loss: 442192592896.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 334622097408.0000 - val_loss: 241111482368.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 184318001152.0000 - val_loss: 134131630080.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 104180482048.0000 - val_loss: 77134233600.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 61363556352.0000 - val_loss: 46844928000.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 38534348800.0000 - val_loss: 30743758848.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 26334853120.0000 - val_loss: 22221447168.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 19830648832.0000 - val_loss: 17715245056.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 16362345472.0000 - val_loss: 15338792960.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14505878528.0000 - val_loss: 14095131648.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 13516081152.0000 - val_loss: 13442113536.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12985468928.0000 - val_loss: 13112240128.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12705896448.0000 - val_loss: 12945154048.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12555840512.0000 - val_loss: 12859755520.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12474419200.0000 - val_loss: 12819132416.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12430735360.0000 - val_loss: 12800300032.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12406700032.0000 - val_loss: 12793456640.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12394439680.0000 - val_loss: 12791815168.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12388111360.0000 - val_loss: 12792340480.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12384704512.0000 - val_loss: 12793598976.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12382978048.0000 - val_loss: 12795008000.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381995008.0000 - val_loss: 12796335104.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381481984.0000 - val_loss: 12797761536.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381186048.0000 - val_loss: 12798873600.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12381032448.0000 - val_loss: 12799171584.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12380932096.0000 - val_loss: 12799990784.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12380948480.0000 - val_loss: 12800737280.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12380991488.0000 - val_loss: 12800729088.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 15599942656.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 3229169993266495488.0000 - val_loss: 591948767297536.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 440075166941184.0000 - val_loss: 315896220352512.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 234821615878144.0000 - val_loss: 168588774211584.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 125301560442880.0000 - val_loss: 89981158686720.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 66864038281216.0000 - val_loss: 48033295237120.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 35683775283200.0000 - val_loss: 25646113751040.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 19045898256384.0000 - val_loss: 13698222522368.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 10168644403200.0000 - val_loss: 7320937955328.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5431890542592.0000 - val_loss: 3916593627136.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 2904311726080.0000 - val_loss: 2098937593856.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 1555701170176.0000 - val_loss: 1128109178880.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 836081352704.0000 - val_loss: 609651654656.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 452142694400.0000 - val_loss: 332438765568.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 247212687360.0000 - val_loss: 184279449600.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 137907748864.0000 - val_loss: 104979808256.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 79583977472.0000 - val_loss: 62486949888.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 48445480960.0000 - val_loss: 39652515840.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 31809548288.0000 - val_loss: 27352508416.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 22914250752.0000 - val_loss: 20746350592.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 18190985216.0000 - val_loss: 17179410432.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15666920448.0000 - val_loss: 15228862464.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 14318125056.0000 - val_loss: 14174670848.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13606010880.0000 - val_loss: 13591511040.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 13222595584.0000 - val_loss: 13264022528.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 13017482240.0000 - val_loss: 13078768640.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12907828224.0000 - val_loss: 12972917760.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12849932288.0000 - val_loss: 12909869056.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12817816576.0000 - val_loss: 12872567808.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12801348608.0000 - val_loss: 12849641472.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12792321024.0000 - val_loss: 12835867648.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12787890176.0000 - val_loss: 12827486208.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12785425408.0000 - val_loss: 12821837824.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12784095232.0000 - val_loss: 12817336320.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12783284224.0000 - val_loss: 12815157248.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782931968.0000 - val_loss: 12812673024.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782691328.0000 - val_loss: 12810728448.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782477312.0000 - val_loss: 12809724928.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782385152.0000 - val_loss: 12808849408.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782400512.0000 - val_loss: 12808474624.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782371840.0000 - val_loss: 12808012800.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: 12782334976.0000 - val_loss: 12808076288.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782349312.0000 - val_loss: 12808340480.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782295040.0000 - val_loss: 12808715264.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782410752.0000 - val_loss: 12808643584.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782344192.0000 - val_loss: 12809040896.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782401536.0000 - val_loss: 12808421376.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782392320.0000 - val_loss: 12808039424.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782388224.0000 - val_loss: 12807866368.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782360576.0000 - val_loss: 12807731200.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782328832.0000 - val_loss: 12807632896.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782364672.0000 - val_loss: 12807475200.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782361600.0000 - val_loss: 12807530496.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782348288.0000 - val_loss: 12808076288.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782344192.0000 - val_loss: 12807755776.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782398464.0000 - val_loss: 12807068672.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 4ms/step - loss: 12782275584.0000 - val_loss: 12807954432.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782404608.0000 - val_loss: 12808018944.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12782368768.0000 - val_loss: 12807510016.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782375936.0000 - val_loss: 12808094720.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782338048.0000 - val_loss: 12808257536.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782351360.0000 - val_loss: 12808594432.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782374912.0000 - val_loss: 12808182784.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782269440.0000 - val_loss: 12808718336.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782381056.0000 - val_loss: 12808101888.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 12782367744.0000 - val_loss: 12808075264.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 14880906240.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 17635389440.0000 - val_loss: 11653229568.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 13084508160.0000 - val_loss: 10928023552.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 12161667072.0000 - val_loss: 10277218304.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 11387731968.0000 - val_loss: 9811961856.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 10725748736.0000 - val_loss: 9326311424.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 10156103680.0000 - val_loss: 8954129408.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9667481600.0000 - val_loss: 8619384832.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9245188096.0000 - val_loss: 8377314816.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8876523520.0000 - val_loss: 8005319680.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8553952256.0000 - val_loss: 7818191872.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8272762368.0000 - val_loss: 7590327808.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8029767680.0000 - val_loss: 7420141568.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7813402624.0000 - val_loss: 7299449344.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7624879616.0000 - val_loss: 7111329280.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7459692544.0000 - val_loss: 6951319040.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7315667968.0000 - val_loss: 6856912384.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7187560448.0000 - val_loss: 6721666560.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7075004416.0000 - val_loss: 6639702016.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6974232064.0000 - val_loss: 6589913600.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6885725184.0000 - val_loss: 6503522304.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6807774208.0000 - val_loss: 6433423360.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6738280960.0000 - val_loss: 6375550464.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6674828800.0000 - val_loss: 6333991936.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6620202496.0000 - val_loss: 6301733888.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6571332096.0000 - val_loss: 6270427136.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6527793664.0000 - val_loss: 6213485056.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6487753216.0000 - val_loss: 6194525696.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6452764672.0000 - val_loss: 6099854848.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6421536768.0000 - val_loss: 6084327936.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6392027648.0000 - val_loss: 6078569984.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6365166592.0000 - val_loss: 6060019200.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6343353344.0000 - val_loss: 6012428288.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6321330688.0000 - val_loss: 6059381760.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6301929984.0000 - val_loss: 5981359104.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6283577344.0000 - val_loss: 5954325504.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6266577408.0000 - val_loss: 5933524480.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6251302912.0000 - val_loss: 5933011456.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6237284352.0000 - val_loss: 5915002368.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6223971328.0000 - val_loss: 5918735360.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6212042752.0000 - val_loss: 5903913984.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6198173696.0000 - val_loss: 5911228928.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6184064512.0000 - val_loss: 5841301504.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6179970560.0000 - val_loss: 5868462080.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6167981568.0000 - val_loss: 5859729920.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6158691328.0000 - val_loss: 5816979456.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6152287232.0000 - val_loss: 5892300800.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6143150080.0000 - val_loss: 5853916672.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6134110720.0000 - val_loss: 5866080768.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6126852096.0000 - val_loss: 5819794432.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6119458304.0000 - val_loss: 5790452224.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6110546432.0000 - val_loss: 5860419072.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6106359808.0000 - val_loss: 5820327424.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6098721280.0000 - val_loss: 5814334976.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6092343296.0000 - val_loss: 5792997376.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6086634496.0000 - val_loss: 5765304832.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6080049152.0000 - val_loss: 5803408384.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6071534080.0000 - val_loss: 5737831424.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6069319168.0000 - val_loss: 5753794560.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6061474816.0000 - val_loss: 5737796608.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6057602048.0000 - val_loss: 5736155136.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6052397568.0000 - val_loss: 5735880704.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6047307776.0000 - val_loss: 5732197376.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6042327552.0000 - val_loss: 5753041920.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6037628416.0000 - val_loss: 5742354944.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6031242752.0000 - val_loss: 5709298688.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6027670016.0000 - val_loss: 5716436992.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6021850624.0000 - val_loss: 5735093760.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6017324032.0000 - val_loss: 5707784192.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6012608000.0000 - val_loss: 5692307968.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6008961024.0000 - val_loss: 5712998912.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6004924416.0000 - val_loss: 5672197632.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5998939136.0000 - val_loss: 5714039296.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5996473344.0000 - val_loss: 5658800128.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5991833088.0000 - val_loss: 5674900480.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5986195968.0000 - val_loss: 5640311296.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5984052736.0000 - val_loss: 5656310784.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5979824128.0000 - val_loss: 5634733056.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5976057344.0000 - val_loss: 5637126656.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5972271104.0000 - val_loss: 5642806784.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5968602624.0000 - val_loss: 5646624768.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5963549184.0000 - val_loss: 5600550912.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5961500672.0000 - val_loss: 5651105792.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5957440512.0000 - val_loss: 5645438464.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5952024576.0000 - val_loss: 5665487360.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5950215168.0000 - val_loss: 5589926400.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5947570176.0000 - val_loss: 5598487552.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5943567360.0000 - val_loss: 5618607616.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5939874304.0000 - val_loss: 5584219648.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5936400384.0000 - val_loss: 5582396928.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5933685760.0000 - val_loss: 5598318080.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5929757184.0000 - val_loss: 5598088704.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5926492160.0000 - val_loss: 5597607424.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5924934144.0000 - val_loss: 5593743872.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5919318528.0000 - val_loss: 5553954304.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5916829696.0000 - val_loss: 5559711232.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5914963456.0000 - val_loss: 5562378240.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5911557120.0000 - val_loss: 5589203456.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5907968512.0000 - val_loss: 5548035584.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5905950720.0000 - val_loss: 5550616064.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5902928896.0000 - val_loss: 5547809280.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 4951275520.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15119971328.0000 - val_loss: 11748043776.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10908134400.0000 - val_loss: 11074875392.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 10231583744.0000 - val_loss: 10485340160.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 9636372480.0000 - val_loss: 9940950016.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 9121297408.0000 - val_loss: 9476536320.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 8661425152.0000 - val_loss: 9070066688.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 8257123840.0000 - val_loss: 8708722688.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7898239488.0000 - val_loss: 8385925632.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7581391360.0000 - val_loss: 8103414272.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7299620352.0000 - val_loss: 7855378432.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7050119680.0000 - val_loss: 7635385856.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6828728832.0000 - val_loss: 7430611968.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6632889344.0000 - val_loss: 7256755712.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6457567232.0000 - val_loss: 7099301888.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6303561216.0000 - val_loss: 6962244608.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6164742656.0000 - val_loss: 6834252288.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6040966656.0000 - val_loss: 6729083904.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5930478080.0000 - val_loss: 6636160000.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5832196096.0000 - val_loss: 6537261568.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5745068544.0000 - val_loss: 6470071808.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5665506304.0000 - val_loss: 6398287872.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5595757056.0000 - val_loss: 6339026432.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5530741248.0000 - val_loss: 6267543552.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5474495488.0000 - val_loss: 6218853376.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5422059520.0000 - val_loss: 6197264384.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5376715776.0000 - val_loss: 6128903680.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5334791680.0000 - val_loss: 6093806592.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5296587264.0000 - val_loss: 6070136320.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5262088192.0000 - val_loss: 6033003520.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5230932480.0000 - val_loss: 6005332480.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5202551808.0000 - val_loss: 5971064320.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5176310784.0000 - val_loss: 5958713344.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5153386496.0000 - val_loss: 5929614336.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5130636288.0000 - val_loss: 5925863936.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5110272000.0000 - val_loss: 5892958208.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5091442688.0000 - val_loss: 5869700608.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5073464832.0000 - val_loss: 5843330048.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5058817536.0000 - val_loss: 5839036416.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5041799680.0000 - val_loss: 5838443008.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5028552192.0000 - val_loss: 5825648640.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5016036864.0000 - val_loss: 5817098240.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5003917312.0000 - val_loss: 5790150656.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4991793664.0000 - val_loss: 5768438272.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4981207552.0000 - val_loss: 5788350464.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4970685952.0000 - val_loss: 5753030144.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4958883840.0000 - val_loss: 5738311168.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4951909888.0000 - val_loss: 5736545280.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4942501376.0000 - val_loss: 5727972352.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4933499392.0000 - val_loss: 5721668096.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4924931072.0000 - val_loss: 5729135104.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4917500928.0000 - val_loss: 5715328000.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4909568000.0000 - val_loss: 5688939520.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4902772224.0000 - val_loss: 5700317184.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4894579200.0000 - val_loss: 5683912704.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 4888141824.0000 - val_loss: 5689856000.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 4880879616.0000 - val_loss: 5674852864.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 4874207744.0000 - val_loss: 5655371776.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 4868379136.0000 - val_loss: 5659217920.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4861829120.0000 - val_loss: 5640636928.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4855528960.0000 - val_loss: 5668364288.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4847979520.0000 - val_loss: 5653324288.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4843504640.0000 - val_loss: 5621627392.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4837957632.0000 - val_loss: 5649739264.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4832996864.0000 - val_loss: 5616207360.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4825363968.0000 - val_loss: 5650875904.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4821905408.0000 - val_loss: 5602630656.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4816100352.0000 - val_loss: 5608059392.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4810842112.0000 - val_loss: 5599794176.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4805954048.0000 - val_loss: 5601912832.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4800223744.0000 - val_loss: 5610728960.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4795764736.0000 - val_loss: 5591813632.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 4790682112.0000 - val_loss: 5578474496.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 4785850368.0000 - val_loss: 5580584448.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 4780805632.0000 - val_loss: 5571118592.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 4776233984.0000 - val_loss: 5554626048.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4772202496.0000 - val_loss: 5558246400.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4766795264.0000 - val_loss: 5588701696.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4763307008.0000 - val_loss: 5567762432.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4758448128.0000 - val_loss: 5551703552.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4753444352.0000 - val_loss: 5563436544.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4749309440.0000 - val_loss: 5527859712.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4745134080.0000 - val_loss: 5539471872.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4740921856.0000 - val_loss: 5540922880.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4736167936.0000 - val_loss: 5515965440.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4733086208.0000 - val_loss: 5535607296.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4729247232.0000 - val_loss: 5535436800.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4725162496.0000 - val_loss: 5517333504.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4720596992.0000 - val_loss: 5540814336.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 4717928960.0000 - val_loss: 5513199104.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 4713038848.0000 - val_loss: 5522214400.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 4710468608.0000 - val_loss: 5497453568.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 4705804800.0000 - val_loss: 5507348480.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4702373888.0000 - val_loss: 5494382592.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4698298368.0000 - val_loss: 5499651584.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4694818304.0000 - val_loss: 5473421824.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4688875008.0000 - val_loss: 5510477824.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4688322560.0000 - val_loss: 5490477568.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4684161024.0000 - val_loss: 5498226688.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4681354752.0000 - val_loss: 5492703744.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 4677943296.0000 - val_loss: 5473661952.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 7140783104.0000\n",
            "Epoch 1/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 15045303296.0000 - val_loss: 11905569792.0000\n",
            "Epoch 2/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 11138289664.0000 - val_loss: 11334397952.0000\n",
            "Epoch 3/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 10513021952.0000 - val_loss: 10887820288.0000\n",
            "Epoch 4/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 9972313088.0000 - val_loss: 10433856512.0000\n",
            "Epoch 5/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 9501213696.0000 - val_loss: 9996357632.0000\n",
            "Epoch 6/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 9096276992.0000 - val_loss: 9768477696.0000\n",
            "Epoch 7/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8740905984.0000 - val_loss: 9511898112.0000\n",
            "Epoch 8/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8429330432.0000 - val_loss: 9262354432.0000\n",
            "Epoch 9/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 8153683968.0000 - val_loss: 8903980032.0000\n",
            "Epoch 10/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7914848768.0000 - val_loss: 8832198656.0000\n",
            "Epoch 11/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7701449728.0000 - val_loss: 8697060352.0000\n",
            "Epoch 12/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7515938816.0000 - val_loss: 8369335808.0000\n",
            "Epoch 13/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7351212544.0000 - val_loss: 8327053312.0000\n",
            "Epoch 14/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7204264960.0000 - val_loss: 8170065408.0000\n",
            "Epoch 15/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 7071521792.0000 - val_loss: 8057083392.0000\n",
            "Epoch 16/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6954531840.0000 - val_loss: 7875384320.0000\n",
            "Epoch 17/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6852266496.0000 - val_loss: 7754166272.0000\n",
            "Epoch 18/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6758906880.0000 - val_loss: 7617219584.0000\n",
            "Epoch 19/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6674191360.0000 - val_loss: 7535736320.0000\n",
            "Epoch 20/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6601092096.0000 - val_loss: 7476601344.0000\n",
            "Epoch 21/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6534967808.0000 - val_loss: 7359691264.0000\n",
            "Epoch 22/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6472740864.0000 - val_loss: 7303827456.0000\n",
            "Epoch 23/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6420127744.0000 - val_loss: 7170523648.0000\n",
            "Epoch 24/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6370351616.0000 - val_loss: 7103768064.0000\n",
            "Epoch 25/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6323825664.0000 - val_loss: 7100794368.0000\n",
            "Epoch 26/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6286577664.0000 - val_loss: 6918402560.0000\n",
            "Epoch 27/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6250503168.0000 - val_loss: 6935373824.0000\n",
            "Epoch 28/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6216702464.0000 - val_loss: 6881697792.0000\n",
            "Epoch 29/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6187031040.0000 - val_loss: 6778834944.0000\n",
            "Epoch 30/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6159638016.0000 - val_loss: 6817408512.0000\n",
            "Epoch 31/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6135265280.0000 - val_loss: 6717225984.0000\n",
            "Epoch 32/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6112348160.0000 - val_loss: 6624276992.0000\n",
            "Epoch 33/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6091786752.0000 - val_loss: 6608622080.0000\n",
            "Epoch 34/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6070320128.0000 - val_loss: 6519995392.0000\n",
            "Epoch 35/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 6053189120.0000 - val_loss: 6477196800.0000\n",
            "Epoch 36/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6038612992.0000 - val_loss: 6527896576.0000\n",
            "Epoch 37/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6023022592.0000 - val_loss: 6490726912.0000\n",
            "Epoch 38/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 6007496192.0000 - val_loss: 6394909696.0000\n",
            "Epoch 39/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5994433024.0000 - val_loss: 6371708928.0000\n",
            "Epoch 40/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5982706688.0000 - val_loss: 6370852864.0000\n",
            "Epoch 41/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5971532288.0000 - val_loss: 6317894144.0000\n",
            "Epoch 42/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5960128000.0000 - val_loss: 6331252224.0000\n",
            "Epoch 43/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5950180864.0000 - val_loss: 6308845056.0000\n",
            "Epoch 44/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5940198912.0000 - val_loss: 6244032512.0000\n",
            "Epoch 45/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5930935296.0000 - val_loss: 6248692224.0000\n",
            "Epoch 46/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5921930752.0000 - val_loss: 6208795136.0000\n",
            "Epoch 47/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5913432576.0000 - val_loss: 6159198208.0000\n",
            "Epoch 48/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5905947648.0000 - val_loss: 6143565312.0000\n",
            "Epoch 49/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5898842112.0000 - val_loss: 6163670016.0000\n",
            "Epoch 50/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5891227136.0000 - val_loss: 6123965440.0000\n",
            "Epoch 51/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5884097536.0000 - val_loss: 6105150464.0000\n",
            "Epoch 52/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5877644288.0000 - val_loss: 6131195904.0000\n",
            "Epoch 53/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5871176192.0000 - val_loss: 6094007296.0000\n",
            "Epoch 54/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5865997824.0000 - val_loss: 6068345344.0000\n",
            "Epoch 55/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5857621504.0000 - val_loss: 6077395456.0000\n",
            "Epoch 56/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5853813248.0000 - val_loss: 6014075392.0000\n",
            "Epoch 57/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5847790080.0000 - val_loss: 6030077952.0000\n",
            "Epoch 58/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5842475520.0000 - val_loss: 6004169216.0000\n",
            "Epoch 59/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5836560896.0000 - val_loss: 5989008384.0000\n",
            "Epoch 60/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5832488448.0000 - val_loss: 5999456256.0000\n",
            "Epoch 61/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5826478080.0000 - val_loss: 5943866880.0000\n",
            "Epoch 62/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5819331072.0000 - val_loss: 5908937728.0000\n",
            "Epoch 63/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5817231872.0000 - val_loss: 5939865088.0000\n",
            "Epoch 64/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5812052480.0000 - val_loss: 5908815360.0000\n",
            "Epoch 65/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5806622208.0000 - val_loss: 5963441664.0000\n",
            "Epoch 66/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5802494976.0000 - val_loss: 5911165440.0000\n",
            "Epoch 67/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5797711360.0000 - val_loss: 5930100736.0000\n",
            "Epoch 68/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5793500672.0000 - val_loss: 5887850496.0000\n",
            "Epoch 69/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5789572096.0000 - val_loss: 5879597056.0000\n",
            "Epoch 70/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5784288768.0000 - val_loss: 5857111552.0000\n",
            "Epoch 71/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5778974720.0000 - val_loss: 5876322816.0000\n",
            "Epoch 72/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5776710144.0000 - val_loss: 5840414208.0000\n",
            "Epoch 73/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5772090880.0000 - val_loss: 5835991040.0000\n",
            "Epoch 74/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5767989760.0000 - val_loss: 5852898304.0000\n",
            "Epoch 75/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5764592128.0000 - val_loss: 5822953472.0000\n",
            "Epoch 76/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5760372224.0000 - val_loss: 5862576640.0000\n",
            "Epoch 77/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5757681664.0000 - val_loss: 5821249536.0000\n",
            "Epoch 78/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5751058432.0000 - val_loss: 5849524736.0000\n",
            "Epoch 79/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5748619776.0000 - val_loss: 5799210496.0000\n",
            "Epoch 80/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5745472512.0000 - val_loss: 5810493952.0000\n",
            "Epoch 81/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5740315648.0000 - val_loss: 5762788352.0000\n",
            "Epoch 82/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5737532416.0000 - val_loss: 5763085312.0000\n",
            "Epoch 83/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5733799424.0000 - val_loss: 5763583488.0000\n",
            "Epoch 84/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5730084864.0000 - val_loss: 5731761664.0000\n",
            "Epoch 85/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5727580672.0000 - val_loss: 5748481536.0000\n",
            "Epoch 86/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5721843200.0000 - val_loss: 5735381504.0000\n",
            "Epoch 87/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5719506432.0000 - val_loss: 5722099712.0000\n",
            "Epoch 88/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5715654144.0000 - val_loss: 5750877696.0000\n",
            "Epoch 89/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5714114048.0000 - val_loss: 5729727488.0000\n",
            "Epoch 90/100\n",
            "355/355 [==============================] - 1s 3ms/step - loss: 5709638656.0000 - val_loss: 5705067520.0000\n",
            "Epoch 91/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5706566656.0000 - val_loss: 5707986432.0000\n",
            "Epoch 92/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5703562752.0000 - val_loss: 5704782848.0000\n",
            "Epoch 93/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5699436544.0000 - val_loss: 5713573376.0000\n",
            "Epoch 94/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5695775744.0000 - val_loss: 5694642688.0000\n",
            "Epoch 95/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5693437440.0000 - val_loss: 5670986240.0000\n",
            "Epoch 96/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5690767872.0000 - val_loss: 5679648768.0000\n",
            "Epoch 97/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5686903296.0000 - val_loss: 5659830272.0000\n",
            "Epoch 98/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5684873216.0000 - val_loss: 5666139648.0000\n",
            "Epoch 99/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5681046016.0000 - val_loss: 5640651264.0000\n",
            "Epoch 100/100\n",
            "355/355 [==============================] - 1s 2ms/step - loss: 5677808640.0000 - val_loss: 5669724160.0000\n",
            "178/178 [==============================] - 1s 2ms/step - loss: 5624435200.0000\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [            nan             nan -8.00676506e+09 -1.35275977e+10\n",
            "             nan             nan -7.63022729e+09             nan\n",
            " -1.35282575e+10 -5.90549794e+09]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "532/532 [==============================] - 1s 2ms/step - loss: 14672966656.0000 - val_loss: 11334981632.0000\n",
            "Epoch 2/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 11290016768.0000 - val_loss: 10379143168.0000\n",
            "Epoch 3/100\n",
            "532/532 [==============================] - 1s 3ms/step - loss: 10359275520.0000 - val_loss: 9601369088.0000\n",
            "Epoch 4/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 9595011072.0000 - val_loss: 8960109568.0000\n",
            "Epoch 5/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8966449152.0000 - val_loss: 8423028736.0000\n",
            "Epoch 6/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8441450496.0000 - val_loss: 7983599104.0000\n",
            "Epoch 7/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 8007727104.0000 - val_loss: 7617878528.0000\n",
            "Epoch 8/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7645033472.0000 - val_loss: 7311277056.0000\n",
            "Epoch 9/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7342018048.0000 - val_loss: 7061051904.0000\n",
            "Epoch 10/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 7091064832.0000 - val_loss: 6845651968.0000\n",
            "Epoch 11/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6879347712.0000 - val_loss: 6670633472.0000\n",
            "Epoch 12/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6702829056.0000 - val_loss: 6522866688.0000\n",
            "Epoch 13/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6555336192.0000 - val_loss: 6396283392.0000\n",
            "Epoch 14/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6429341184.0000 - val_loss: 6291622912.0000\n",
            "Epoch 15/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6324178432.0000 - val_loss: 6200891392.0000\n",
            "Epoch 16/100\n",
            "532/532 [==============================] - 1s 3ms/step - loss: 6234046976.0000 - val_loss: 6125500928.0000\n",
            "Epoch 17/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6158370304.0000 - val_loss: 6065203200.0000\n",
            "Epoch 18/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6092965888.0000 - val_loss: 6008921600.0000\n",
            "Epoch 19/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 6037227520.0000 - val_loss: 5962823168.0000\n",
            "Epoch 20/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5989412864.0000 - val_loss: 5923895296.0000\n",
            "Epoch 21/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5946895360.0000 - val_loss: 5888612864.0000\n",
            "Epoch 22/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5910737920.0000 - val_loss: 5852480512.0000\n",
            "Epoch 23/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5878975488.0000 - val_loss: 5827045376.0000\n",
            "Epoch 24/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5850276864.0000 - val_loss: 5804502016.0000\n",
            "Epoch 25/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5825814528.0000 - val_loss: 5778495488.0000\n",
            "Epoch 26/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5803119104.0000 - val_loss: 5757345792.0000\n",
            "Epoch 27/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5781674496.0000 - val_loss: 5740350464.0000\n",
            "Epoch 28/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5763532288.0000 - val_loss: 5722659328.0000\n",
            "Epoch 29/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5747027968.0000 - val_loss: 5707902976.0000\n",
            "Epoch 30/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5731532288.0000 - val_loss: 5692570112.0000\n",
            "Epoch 31/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5716760064.0000 - val_loss: 5679374336.0000\n",
            "Epoch 32/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5702990336.0000 - val_loss: 5680039936.0000\n",
            "Epoch 33/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5688243200.0000 - val_loss: 5655613440.0000\n",
            "Epoch 34/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5678641152.0000 - val_loss: 5646680064.0000\n",
            "Epoch 35/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5666664960.0000 - val_loss: 5631779840.0000\n",
            "Epoch 36/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5656651264.0000 - val_loss: 5623104512.0000\n",
            "Epoch 37/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5646554112.0000 - val_loss: 5611620352.0000\n",
            "Epoch 38/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5636395008.0000 - val_loss: 5608965632.0000\n",
            "Epoch 39/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5626756096.0000 - val_loss: 5593019392.0000\n",
            "Epoch 40/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5616988160.0000 - val_loss: 5591475712.0000\n",
            "Epoch 41/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5610399232.0000 - val_loss: 5578081792.0000\n",
            "Epoch 42/100\n",
            "532/532 [==============================] - 1s 3ms/step - loss: 5599480832.0000 - val_loss: 5573686784.0000\n",
            "Epoch 43/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5593177600.0000 - val_loss: 5560990208.0000\n",
            "Epoch 44/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5584666112.0000 - val_loss: 5552433664.0000\n",
            "Epoch 45/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5577547264.0000 - val_loss: 5544115200.0000\n",
            "Epoch 46/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5569904640.0000 - val_loss: 5538665984.0000\n",
            "Epoch 47/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5561515520.0000 - val_loss: 5543086080.0000\n",
            "Epoch 48/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5555352576.0000 - val_loss: 5526449152.0000\n",
            "Epoch 49/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5547403264.0000 - val_loss: 5515219456.0000\n",
            "Epoch 50/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5540637184.0000 - val_loss: 5515627008.0000\n",
            "Epoch 51/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5533830656.0000 - val_loss: 5508759552.0000\n",
            "Epoch 52/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5527685120.0000 - val_loss: 5495035392.0000\n",
            "Epoch 53/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5519140352.0000 - val_loss: 5512598016.0000\n",
            "Epoch 54/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5515183104.0000 - val_loss: 5482603520.0000\n",
            "Epoch 55/100\n",
            "532/532 [==============================] - 2s 3ms/step - loss: 5507383808.0000 - val_loss: 5482532864.0000\n",
            "Epoch 56/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5502601728.0000 - val_loss: 5470411776.0000\n",
            "Epoch 57/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5495752192.0000 - val_loss: 5468528128.0000\n",
            "Epoch 58/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5490492416.0000 - val_loss: 5459228672.0000\n",
            "Epoch 59/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5485595648.0000 - val_loss: 5459119616.0000\n",
            "Epoch 60/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5480388096.0000 - val_loss: 5452174336.0000\n",
            "Epoch 61/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5474284032.0000 - val_loss: 5443230208.0000\n",
            "Epoch 62/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5469884928.0000 - val_loss: 5436545536.0000\n",
            "Epoch 63/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5463187456.0000 - val_loss: 5431672832.0000\n",
            "Epoch 64/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5458844672.0000 - val_loss: 5428278272.0000\n",
            "Epoch 65/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5453473792.0000 - val_loss: 5424264192.0000\n",
            "Epoch 66/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5449008128.0000 - val_loss: 5416000512.0000\n",
            "Epoch 67/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5444720640.0000 - val_loss: 5411215360.0000\n",
            "Epoch 68/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5438422016.0000 - val_loss: 5411525120.0000\n",
            "Epoch 69/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5433373696.0000 - val_loss: 5403487232.0000\n",
            "Epoch 70/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5429622784.0000 - val_loss: 5397185536.0000\n",
            "Epoch 71/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5425092608.0000 - val_loss: 5392762368.0000\n",
            "Epoch 72/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5420469248.0000 - val_loss: 5388465152.0000\n",
            "Epoch 73/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5415406592.0000 - val_loss: 5383929344.0000\n",
            "Epoch 74/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5411734528.0000 - val_loss: 5379975168.0000\n",
            "Epoch 75/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5408506368.0000 - val_loss: 5375602688.0000\n",
            "Epoch 76/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5404770816.0000 - val_loss: 5371382272.0000\n",
            "Epoch 77/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5400078336.0000 - val_loss: 5367753728.0000\n",
            "Epoch 78/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5395585536.0000 - val_loss: 5367504384.0000\n",
            "Epoch 79/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5392894976.0000 - val_loss: 5359580672.0000\n",
            "Epoch 80/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5388443648.0000 - val_loss: 5359419904.0000\n",
            "Epoch 81/100\n",
            "532/532 [==============================] - 1s 3ms/step - loss: 5384543232.0000 - val_loss: 5355058176.0000\n",
            "Epoch 82/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5380736512.0000 - val_loss: 5348638720.0000\n",
            "Epoch 83/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5376525312.0000 - val_loss: 5358353920.0000\n",
            "Epoch 84/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5374809088.0000 - val_loss: 5344647168.0000\n",
            "Epoch 85/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5369673216.0000 - val_loss: 5342689280.0000\n",
            "Epoch 86/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5366976512.0000 - val_loss: 5334500352.0000\n",
            "Epoch 87/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5363158016.0000 - val_loss: 5338639360.0000\n",
            "Epoch 88/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5359622656.0000 - val_loss: 5327225856.0000\n",
            "Epoch 89/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5355773440.0000 - val_loss: 5324020736.0000\n",
            "Epoch 90/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5351764992.0000 - val_loss: 5321698816.0000\n",
            "Epoch 91/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5349678080.0000 - val_loss: 5319019520.0000\n",
            "Epoch 92/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5347337728.0000 - val_loss: 5314970624.0000\n",
            "Epoch 93/100\n",
            "532/532 [==============================] - 1s 3ms/step - loss: 5344373760.0000 - val_loss: 5311552000.0000\n",
            "Epoch 94/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5340682240.0000 - val_loss: 5309962752.0000\n",
            "Epoch 95/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5337370624.0000 - val_loss: 5307006976.0000\n",
            "Epoch 96/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5334627328.0000 - val_loss: 5303126016.0000\n",
            "Epoch 97/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5330954240.0000 - val_loss: 5301391360.0000\n",
            "Epoch 98/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5329063424.0000 - val_loss: 5297171968.0000\n",
            "Epoch 99/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5326373376.0000 - val_loss: 5295928320.0000\n",
            "Epoch 100/100\n",
            "532/532 [==============================] - 1s 2ms/step - loss: 5322896384.0000 - val_loss: 5292921344.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3,\n",
              "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x7ff7e5b47dc0>,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7ff7e5b47af0>,\n",
              "                                        'n_hidden': [0, 1, 2, 3],\n",
              "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
              "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
              "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
              "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
              "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
              "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x7ff7e5b47dc0&gt;,\n",
              "                   param_distributions={&#x27;learning_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7ff7e5b47af0&gt;,\n",
              "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
              "                                        &#x27;n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
              "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
              "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
              "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
              "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
              "                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x7ff7e5b47dc0&gt;,\n",
              "                   param_distributions={&#x27;learning_rate&#x27;: &lt;scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7ff7e5b47af0&gt;,\n",
              "                                        &#x27;n_hidden&#x27;: [0, 1, 2, 3],\n",
              "                                        &#x27;n_neurons&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
              "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
              "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
              "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
              "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x7ff7e5b47dc0&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x7ff7e5b47dc0&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDyLtOT8dqOB"
      },
      "source": [
        "The above  exploration  may  last  many  hours  depending  on  the  hardware,  the  size  of  the dataset, the complexity of the model and the value of n_iter and cv. When it is over, we can access the best parameters found, the best score, and the trained Keras model like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgsAM812dqOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36a6fc8-80e8-4ccc-b9bf-25ca0f484c16"
      },
      "source": [
        "# finding the best parameters\n",
        "rnd_search_cv.best_params_"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.005308333470513068, 'n_hidden': 0, 'n_neurons': 96}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeOHn8WVdqOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619b7021-e1e6-4769-caf2-434376e52964"
      },
      "source": [
        "# best score\n",
        "rnd_search_cv.best_score_"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-5905497941.333333"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNY1vgDAdqOD"
      },
      "source": [
        "# applying best parameters to the model for predictions\n",
        "model = rnd_search_cv.best_estimator_.model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqut1OR3dqOE"
      },
      "source": [
        "Refer to the guidelines below for choosing the  number  of  hidden  layers  and  neurons  in  an  MLP,  and  selecting  appropriate  values  for some of the main hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9t_szGwdqOF"
      },
      "source": [
        "#### Number of Hidden Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqiYlS6AdqOG"
      },
      "source": [
        "- For simple problems, we can start with just one or two hidden layers and get the accurate results.\n",
        "- For more complex problems, we can gradually rampup the number of hidden layers, until we start overfitting the training set. Very complex  tasks,  such  as  large  image  classification  or  speech  recognition,  typically  require networks  with  dozens  of  layers  (or  even  hundreds,  but  not  fully  connected  ones),  and  they  need  a  huge  amount  of  training  data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZu_Fc1ydqOH"
      },
      "source": [
        "#### Number of Neurons per Hidden Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-1AYlmBdqOI"
      },
      "source": [
        "- We can try increasing the number of neurons gradually  until  the  network  starts  overfitting. \n",
        "- In general, it may be more advantageous to increase  the  number  of  layers  than  the  number  of  neurons  per  layer.\n",
        "- A  simpler  approach  is  to  pick  a  model  with  more  layers  and  neurons  than  we actually need, then use early stopping to prevent it from overfitting (and other regularization  techniques,  such  as  dropout, which we will discuss further in this notebook)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQJXWhRIdqOJ"
      },
      "source": [
        "#### Learning Rate, Batch Size, and Other Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMuxqHl1dqOJ"
      },
      "source": [
        "Here are some of the important hyperparameters other than hidden layers and neurons, and some tips on how to set them:\n",
        "\n",
        "- The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges). So a  simple  approach  for  tuning  the  learning  rate  is  to  start  with  a  large  value  that makes  the  training  algorithm  diverge,  then  divide  this  value  by  3  and  try  again, and repeat until the training algorithm stops diverging.\n",
        "- Choosing  a  better  optimizer  than  plain  old  Mini-batch  Gradient  Descent  (and tuning its hyperparameters) is also quite important. We will discuss this in further sections.\n",
        "- The  batch  size  can  also  have  a  significant  impact  on  our  model’s  performance and the training time. In general the optimal batch size will be lower than 32. We will study batch normalization further in this notebook.\n",
        "- We discussed the choice of the activation function in previous assignment notebook, the $ReLU$ activation function will be a good default for all hidden layers. For the output layer, it really depends on our task.\n",
        "- In  most  cases,  the  number  of  training  iterations  does  not actually  need  to  be tweaked: just use early stopping instead.\n",
        "\n",
        "Let us also take a look at techniques such as Batch normalization, overfitting, drop out, optimizers and learning rate to  train deep neural networks.\n",
        "\n",
        "To know more about hyperparameter tuning of deep neural networks, click [here](https://towardsdatascience.com/the-art-of-hyperparameter-tuning-in-deep-neural-nets-by-example-685cb5429a38)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc_IEVbFdqOK"
      },
      "source": [
        "### Accelerate Learning of Deep Neural Networks With Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRK5Px-qdqOM"
      },
      "source": [
        "Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n",
        "\n",
        "![Image](https://lh3.googleusercontent.com/-9hMD_jyPLuE/YNQoKrc-_4I/AAAAAAAACTs/9nZ-BEdtI-QoAe6R5HXlG6X3AcprX8NaQCJEEGAsYHg/s512/2021-06-23.png)\n",
        "\n",
        "$\\text{Figure: Batch Normalization Algorithm}$\n",
        "\n",
        "So  during  training,  BN  just  standardizes  its  inputs  then  rescales  and  offsets  them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_el24cRdqON"
      },
      "source": [
        "#### Implementing Batch Normalization with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmjykNm-dqOO"
      },
      "source": [
        "Implementing  Batch  Normalization  is  quite  simple, just  add  a  BatchNormalization  layer  before  or  after  each  hidden  layer’s  activation function,  and  optionally  add  a  BN  layer  as  well  as  the  first  layer  in  our  model.  For example,  this  model  applies  BN  after  every  hidden  layer  and  as  the  first  layer  in  the\n",
        "model (after flattening the input images):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUc-PjdHdqOP"
      },
      "source": [
        "# create model with Batch Normalization\n",
        "model = Sequential([\n",
        "                    Flatten(input_shape=[28, 28]),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(10, activation=\"softmax\")\n",
        "                    ])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3u4zONAdqOQ"
      },
      "source": [
        "If we display the model summary, we can see that each BN layer adds 4 parameters per input: γ, β, μ and σ (for example, the first BN layer adds 3136 parameters, which is 4 times 784). The last two parameters, μ and σ, are the moving averages, they are not affected by backpropagation, so Keras calls them “Nontrainable” (if we count the total number of BN parameters, 3136 + 1200 + 400, and divide  by  two,  we get  2,368,  which  is  the  total  number  of  non-trainable  params  in this model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpyQU5LLdqOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7035d593-bc39-407d-b206-2d17e94a138c"
      },
      "source": [
        "# summary of model\n",
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 784)              3136      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_72 (Dense)            (None, 300)               235500    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 300)              1200      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 100)               30100     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 100)              400       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaE1qvfhdqOS"
      },
      "source": [
        "Let’s look at the parameters of the first BN layer. Two are trainable (by backprop), and two are not:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTpS00QzdqOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba30624f-e5f6-47f9-94c3-8c19a65abb6f"
      },
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('batch_normalization/gamma:0', True),\n",
              " ('batch_normalization/beta:0', True),\n",
              " ('batch_normalization/moving_mean:0', False),\n",
              " ('batch_normalization/moving_variance:0', False)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6zEJrikdqOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509c193a-138b-45df-c5b0-30d5bf482782"
      },
      "source": [
        "model.layers[1].updates"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-11fe563bf3e2>:1: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  model.layers[1].updates\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ku_jrTpdqOU"
      },
      "source": [
        "Moreover,  since  a  Batch  Normalization layer includes one offset parameter per input, we can remove the bias term from the previous layer (just pass `use_bias=False` when creating it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWeHUnCvdqOV"
      },
      "source": [
        "# create model\n",
        "model = Sequential([\n",
        "                    Flatten(input_shape=[28, 28]),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "                    BatchNormalization(),\n",
        "                    Activation(\"relu\"),\n",
        "                    Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "                    Activation(\"relu\"),\n",
        "                    BatchNormalization(),\n",
        "                    Dense(10, activation=\"softmax\")\n",
        "                    ])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nQq1BuIdqOW"
      },
      "source": [
        "The  BatchNormalization class has regularizable hyperparameters. Tweaking the “momentum” argument allows us to control how much of the statistics from the previous mini batch to include when the update is calculated.\n",
        "\n",
        "\n",
        "A good momentum value is typically close to 1, for example, 0.9, 0.99, or 0.999 \n",
        "\n",
        "To know more about batch normalization, click [here](https://towardsdatascience.com/batch-normalization-in-practice-an-example-with-keras-and-tensorflow-2-0-b1ec28bde96f)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvBrjDXmdqOX"
      },
      "source": [
        "###  Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tv15JM0dqOX"
      },
      "source": [
        "Some popular optimizers used for boosting the speed in training large deep neural networks are: Momentum optimization, RMSProp, and Adam optimization. Refer [here](https://mlfromscratch.com/optimizers-explained/#/) for a detailed understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA4iM1aQdqOY"
      },
      "source": [
        "#### Momentum Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz-Y0M-SdqOZ"
      },
      "source": [
        "Momentum  optimization subtracts  the  local  gradient  from  the  momentum  vector  m  (multiplied  by  the  learning  rate  η),  and  it  updates  the  weights  by  simply  adding  this momentum vector, thus accelerating the speed. The momentum hyperparameter $β$ is introduced to prevent  the momentum from growing too large (set between 0 and 1, typically 0.9).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz1B-VDedqOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4e9826-aafa-4e46-e4fa-4707311bc12e"
      },
      "source": [
        "#Implementing the momentum optimizer\n",
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFp9_IkfdqOb"
      },
      "source": [
        "#### RMSProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cWLQD2hdqOc"
      },
      "source": [
        "The RMSProp algorithm fixes only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step. \n",
        "\n",
        "The decay rate $β$ is typically set to 0.9. Yes, it is once again a new hyperparameter, but this default value often works well, so we may not need to tune it at all.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ta7gsJFdqOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2a6725-e39f-472f-c4e7-b8223ae410b3"
      },
      "source": [
        "#Implementing the RMSProp optimizer\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1vPNNs7dqOe"
      },
      "source": [
        "#### Adam Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMvwuAridqOf"
      },
      "source": [
        "Adam combines the ideas of Momentum  optimization  and  RMSProp:  it keeps track of both, an  exponentially  decaying  average  of  past  gradients,  and  an  exponentially  decaying  average  of  past  squared  gradients.\n",
        "\n",
        "The momentum decay hyperparameter $β_1$ is typically initialized to 0.9, while the scaling  decay  hyperparameter  $β_2$  is  often  initialized  to  0.999."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EfFHwAFdqOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12c345ec-932d-4f0f-8547-d57edf7573e5"
      },
      "source": [
        "#Implementing the Adam optimizer\n",
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYIwICgtdqOh"
      },
      "source": [
        "To know more about optimizers, click [here](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6#:~:text=Optimizers%20are%20algorithms%20or%20methods,help%20to%20get%20results%20faster)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt5i_U0udqOh"
      },
      "source": [
        "#### Learning Rate Schedule For Training Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3ln8QNTdqOi"
      },
      "source": [
        "The simplest adaptation of learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdmHxgkMdqOj"
      },
      "source": [
        "##### Time-Based Learning Rate Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI2I4w5ZdqOk"
      },
      "source": [
        "Keras has an in-built time-based learning rate schedule function.\n",
        "\n",
        "The decay argument in the stochastic gradient descent optimization algorithm  is used in the time-based learning rate decay schedule equation as follows:\n",
        "\n",
        "- LearningRate = LearningRate * $\\frac{1}{(1 + decay * epoch)}$\n",
        "\n",
        "When the decay argument is zero (the default), this has no effect on the learning rate.\n",
        "\n",
        "- LearningRate = 0.1 * $\\frac{1}{(1 + 0.0 * 1)} \\implies $LearningRate = 0.1\n",
        "\n",
        "When the decay argument is specified, it will decrease the learning rate from the previous epoch by the given fixed amount.\n",
        "\n",
        "*See the implementation of time-based learning rate scheduling with the MNIST dataset Example at the end of this notebook.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ltSUwe4dqOu"
      },
      "source": [
        "### Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTtpElDodqOv"
      },
      "source": [
        "Deep neural networks may have millions of parameters. The network, therefore,   has vast freedom and can fit a huge variety of complex datasets. This flexibility however also makes it prone to overfitting the training set. Thus we need regularization.\n",
        "\n",
        "Let us now see some popular regularization techniques for neural networks: $ℓ1$ and $ℓ2$ regularization and dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD98NJQ6dqOw"
      },
      "source": [
        "#### $ℓ1$ and $ℓ2$ Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7-pZLbsdqOw"
      },
      "source": [
        "We can use $ℓ1$ and $ℓ2$ regularization  to  constrain  a  neural  network’s  connection  weights  (but  typically  not  its  biases).  Here  is  how  to  apply  $ℓ2$  regularization  to  a  Keras  layer’s  connection  weights, using a regularization factor of 0.01:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E369mam-dqOx"
      },
      "source": [
        "layer = Dense(100, activation=\"relu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD9lTIltdqOy"
      },
      "source": [
        "Applying the  same  regularizer, activation function and initialization strategy repeatedly to  all  layers  in  our  network may make it error-prone. To avoid this, we can try refactoring our code to use loops. Another option is to use Python’s `functools.partial()` function: it lets us  create  a  thin  wrapper  for  any  callable,  with  some  default  argument  values.  For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IUZfTosdqOz"
      },
      "source": [
        "# creating regularized dense layer for model\n",
        "RegularizedDense = partial(keras.layers.Dense,\n",
        "                           activation=\"relu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=keras.regularizers.l2(0.01))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwH4qCwhdqO0"
      },
      "source": [
        "# defining model with regularization\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=[28, 28]),\n",
        "    RegularizedDense(300),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(10, activation=\"softmax\",\n",
        "                     kernel_initializer=\"glorot_uniform\")\n",
        "])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ9wGk2FdqO1"
      },
      "source": [
        "#### Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23QlGnLbdqO2"
      },
      "source": [
        "Dropout  is  one  of  the  most  popular  regularization  techniques  for  deep  neural  networks. At each training stage, individual nodes are either dropped out of the net with probability 1-p or kept with probability p, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.\n",
        "\n",
        "![Image](https://i.ibb.co/HnfSTyX/M5-2.jpg)\n",
        "\n",
        "$\\text{Figure: Dropout Regularization}$\n",
        "\n",
        "To  implement  dropout  using  Keras,  we  can  use  the  keras.layers.Dropout  layer. During  training,  it  randomly  drops  some  inputs  (setting  them  to  0)  and  divides  the remaining inputs by the keep probability. After training, it just passes  the  inputs  to  the  next  layer.  For  example,  the  following  code  applies  dropout regularization before every Dense layer, using a dropout rate of 0.2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IErGvcSdqO3"
      },
      "source": [
        "model = Sequential([\n",
        "                    Flatten(input_shape=[28, 28]),\n",
        "                    Dropout(rate=0.2),\n",
        "                    Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "                    Dropout(rate=0.2),\n",
        "                    Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "                    Dropout(rate=0.2),\n",
        "                    Dense(10, activation=\"softmax\")\n",
        "                    ])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpTgUd2EdqO4"
      },
      "source": [
        "If we observe that the model is overfitting, we can increase the dropout rate. Conversely, we should try decreasing the dropout rate if the model underfits the training set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iQC5OcP8cTx"
      },
      "source": [
        "Based on the learnings above, let us now explore hyperparameter tuning during the neural network training phase.\n",
        "\n",
        "Here, we implement the sequential model and use the **MNIST dataset**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVb26VZQMLXn"
      },
      "source": [
        "#### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYcT8C3cO7Nc"
      },
      "source": [
        "We load the MNIST dataset, using Keras' dataset utilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJsnHKAIMsW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "018706ab-ca16-4c9e-d25f-3b5d4bc2ec05"
      },
      "source": [
        "# data resizing variables\n",
        "NUM_ROWS = 28\n",
        "NUM_COLS = 28\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Load data\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWHAgEhhVSJE"
      },
      "source": [
        "To feed MNIST instances into a neural network, they need to be reshaped, from a 2D image representation to a single dimension sequence. We also convert the class vector to a binary matrix (using to_categorical). This is accomplished below after which the same function defined above is called again in order to show the effects of our data reshaping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4gQx33YR1ca"
      },
      "source": [
        "# Reshape data\n",
        "X_train = X_train.reshape((X_train.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_train = X_train.astype('float32') / 255\n",
        "X_test = X_test.reshape((X_test.shape[0], NUM_ROWS * NUM_COLS))\n",
        "X_test = X_test.astype('float32') / 255\n",
        "\n",
        "# Categorically encode labels\n",
        "y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = to_categorical(y_test, NUM_CLASSES)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBfeJhORQCyB"
      },
      "source": [
        "# create the sequential model with BN and dropout layers\n",
        "model = Sequential([\n",
        "    Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    # dropout layer to drop neurons with rate less than 0.2\n",
        "    Dropout(rate=0.2),\n",
        "    # BN layer to rescale the inputs\n",
        "    BatchNormalization(),\n",
        "    Activation(\"relu\"),\n",
        "    Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    Dropout(rate=0.2),\n",
        "    Activation(\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgVUeM5yWSTS"
      },
      "source": [
        "**Note:** You can also try to define Regularized dense layer and can create a sequential model as we see in the $l1$ and $l2$ regularization section discussed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsKGOEeHQg_k"
      },
      "source": [
        "# time based learning-rate scheduling\n",
        "epochs = 10\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs\n",
        "# define optimizer\n",
        "optimizer = keras.optimizers.legacy.Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, decay=decay_rate) #CHANGE\n",
        " \n",
        "# Compile model\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ3bT-_DUpLq"
      },
      "source": [
        "**Note:** In the above code cell, you can also try compiling the with other optimizers like RMS prop, momentum optimization, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47V1U9LhQkIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "outputId": "1fb23e12-82c5-4876-cc0b-026142de819f"
      },
      "source": [
        "# outputs epoch-by-epoch loss functions and accuracies at the end of each epoch of training\n",
        "plot_losses = livelossplot.PlotLossesKeras()\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=epochs,\n",
        "          callbacks=[plot_losses],\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, y_test))\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy\n",
            "\ttraining         \t (min:    0.905, max:    0.980, cur:    0.980)\n",
            "\tvalidation       \t (min:    0.963, max:    0.981, cur:    0.981)\n",
            "Loss\n",
            "\ttraining         \t (min:    0.060, max:    0.313, cur:    0.060)\n",
            "\tvalidation       \t (min:    0.067, max:    0.123, cur:    0.068)\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r469/469 [==============================] - 7s 14ms/step - loss: 0.0605 - accuracy: 0.9799 - val_loss: 0.0675 - val_accuracy: 0.9806\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff7dbd09180>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPTUDJNjVd_n"
      },
      "source": [
        "##### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQhlgALoVgzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d095b6a0-1088-4c42-9b6b-c5654246e818"
      },
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0675298348069191\n",
            "Test accuracy: 0.9805999994277954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2rywYq6bjG4"
      },
      "source": [
        "##### Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhEfjsTKbny1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824f994c-69f3-4373-f2a0-e563213d66e5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_85 (Dense)            (None, 300)               235200    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 300)               0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 300)              1200      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 300)               0         \n",
            "                                                                 \n",
            " dense_86 (Dense)            (None, 100)               30000     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 100)               0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 100)              400       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_87 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 267,810\n",
            "Trainable params: 267,010\n",
            "Non-trainable params: 800\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v4LSY3AdqO5"
      },
      "source": [
        "### Theory Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z62bkuLOdqO5"
      },
      "source": [
        "**Q1.** How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer?\n",
        "\n",
        "**Answer 1:** Email classification is a binary classification problem, so you would only need one neuron in the output layer. This neuron would indicate the probability that the email is spam or ham. You'd most likely use the sigmoid activation function in the output layer.\n",
        "\n",
        "For the MNIST problem you would need 10 output neurons in the final layer, one for each digit. You would then replace the logistic function with the softmax function which can output one probability per class per digit.\n",
        "\n",
        "**Q2.** Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
        "\n",
        "**Answer 2:** In general, the hyperparameters of a neural network you can adjust are the number of hidden layers, the number of neurons in each hidden layer, and the activation function used by each neuron.\n",
        "\n",
        "For binary classification, use the logistic activation function. For a multi-class problem, use softmax. For a linear regression problem, don't use an activation function.\n",
        "\n",
        "Some simple ways to try and solve overfitting are reducing the number of hidden layers or the number of neurons.\n",
        "\n",
        "**Q3.** What  may  happen  if  you  set  the  momentum  hyperparameter  too  close  to  1  (e.g., 0.99999) when using an SGD optimizer?\n",
        "\n",
        "**Answer 3:** If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value.\n",
        "\n",
        "**Q4.** Does dropout slow down training?\n",
        "\n",
        "**Answer 4:** Yes, dropout does slow down training, in general roughly by a factor of two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "#@title Which of the following is designed to automatically standardize the inputs to a layer in a deep learning neural network? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"Batch Normalization\" #@param [\"\", \"Momentum\", \"Batch Normalization\", \"ReLU\", \"Dropout\"]"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"Perfect for practice\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5aefcf-3b78-4450-bf9a-c70a1b6b0d58"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 5249\n",
            "Date of submission:  04 Jun 2023\n",
            "Time of submission:  01:34:26\n",
            "View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RBLty86JnqQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}