{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subho99/Computational-Data-Science/blob/main/SubhajitBasistha_M5_AST_04_PySpark_Transform_and_Actions_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps9llghv8jX1"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 4: PySpark Transform and Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeP1PAXf8jYD"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkwaW3k58jYG"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* Perform RDD (Resilient Distributed Datasets) operations including:\n",
        "        \n",
        "  1.   Transformations\n",
        "  2.   Actions\n",
        "\n",
        "* Obtain an overview of shuffle operations\n",
        "* Implement RDD based model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3oZtbys8jYL"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nR2ZLd-8jYM"
      },
      "source": [
        "**Overview about Spark, PySpark and Apache Spark in simple language**\n",
        "\n",
        "**Spark:** A data computational framework that handles Big data.\n",
        "\n",
        "**PySpark:** A tool to support Python with Spark\n",
        "\n",
        "**Apache Spark:** It is an open-source cluster-computing framework, built around speed, ease of use, and streaming analytics.\n",
        "\n",
        "* Like Spark, PySpark helps data scientists to work with (RDDs) Resilient Distributed Datasets. It is also used to work on Data frames. PySpark can be used to work with machine learning algorithms as well.\n",
        "\n",
        "### ***Spark RDD is a major concept in Apache Spark***\n",
        "\n",
        "**Resilient Distributed Datasets:**\n",
        "\n",
        "**Resilient:**    because RDDs are immutable (can’t be modified once created)                        and fault tolerant.\n",
        "\n",
        "**Distributed:**  because it is distributed across clusters\n",
        "\n",
        "**Dataset:**      because it holds data.\n",
        "\n",
        "**Why RDD?**\n",
        "\n",
        "* Apache Spark lets you treat your input files almost like any other variable, which you cannot do in Hadoop MapReduce.\n",
        "* RDDs are automatically distributed across the network by means of Partitions.\n",
        "\n",
        "RDDs are divided into smaller chunks called Partitions, and when you execute some action, a task is launched per partition. This means, the more the number of partitions, the more will be the parallelism.\n",
        "\n",
        "Spark automatically decides the number of partitions that an RDD has to be divided into, but you can also specify the number of partitions when creating an RDD. These partitions of an RDD are distributed across all the nodes in the network.\n",
        "\n",
        "**Difference between Dataframe and RDD (Resilient Distributed Datasets):**\n",
        "\n",
        "**Dataframe:**\n",
        "* Automatically finds out the schema of the dataset.\n",
        "* Performs aggregation faster than RDDs, as it provides an easy API to perform aggregation operations.\n",
        "\n",
        "**RDD:**\n",
        "* We need to define the schema manually.\n",
        "* RDD is slower than Dataframes to perform simple operations like grouping the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-PfJvpLuTFa"
      },
      "source": [
        "**Creating an RDD**\n",
        "\n",
        "**There are three ways to create an RDD in Spark:**\n",
        "1. Parallelizing already existing collection in the driver program.\n",
        "\n",
        "  The key point to note in a parallelized collection is the number of partitions the dataset is divided into. Spark will run one task for each partition of the cluster. We require two to four partitions for each CPU in the cluster. Spark sets the number of partition based on our cluster.\n",
        "\n",
        "2. Referencing a dataset in an external storage system (e.g. HDFS, Hbase, shared file system).\n",
        "  \n",
        "  In Spark, the distributed dataset can be formed from any data source supported by Hadoop, including the local file system, HDFS, Cassandra, HBase etc. In this, the data is loaded from the external dataset.\n",
        "\n",
        "  * csv (String path): It loads a CSV file and returns the result as a Dataset.\n",
        "\n",
        "  * json (String path): It loads a JSON file (one object per line) and returns the result as a Dataset\n",
        "\n",
        "  * textFile (String path) It loads text files and returns a Dataset of String.\n",
        "\n",
        "3. Creating RDD from already existing RDDs.\n",
        "\n",
        "  Transformation mutates one RDD into another RDD, this transformation is the way to create an RDD from an already existing RDD. This creates a difference between Apache Spark and Hadoop MapReduce."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0zutSoUD0cs"
      },
      "source": [
        "**Actions/Transformations**\n",
        "\n",
        "There are two types of operations that you can perform on an RDD-\n",
        "* Transformations\n",
        "* Actions.\n",
        "\n",
        "**Transformation** applies some function on an RDD and creates a new RDD, it does not modify the RDD that you apply the function on. Also, the new RDD keeps a pointer to its parent RDD.\n",
        "\n",
        "When you call a transformation, Spark does not execute it immediately, instead it creates a lineage. A lineage keeps track of what all transformations have to be applied on that RDD, including from where it has to read the data.\n",
        "\n",
        "\n",
        "**Action** is used to either save the result to some location or to display it. You can also print the RDD lineage information by using the command:\n",
        "\n",
        "\"filtered.toDebugString\" -> (*filtered* is the RDD here)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ja7Mz2TMhIR"
      },
      "source": [
        "![img](https://cdn.iisc.talentsprint.com/CDS/Images/Pyspark_RDD.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZfXZMpmdJ8p"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2236624\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HM6RuGGdJ8-"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"8240187807\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8dea80e4-4926-458a-cba8-dd630f83c16f"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M5_AST_04_PySpark_Transform_and_Actions_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/Spark_Text.txt\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/google_books.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2236624&recordId=6281\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q9PCT0kBVB3"
      },
      "source": [
        " **Install PySpark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xzay908G5qQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c1e588-447a-419f-8215-4f1e70ed75ea"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285397 sha256=caf10170a8d1f9bde079737fcb5b0dfe42ee3c70d85e0bb1e619edf3653fc5ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BobYAePRT6Ok"
      },
      "source": [
        "**Creating Spark Session**\n",
        "\n",
        "Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0 (Instead of having various contexts, everything is encapsulated in a Spark session)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-tj32cQHmBb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "a67b2e0f-6b43-4a8b-d3d0-091bab2b7738"
      },
      "source": [
        "# Start spark session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf  # User Defined Functions\n",
        "from pyspark.sql.types import StringType\n",
        "spark = SparkSession.builder.appName('Rdd').getOrCreate()\n",
        "spark"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e20b4c0ff70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://918936dcdec9:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Rdd</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypfIQnQczPMy"
      },
      "source": [
        "# Accessing sparkContext from sparkSession instance.\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kt4YX3C3dGT"
      },
      "source": [
        "### Spark Python Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuBu_zaiYWX-"
      },
      "source": [
        "**map()** - A map transformation is useful when we need to transform an RDD by applying a function to each element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vJJS0NS_o8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab04ccc1-2609-4255-f79a-a2f83c934fa9"
      },
      "source": [
        "# Return a new RDD by applying a function to each element of this RDD.\n",
        "rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
        "sorted(rdd.map(lambda x: (x, 1)).collect())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 1), ('b', 1), ('c', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRmlweKeDvjf"
      },
      "source": [
        "**take()** - Take the first num elements of the RDD.\n",
        "\n",
        "It works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGskOAX2_s0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b24d71d0-a145-424f-9b5e-0bb085635564"
      },
      "source": [
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2) #take()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oZr8ukWkHAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d536c002-6fec-4976-d9c5-f7cf53190a77"
      },
      "source": [
        "sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3) #take()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[91, 92, 93]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3fEehJeEguG"
      },
      "source": [
        "**flatMap()** - The flatMap transformation will return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. This is the main difference between the flatMap and *map transformations.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAelIgUTf3Sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d20f519-6e8b-4f54-c102-64c60ee7ab62"
      },
      "source": [
        "s0 = sc.parallelize([3,4,5])\n",
        "s0.flatMap(lambda x: [x, x*x]).collect()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 9, 4, 16, 5, 25]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJLRChBQxnmG"
      },
      "source": [
        "Compare the same function using map()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBJegE2cxmAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517dddfb-845d-4e1f-93f5-708d87ec44fb"
      },
      "source": [
        "sc.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 9], [4, 16], [5, 25]]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WihTRM55aQzK"
      },
      "source": [
        "**filter()** - The filter transformation returns a new dataset formed by selecting  those elements of the source on which func returns true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgypZMSQGyR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecaffcb2-4f72-4837-e055-3e83488d271f"
      },
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd.filter(lambda x: x % 2 == 0).collect() # Return a new RDD containing only the elements that satisfy a predicate."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FV3bYIKJi31"
      },
      "source": [
        "**groupByKey()** - We can apply the “groupByKey” transformations on (key,val) pair RDD. The “groupByKey” will group the values for each key in the original RDD. It will create a new pair, where the original key corresponds to this collected group of values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyh3Z6mDOMbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f3ba8e-dbb8-4006-9b5e-cf17a4d8b4a8"
      },
      "source": [
        "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "x.groupByKey().map(lambda x : (x[0], list(x[1]))).collect()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', [1]), ('a', [1, 1])]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjbYWjBCLfUn"
      },
      "source": [
        "**reduceByKey()** - Merge the values for each key using an associative reduce function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_kSwLv6OmzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c60cbc91-4614-467b-9cf7-67e9bf3cbe7a"
      },
      "source": [
        "from operator import add\n",
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "sorted(rdd.reduceByKey(add).collect())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 2), ('b', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV-_pmJVLfl5"
      },
      "source": [
        "**mapPartitions()** - Is similar to map, but runs separately on each partition (block) of the RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2UtmrsS5uA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092e5436-b550-4f05-9dd4-d0f08d664514"
      },
      "source": [
        "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
        "\n",
        "wordsRDD = sc.parallelize(wordsList, 4) # number of partitions - 4\n",
        "\n",
        "print(wordsRDD.collect())\n",
        "\n",
        "itemsRDD = wordsRDD.mapPartitions(lambda iterator: [','.join(iterator)])\n",
        "# mapPartitions() loops through 4 partitions and combines('rat,cat') in 4th iteration.\n",
        "print (itemsRDD.collect())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'elephant', 'rat', 'rat', 'cat']\n",
            "['cat', 'elephant', 'rat', 'rat,cat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBZnXOHiHMjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25cb41c-e4c7-4044-8637-2fedcaa19464"
      },
      "source": [
        "L = range(1,10)\n",
        "\n",
        "parallel = sc.parallelize(L, 3) # number of partitions - 3\n",
        "\n",
        "def f(iterator):\n",
        "  yield sum(iterator)\n",
        "\n",
        "parallel.mapPartitions(f).collect()\n",
        "\n",
        "# Results [6,15,24] are created because mapPartitions() loops through 3 partitions, Partion 1: 1+2+3 = 6, Partition 2: 4+5+6 = 15, Partition 3: 7+8+9 = 24\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 15, 24]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rclESZfdRbMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1467a744-d2f2-4e07-eb88-e2e3001b26af"
      },
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4], 2) # number of partitions - 2\n",
        "\n",
        "def f(iterator):\n",
        "  yield sum(iterator)\n",
        "\n",
        "rdd.mapPartitions(f).collect()\n",
        "\n",
        "# Results [3, 7], partition 1 : 1+2 = 3, partition 2 : 3+4 =7"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-SUV2VcJjMJ"
      },
      "source": [
        "**mapPartitionsWithIndex()** - Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dgV6f21Us_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b816784-d56d-445e-e8d4-9e91e05510b1"
      },
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
        "def f(splitIndex, iterator): yield splitIndex\n",
        "rdd.mapPartitionsWithIndex(f).sum()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82fgwxs8kzy5"
      },
      "source": [
        "### Spark Python Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaYgbjhklcT0"
      },
      "source": [
        "**Creating an RDD to explain \"RDD actions with Examples\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seJ35vrGleKN"
      },
      "source": [
        "data=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\n",
        "\n",
        "inputRDD = spark.sparkContext.parallelize(data)\n",
        "\n",
        "listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])\n",
        "\n",
        "from operator import add"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTRyVfftnLaD"
      },
      "source": [
        "After creating two RDDs as given above, we use these two as and when necessary to demonstrate the RDD actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjcr2K9fk0LT"
      },
      "source": [
        "**first()** – Return the first element in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR-hsPBqnds7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ade7b89b-845a-405d-a8c5-54f32e54e58e"
      },
      "source": [
        "#first\n",
        "print(\"first :  \"+str(listRdd.first()))\n",
        "print(\"first :  \"+str(inputRDD.first()))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first :  1\n",
            "first :  ('Z', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFIF21pnk0tc"
      },
      "source": [
        "**take()** – Return the first num elements of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8iyLrZ0nopK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce89cc4-d2f2-4410-c782-716bb8a4de4d"
      },
      "source": [
        "#take()\n",
        "print(\"take : \"+str(listRdd.take(2)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "take : [1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5dG25M-k02C"
      },
      "source": [
        "**takeSample()** – Return the subset of the dataset in an Array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J58_ENwkn15A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b64d19-7bdd-45ff-d169-976432a87324"
      },
      "source": [
        "print(\"take : \"+str(listRdd.takeSample(0,3))) # ([1,2,3,4,5,3,2])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "take : [5, 3, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XddOZ8LRk08Y"
      },
      "source": [
        "**takeOrdered()** – Return the first num (smallest) elements from the dataset and this is the opposite of the take() action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nptmLS_qoSKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c808972-dbb6-4280-aa1f-7b936cb8a1d7"
      },
      "source": [
        "print(\"takeOrdered : \"+ str(listRdd.takeOrdered(2)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "takeOrdered : [1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-chsPGWPk1EQ"
      },
      "source": [
        "**collect()** - Return the complete dataset as an Array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMDnTnA0pNHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed56f87-88f3-4221-b86c-536b6a6b6c85"
      },
      "source": [
        "#Collect\n",
        "data = listRdd.collect()\n",
        "print(data)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 3, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1How8QYk1NK"
      },
      "source": [
        "**count()** – Return the count of elements in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5YNtF_RpW6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b1ad290-b483-4803-a180-4d9f760eab24"
      },
      "source": [
        "print(\"Count : \"+str(listRdd.count()))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count : 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNSjv_LCpcAU"
      },
      "source": [
        "**countByValue()** – Return Map[T,Long] key representing each unique value in dataset and value represents count each value present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij3GMKfbpcLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53414e93-4112-47c7-89aa-643a9ec2fbf7"
      },
      "source": [
        "print(\"countByValue :  \"+str(listRdd.countByValue()))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "countByValue :  defaultdict(<class 'int'>, {1: 1, 2: 2, 3: 2, 4: 1, 5: 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNHqoneIplsk"
      },
      "source": [
        "**reduce()** – Reduces the elements of the dataset using the specified binary operator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McRegvjapl0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426aa315-257c-403f-ff5b-a52efc0f980b"
      },
      "source": [
        "redRes=listRdd.reduce(add)\n",
        "print(redRes)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yklH98ELqV_q"
      },
      "source": [
        "**top()** – Return top n elements from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj1f2tsCqWWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d28bcd3-14d5-44ee-9d68-39327a5c3559"
      },
      "source": [
        "print(\"top : \"+str(listRdd.top(2)))\n",
        "print(\"top : \"+str(inputRDD.top(2)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top : [5, 4]\n",
            "top : [('Z', 1), ('C', 40)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OONqU-CTtEkZ"
      },
      "source": [
        "**fold()** - Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral \"zero value.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euxrIYOYtFEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c551d9d-1971-4d28-a6ba-6246e5ed8c33"
      },
      "source": [
        "foldRes=listRdd.fold(0, add)\n",
        "print(foldRes)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khV8HhGc0lhx"
      },
      "source": [
        "**foldByKey()** -  is quite similar to fold(), both use a zero value of the same type of the data in our RDD and combination function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aGQdKuM0L9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3ec07c-5a4a-4c1e-cc07-c0a00c8f8e6b"
      },
      "source": [
        "inputRDD.foldByKey(0, add).collect()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('C', 40), ('Z', 1), ('A', 20), ('B', 120)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehIGu2bG0-kc"
      },
      "source": [
        "**reduceByKey()** - Merge the values for each key using an associative reduce function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PXKBSyi0-zi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce107f36-c7c0-4394-afec-3e044286ad53"
      },
      "source": [
        "sorted(inputRDD.reduceByKey(add).collect())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', 20), ('B', 120), ('C', 40), ('Z', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6S5yGzv13Ck"
      },
      "source": [
        "**combineByKey()** - Generic function to combine the elements for each key using a custom set of aggregation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSpVQHCL13Ls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8025c49-9009-4a46-9599-19c4b87ee7af"
      },
      "source": [
        "def f(inputRDD):\n",
        "  return inputRDD\n",
        "def add(A, B):\n",
        "  return A + str(B)\n",
        "sorted(inputRDD.combineByKey(str, add, add).collect())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', '20'), ('B', '303060'), ('C', '40'), ('Z', '1')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFXAbayxXKtw"
      },
      "source": [
        "### PySpark User Defined Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXEXD6LUXFu7"
      },
      "source": [
        "* PySpark UDF is a User Defined Function that is used to create a reusable\n",
        "function in Spark.\n",
        "\n",
        "* Once UDF is created, that can be re-used on multiple DataFrames and SQL (after registering).\n",
        "\n",
        "* The default type of the udf() is StringType."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCyKMMvCGRkk"
      },
      "source": [
        "Created dataframe with two columns \"Seqno\" and \"Name\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvCmOPMTDtZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9722855d-934e-4700-8eeb-31e69ba34294"
      },
      "source": [
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "\n",
        "df.show(truncate=False)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "|Seqno|Name        |\n",
            "+-----+------------+\n",
            "|1    |john jones  |\n",
            "|2    |tracey smith|\n",
            "|3    |amy sanders |\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lon2qElWGW8Z"
      },
      "source": [
        "Applying UDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9TQBD5JZTh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2531272c-ce76-4a89-d4a1-962840ecd616"
      },
      "source": [
        "# creating a udf using lambda\n",
        "convertUDF = udf(lambda z: z.upper())\n",
        "df.select(col(\"Seqno\"), convertUDF(col(\"Name\")).alias(\"Name\") ).show(truncate=False)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "|Seqno|Name        |\n",
            "+-----+------------+\n",
            "|1    |JOHN JONES  |\n",
            "|2    |TRACEY SMITH|\n",
            "|3    |AMY SANDERS |\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiEIyKJ4xmKj"
      },
      "source": [
        "#### **Shuffle Operations**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XP9I8tYbl_i"
      },
      "source": [
        "Shuffling is a mechanism PySpark uses to redistribute the data across different executors and even across machines. PySpark shuffling triggers when we perform certain transformation operations like gropByKey(), reduceByKey(), join() on RDDS\n",
        "\n",
        "Spark also supports transformations with wide dependencies, such as groupByKey and reduceByKey. In these dependencies, the data required to compute the records in a single partition can reside in many partitions of the parent dataset.\n",
        "\n",
        "To perform these transformations, all of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy this requirement, Spark performs a shuffle, which transfers data around the cluster and results in a new stage with a new set of partitions.\n",
        "\n",
        "For example, consider the following code:\n",
        "\n",
        "**sc.textFile(\"someFile.txt\").map(mapFunc).flatMap(flatMapFunc).filter(filterFunc).count()**\n",
        "\n",
        "It runs a single action, count, which depends on a sequence of three transformations on a dataset derived from a text file. This code runs in a single stage because none of the outputs of these three transformations depend on data that comes from different partitions than their inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huYvlG3IAq_A"
      },
      "source": [
        "**Below is an example implementing RDD based model to count the words given in a file**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b3HdfYQjLe-"
      },
      "source": [
        "\n",
        "\n",
        "To implement RDD based model, we have used the text file (**Spark_Text.txt**) which includes Apache Spark notes/information. This text file contains 5 paragraphs of information on Spark.\n",
        "\n",
        "We would perform RDD Transformations and Actions on the file to count the words given in the text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1WbnApmkBle"
      },
      "source": [
        "rdd = sc.textFile(\"Spark_Text.txt\")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gpTlIzzYH3V"
      },
      "source": [
        "# To lower the case of each word of a document, we can use the map transformation.\n",
        "def Func(lines):\n",
        "      lines = lines.lower()\n",
        "      lines = lines.split()\n",
        "      return lines\n",
        "rdd1 = rdd.map(Func)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-h8E_vJiplH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f44d3383-c7da-4331-b527-eb44f6518593"
      },
      "source": [
        "rdd1.take(1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['apache',\n",
              "  'spark',\n",
              "  'lets',\n",
              "  'you',\n",
              "  'treat',\n",
              "  'your',\n",
              "  'input',\n",
              "  'files',\n",
              "  'almost',\n",
              "  'like',\n",
              "  'any',\n",
              "  'other',\n",
              "  'variable,',\n",
              "  'which',\n",
              "  'you',\n",
              "  'cannot',\n",
              "  'do',\n",
              "  'in',\n",
              "  'hadoop',\n",
              "  'mapreduce.']]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3zknNLjYH-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5f3746-7d13-40db-8616-0e933a2810b7"
      },
      "source": [
        "#To get the flat output, we need to apply a transformation which will flatten the output, The transformation “flatMap\" will help here:\n",
        "rdd2 = rdd.flatMap(Func)\n",
        "rdd2.take(3)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apache', 'spark', 'lets']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0wfA1urYIB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a66cc2-2306-40c6-803a-b21b25b5cdb0"
      },
      "source": [
        "rdd3 = rdd2.filter(lambda x:x!= '')\n",
        "rdd3.take(7)  # We can check first 7 elements of “rdd3” by applying take action."
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apache', 'spark', 'lets', 'you', 'treat', 'your', 'input']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c9lxazfYIFM"
      },
      "source": [
        "rdd3_mapped = rdd3.map(lambda x: (x,1))\n",
        "rdd3_grouped = rdd3_mapped.groupByKey()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIozl8hjYV0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb28d65-a332-4d8b-95fc-3f4b45d4de54"
      },
      "source": [
        "rdd3_mapped.reduceByKey(lambda x,y: x+y).map(lambda x:(x[1],x[0])).sortByKey(False).take(200)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(13, 'the'),\n",
              " (7, 'a'),\n",
              " (6, 'of'),\n",
              " (6, 'you'),\n",
              " (5, 'rdd'),\n",
              " (4, 'that'),\n",
              " (4, 'to'),\n",
              " (4, 'it'),\n",
              " (3, 'are'),\n",
              " (3, 'when'),\n",
              " (3, 'an'),\n",
              " (3, 'spark'),\n",
              " (3, 'rdds'),\n",
              " (3, 'number'),\n",
              " (3, 'be'),\n",
              " (3, 'partitions'),\n",
              " (3, 'has'),\n",
              " (2, 'in'),\n",
              " (2, 'partitions,'),\n",
              " (2, 'execute'),\n",
              " (2, 'is'),\n",
              " (2, 'more'),\n",
              " (2, 'rdd.'),\n",
              " (2, 'new'),\n",
              " (2, 'rdd,'),\n",
              " (2, 'keeps'),\n",
              " (2, 'which'),\n",
              " (2, 'automatically'),\n",
              " (2, 'distributed'),\n",
              " (2, 'across'),\n",
              " (2, 'divided'),\n",
              " (2, 'and'),\n",
              " (2, 'some'),\n",
              " (2, 'all'),\n",
              " (2, 'function'),\n",
              " (2, 'on'),\n",
              " (2, 'creates'),\n",
              " (2, 'does'),\n",
              " (2, 'not'),\n",
              " (1, 'treat'),\n",
              " (1, 'input'),\n",
              " (1, 'files'),\n",
              " (1, 'like'),\n",
              " (1, 'other'),\n",
              " (1, 'variable,'),\n",
              " (1, 'cannot'),\n",
              " (1, 'do'),\n",
              " (1, 'hadoop'),\n",
              " (1, 'network'),\n",
              " (1, 'means'),\n",
              " (1, 'into'),\n",
              " (1, 'task'),\n",
              " (1, 'means,'),\n",
              " (1, 'decides'),\n",
              " (1, 'but'),\n",
              " (1, 'creating'),\n",
              " (1, 'these'),\n",
              " (1, 'nodes'),\n",
              " (1, 'network.'),\n",
              " (1, 'applies'),\n",
              " (1, 'on.(remember'),\n",
              " (1, 'also,'),\n",
              " (1, 'pointer'),\n",
              " (1, 'parent'),\n",
              " (1, 'call'),\n",
              " (1, 'transformation,'),\n",
              " (1, 'instead'),\n",
              " (1, 'lineage.'),\n",
              " (1, 'transformations'),\n",
              " (1, 'applied'),\n",
              " (1, 'where'),\n",
              " (1, 'read'),\n",
              " (1, 'apache'),\n",
              " (1, 'lets'),\n",
              " (1, 'your'),\n",
              " (1, 'almost'),\n",
              " (1, 'any'),\n",
              " (1, 'mapreduce.'),\n",
              " (1, 'by'),\n",
              " (1, 'partitions.'),\n",
              " (1, 'smaller'),\n",
              " (1, 'chunks'),\n",
              " (1, 'called'),\n",
              " (1, 'action,'),\n",
              " (1, 'launched'),\n",
              " (1, 'per'),\n",
              " (1, 'partition.'),\n",
              " (1, 'will'),\n",
              " (1, 'parallelism.'),\n",
              " (1, 'into,'),\n",
              " (1, 'can'),\n",
              " (1, 'also'),\n",
              " (1, 'specify'),\n",
              " (1, 'transformation'),\n",
              " (1, 'modify'),\n",
              " (1, 'apply'),\n",
              " (1, 'resilient/immutable).'),\n",
              " (1, \"it's\"),\n",
              " (1, 'immediately,'),\n",
              " (1, 'lineage'),\n",
              " (1, 'track'),\n",
              " (1, 'what'),\n",
              " (1, 'including'),\n",
              " (1, 'from'),\n",
              " (1, 'data')]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTbilp60aHcl"
      },
      "source": [
        "**In the below example we can see Spark Transformations in Python using a CSV file.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqdCmWOejijF"
      },
      "source": [
        "We will use this CSV file (**Google_Books.csv**) to work on Spark Transformations.\n",
        "\n",
        "This data was acquired from the Google Books store. Google API was used to acquire the data. Nine features were gathered for each book in the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P357wQsFb5XS"
      },
      "source": [
        "book_names = sc.textFile(\"google_books.csv\")\n",
        "rows = book_names.map(lambda line: line.split(\",\")) #we are creating a new RDD called “rows” by splitting every row in the book_names RDD."
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0wNy70CdRpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310304dc-797a-45bb-a7ac-e26e0075a696"
      },
      "source": [
        "for row in rows.take(rows.count()):\n",
        "  print(row[1])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "authors\n",
            "['Wendelin Van Draanen']\n",
            "\n",
            "\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "\"['James Dickey'\n",
            "\"['Katherine M. Keyes'\n",
            "['Martina Handler']\n",
            "['BookCaps Study Guides Staff']\n",
            "\"['Richard M. Lerner'\n",
            "\"['David I. Durham'\n",
            "\n",
            "['Ruth Roy Harris']\n",
            "\n",
            "\"['Gale\n",
            "\"['David E. Keyes'\n",
            "['United States. Dept. of Commerce']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "\n",
            "['United States. Bureau of Mines']\n",
            "['Elizabeth Walsh']\n",
            " 1997-98\"\n",
            "\n",
            "\n",
            "['Association of Collegiate Schools of Architecture']\n",
            " 1941-1945\"\n",
            "['New Mexico. Bureau of Mines and Mineral Resources']\n",
            "\n",
            "['Maureen Callahan']\n",
            "\n",
            " Keyed to Soderquist\"\n",
            "['William T. Coyle']\n",
            " Keyed to Smiddy and Cunningham\"\n",
            "['Phil Geusz']\n",
            "\"['Joseph M. Flora'\n",
            "['Jeana L. Magyar-Moe']\n",
            "\"['Teresa L. Scheid'\n",
            "['Timothy P Melchert']\n",
            "['Jessica Keyes']\n",
            "['Charles Don Keyes']\n",
            "['Ralph Keyes']\n",
            "\"['Katherine M. Keyes'\n",
            " carbon dioxide and carbon monoxide\"\n",
            "\n",
            "\"['Graeme Simsion'\n",
            "['Janet Clark']\n",
            "['Greg Keyes']\n",
            "['Marian Keyes']\n",
            "['Marian Keyes']\n",
            "\"['R. Reginald'\n",
            "['Greg Keyes']\n",
            "['George Mann']\n",
            "['Marian Keyes']\n",
            "['James A. Kaser']\n",
            "['J. Gregory Keyes']\n",
            " 1915\"\n",
            "['Michael Burke']\n",
            "['Rick Atkinson']\n",
            "['Kerry Reis']\n",
            " Short Story Series\n",
            "\"['James J. Lee (III.)'\n",
            "['Jay Riley Case']\n",
            "['Charles Edward May']\n",
            "['BookCaps Study Guides Staff']\n",
            "\n",
            " 2d ed.\"\n",
            "['Patrice Cassedy']\n",
            "\n",
            "['Janet Clark']\n",
            "['Wendelin Van Draanen']\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "['John Caius (Spirit)']\n",
            "\n",
            "['Richard R. Valencia']\n",
            "\"['Kenneth J. Meier'\n",
            "\n",
            "['William A. Smalley']\n",
            "\"['Leone\n",
            " 1954-2007\"\n",
            "\"['David I. Durham'\n",
            "['Raymond E. Callahan']\n",
            "\"['Abbas Tashakkori'\n",
            "\n",
            "['Maureen Callahan']\n",
            "['Gavin Mortimer']\n",
            "['Charles Higham']\n",
            " Arlington and Belmont Directory\"\n",
            " Jones & Co's New Directory of the Inhabitants\n",
            "\n",
            "\n",
            "['State Historical Society of Wisconsin']\n",
            "\n",
            "['Finis Welch']\n",
            "['Sharon Martin Zankel']\n",
            "['Bruce Barber']\n",
            "['Gail Furman']\n",
            "['Ralph Keyes']\n",
            "['Joshua M. Dunn']\n",
            " A World Apart\"\n",
            "\"['William M. Cruickshank'\n",
            "['California. Bureau of State Audits']\n",
            "\"['Stephan Thernstrom'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "['David E. Keyes']\n",
            "\n",
            "\n",
            "\n",
            "['Purdue Reamer Club']\n",
            "\n",
            "\"['W. Jollie'\n",
            "['Richard K. Reed']\n",
            " Assessing the Effectiveness and Costs of the Keyes Technique\"\n",
            "\n",
            "['Constance Kies']\n",
            "\"['Alan O. Trounson'\n",
            "\n",
            "\"['Sara Stinson'\n",
            "['Northwest and Alaska Fisheries Center (U.S.)']\n",
            "['Jerome Groopman']\n",
            "['American Medical Association']\n",
            "['W. Noel Keyes']\n",
            "['United States. Patent Office']\n",
            "['American College of Physicians']\n",
            "['Cecil M. Robeck']\n",
            "['American College of Physicians']\n",
            " assessing the effectiveness and costs of the Keyes technique\"\n",
            "\n",
            "\"['Tadhg Eoghan MacIntyre'\n",
            " the Third Millennium\"\n",
            " Keyed to Weiler\n",
            " Sport\n",
            "['Don Morrow']\n",
            "['Kathy Nemeh']\n",
            "\n",
            "\"['Laura Lemay'\n",
            "['Wendelin Van Draanen']\n",
            " Keyed to Freeland\n",
            "['Elizabeth Keyes']\n",
            "['Nancy L. Cohen']\n",
            "['Phillip Lopate']\n",
            "['Robert T. Francoeur']\n",
            "['Janet Clark']\n",
            " the Canadian Bar\n",
            " Keyed to Ellman\n",
            " Krier\n",
            " Keyed to Krause\n",
            " Keyed to Weisberg and Appleton's\"\n",
            "['Charles Don Keyes']\n",
            " Edward King\n",
            "\"['John C. Cavanaugh'\n",
            "['Marian Keyes']\n",
            "['Jeffrey Foss']\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            " Keyed to Plater\n",
            "['Dick Keyes']\n",
            "\n",
            " Unit 5\n",
            "['Margie E. Lachman']\n",
            "\"['Nisha Garg'\n",
            "['Kerry Reis']\n",
            "\"['Daniel M. Joel'\n",
            "['Langley Carleton Keyes']\n",
            "\n",
            " Volume 1\"\n",
            "['Iowa Archeological Society']\n",
            "['Henry D. Delcore']\n",
            " Al-Muhit\n",
            " Dr. Mohamad Badawi\n",
            " 2003\"\n",
            "['Michael Asher']\n",
            "\n",
            "\"['Jane Wightwick'\n",
            "['Cameron Powers']\n",
            "['Raji M. Rammuny']\n",
            "['Raji M. Rammuny']\n",
            "['Elizabeth Keyes']\n",
            "['معتز مشعل']\n",
            "['غوستاف لوبون']\n",
            "['ديل كارنيجي']\n",
            "['سعد سعود الكريباني']\n",
            "['Donna Watson']\n",
            "['أسامة شاهين']\n",
            "['د. إبراهيم الفقي']\n",
            "['سلامة موسى/-/-']\n",
            "['أنتوني روبنز']\n",
            "\"['د. إبراهيم الفقي'\n",
            "\n",
            "\"['Ilya Prigogine'\n",
            "['Peter W. Hawkes']\n",
            "['Robert W. Keyes']\n",
            "\n",
            "\n",
            "\"['Jaques Cattell Press'\n",
            "['John M. Blain']\n",
            " 2001\"\n",
            "['Driss Fatih']\n",
            "['National Research Council (U.S.). Division of Chemistry and Chemical Technology']\n",
            "['National Science Foundation (U.S.)']\n",
            "\"['National Academy of Sciences'\n",
            "\"['University of California\n",
            " Volume One\"\n",
            "\n",
            "\n",
            "\n",
            "['Mary Frame']\n",
            "['Wendelin Van Draanen']\n",
            "\"['University of California\n",
            "\n",
            "\"['University of California\n",
            "['National Science Foundation (U.S.)']\n",
            "\"['Christopher G. Morris'\n",
            "\"['Christine Avery'\n",
            "['Jessica Keyes']\n",
            "\n",
            "['National Research Council (U.S.). Division of Chemistry and Chemical Technology']\n",
            "\n",
            "['University of Texas at Austin. Graduate School of Library Science']\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 1992-1993\"\n",
            "\n",
            "\n",
            "\n",
            "\"['Kenneth C. Land'\n",
            "['Marian Keyes']\n",
            "['Richard Watson Hantke']\n",
            "['Zeth Lundy']\n",
            "['Charles Winfield Scott']\n",
            "\"['James Reapsome'\n",
            " Individual Bases of Adolescent Development\"\n",
            " Living and Deceased\"\n",
            "['Ralph Keyes']\n",
            " 1855-1884\"\n",
            "\"['Christopher G. Morris'\n",
            "['Valliappa Lakshmanan']\n",
            "\"['Jonathan S. Comer'\n",
            "\"['Jun Liu'\n",
            "\"['Katherine M. Keyes'\n",
            "['Elias M. Awad']\n",
            " Initialisms & Abbreviations Dictionary\"\n",
            "['Elisabeth Oswald']\n",
            "['Dr. Henry M. Morris']\n",
            " reports and prints of the House Committee on Science and Astronautics\"\n",
            "['Pearson Education']\n",
            " Security\n",
            "\n",
            "['United States. Office of the Assistant Secretary of Defense (Research and Development) ADVISORY PANEL ON PERSONNEL AND TRAINING RESEARCH']\n",
            "\"['Tanja Lange'\n",
            "['James Ohwofasa Akpeninor']\n",
            "\n",
            "\n",
            "['John I Knight']\n",
            "\"['Donald R. Hughes'\n",
            " Civil and Military\"\n",
            " Keyed to Yeazell\"\n",
            "\"['Casenotes'\n",
            " Keyed to Friedenthal\n",
            " Tait\n",
            " Minow\n",
            " Keyed to Yeazell and Schwartz\"\n",
            " 5-9 June 1985\n",
            "\"['James Albert Servies'\n",
            " Mining and Electrical Engineering\"\n",
            "\n",
            "['Library of Congress. Cataloging Policy and Support Office']\n",
            "['Cheryl Lynette Keyes']\n",
            "\n",
            "\"['Murray Forman'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "['Doug Griffiths']\n",
            "['Chris Serb']\n",
            "['David K. Wiggins']\n",
            "['Andrew Sturgeon Young']\n",
            "['Floyd Conner']\n",
            "\"['Bob Boyles'\n",
            "\"['Michael MacCambridge'\n",
            "\n",
            "\n",
            "\n",
            " Keyed to Weiler\n",
            " Keyed to Freeland\n",
            "\n",
            " Keyed to Goldberg Sebok and Ziprusky\"\n",
            "\n",
            " Keyed to Calamari\n",
            "['Aspen Publishers']\n",
            "['Janet A. Mitchell']\n",
            "\"['James Cathcart'\n",
            "\n",
            "\n",
            "\n",
            "['Laura Dingle Ewing']\n",
            "['James Hilton Manning']\n",
            "['Sheldon Zerden']\n",
            "['Marian Keyes']\n",
            " Their Trustees\n",
            "['Verne Thompson']\n",
            "\n",
            "['Janie Warren Hollingsworth Lane']\n",
            "['David Mertz']\n",
            "['Alex Martelli']\n",
            "['Mark Lutz']\n",
            "['José Manuel Ortega']\n",
            "\"['Kevin D. Smith'\n",
            "['David Beazley']\n",
            "['Pratik Desai']\n",
            "['Brett Slatkin']\n",
            "['Tim Altom']\n",
            " Lua\n",
            "['Michael Heydt']\n",
            "['Nigel P. Smart']\n",
            "\"['John Shovic'\n",
            " Platforms\n",
            "['Jessica Keyes']\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"['Christopher G. Morris'\n",
            "['Shashank Johri']\n",
            "['James Murty']\n",
            "['Earl H. Parsons']\n",
            "['American Library Association. Buildings Committee']\n",
            "['Joe Duffy']\n",
            "['B. J. Holmes']\n",
            "['J. Vlietstra']\n",
            "['National Semiconductor Corporation']\n",
            "['Henry Romaine Pattengill']\n",
            " Oversight Hearings\"\n",
            " Oversight Hearings\"\n",
            "\n",
            "\"['United States. Congress. Senate. Committee on Banking\n",
            "['John H. Keyes']\n",
            "\"['Conrad G. Keyes'\n",
            "\n",
            "['A. Eggers-Lura']\n",
            "\n",
            "\n",
            "\n",
            "\"['M. Friedjung'\n",
            "['Joseph Silas Diller']\n",
            "\n",
            "['Victoria Wilson']\n",
            "['Arthur J Marder']\n",
            "['Wendelin Van Draanen']\n",
            "\"['Leonard Maltin'\n",
            "['Arthur Jacob Marder']\n",
            "\n",
            "\n",
            "['Andrew Sturgeon Young']\n",
            "['Wendelin Van Draanen']\n",
            "['United States. Congress. Senate. Committee on Indian Affairs (1993- )']\n",
            "['Steve Neal']\n",
            "\n",
            "\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "['Jessica Keyes']\n",
            "['Wendelin Van Draanen']\n",
            "\n",
            "\n",
            "\n",
            "['John Cotton']\n",
            "\n",
            "\n",
            "['Henry Pettus Randall']\n",
            "\"['Pei Zheng'\n",
            "\"['Mohammad Ilyas'\n",
            "['Casenote Legal Briefs']\n",
            " Seventh Edition\"\n",
            " Keyed to Knapp\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "\n",
            "\"['X. Huang'\n",
            " Education\n",
            "['Pattana Kitiarsa']\n",
            "['Avner Falk']\n",
            "['Joint Committee on Southeast Asia']\n",
            "['Kevin Timpe']\n",
            "['Andrew Eshleman']\n",
            "\n",
            " Ethnicity and Modernity in Southeast Asia\"\n",
            "\"['David G. Truemper'\n",
            "['Alan Dershowitz']\n",
            "\"['N. Walker'\n",
            " Or\n",
            "['Matthew S. Mccormick']\n",
            "['Alan Dershowitz']\n",
            "['Henry Smith']\n",
            "['Henry Smith']\n",
            "\n",
            "['Charles Don Keyes']\n",
            " an Answer to Atheism\"\n",
            "\n",
            "['Michael Asher']\n",
            "['United States. Patent Office']\n",
            "['Wendelin Van Draanen']\n",
            "['Bevan Amberhill']\n",
            "['Charles Stewart Macnair']\n",
            "\"['Book-keeper publishing co.\n",
            "\n",
            "['Laura Childs']\n",
            "['Vincent Virga']\n",
            "['United States. National Labor Relations Board']\n",
            "['United States. Congress. House. Special Committee to Investigate Food Shortages']\n",
            "['Wisconsin Dairy and Food Commission']\n",
            "['United States. Congress. House. Special Committee to Investigate Food Shortages']\n",
            "['Wisconsin Dairy and Food Commission']\n",
            "['Wisconsin. Dairy and Food Commission']\n",
            " 2d ed.\"\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "['Jeffrey Haugaard']\n",
            "\"['Patricia Chambers Walker'\n",
            "\"['Frances Gruber Safford'\n",
            "['Library of Congress. Copyright Office']\n",
            "\n",
            "['Luke Beckerdite']\n",
            "\n",
            "['Clarence Carlson']\n",
            "\n",
            "\n",
            "['Rosanna Cirigliano']\n",
            "\"['James (Jong Hyuk) Park'\n",
            "\"['Paul R. Baker'\n",
            "['Jessica Keyes']\n",
            "\"['Adam Gordon'\n",
            "\"['Peter Thorsteinson'\n",
            "\"['Anonymous'\n",
            "['Howon Kim']\n",
            "['Stuart Jacobs']\n",
            " Site and Office Security\"\n",
            "['Geoff Craighead']\n",
            "['Brad Keyes']\n",
            "\"['Eric Nylund'\n",
            "['Kathleen Masters']\n",
            "['Susan Hill']\n",
            " Third Edition\"\n",
            "\n",
            " Principles and Practices\"\n",
            "\n",
            "['Dick Keyes']\n",
            "['Wendelin Van Draanen']\n",
            "['Michael Newton']\n",
            " Movie-Maker\"\n",
            "\"['Bill Streett'\n",
            " U.S.A.: The Story of Stax Records\"\n",
            "['John Grisham']\n",
            "\n",
            "['Phillip Lopate']\n",
            "['Wendelin Van Draanen']\n",
            " Churchill and the Dardanelles\"\n",
            " the American Theme\"\n",
            "['Georges-Claude Guilbert']\n",
            "\"['Shirlee McCoy'\n",
            "['Will Beall']\n",
            "['Sharon Dunn']\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "['Ann Nocenti']\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "\n",
            "['Hart-Davis']\n",
            "\"['Steve Lane'\n",
            "\n",
            "['Bruno Apolloni']\n",
            "\"['Alan Simpson'\n",
            "['Curt Simmons']\n",
            " PC HARDWARE AND INTERFACING\"\n",
            " Version 2.0\"\n",
            "\n",
            "['Tyler Justin Speed']\n",
            "['شيريل بينارد']\n",
            " Dr. Mohamad Badawi\n",
            " Al-Muhit\n",
            "\n",
            "\n",
            "['DePaul University. International Human Rights Law Institute']\n",
            "['أحمد تيمور باشا']\n",
            "['جون شتاينبك']\n",
            "['Jeff Kinney']\n",
            "['عزمي بشارة']\n",
            "['ميشيل حنا']\n",
            "['DePaul University. International Human Rights Law Institute']\n",
            "['عزمي بشارة']\n",
            "['فولتير']\n",
            "['فراس السواح']\n",
            "['ميرفت محمود محمد']\n",
            "\n",
            "\n",
            "['Maḥmūd Muḥammad Shākir']\n",
            "\n",
            "['مصطفى محمود']\n",
            "['DePaul University. International Human Rights Law Institute']\n",
            "['مؤمنة أبو صالح']\n",
            "['صلابي، علي محمد محمد،']\n",
            "['محمد قطب عبد العال']\n",
            "['فراس السواح']\n",
            "['عزمي بشارة']\n",
            "['عبد العزيز، محسن']\n",
            " December 2013\"\n",
            "\n",
            "['Daniel Keyes']\n",
            "\n",
            "\n",
            "['Stephen Crane']\n",
            "\"[\"\"Sheila O'Flanagan\"\"]\"\n",
            "['Howard Gardner']\n",
            "['مالكولم غلادويل']\n",
            "['Louise L. Hay']\n",
            "['Michael E. Gerber']\n",
            "['Ghassān Kanafānī']\n",
            "['محمد أحمد']\n",
            "['Raji M. Rammuny']\n",
            "['Raji M. Rammuny']\n",
            " Dr. Mohamad Badawi\n",
            " Al-Muhit\n",
            "['Fuʼād Afrām Bustānī']\n",
            "\n",
            "['شيريل بينارد']\n",
            "\n",
            "\n",
            "['Emilie Richards']\n",
            "\"['Ken Keyes\n",
            "\"['Luigino Bruni'\n",
            "['Trevor Eddolls']\n",
            "['Ken Keyes']\n",
            "\"['José Antonio Muñiz Velázquez'\n",
            " or\n",
            "\n",
            "['Sharon L. Hanna']\n",
            "['Len Fulton']\n",
            "\n",
            " Edward King\n",
            " Edward King\n",
            " the Davis-Bacon Reform Bill of 1993\"\n",
            "['Naval War College (U.S.)']\n",
            "['Richard Watson Hantke']\n",
            "\"['Edward Swift Dunster'\n",
            "['William Kent']\n",
            "\n",
            "['George W. Whiting']\n",
            "['Hiram Ketchum']\n",
            "['Military Service Institution of the United States']\n",
            "['Military Service Institution of the United States']\n",
            "['Maine volunteer infantry. 11th regiment']\n",
            "['United States. War Department']\n",
            " Civil and Military\"\n",
            " Civil and Military\"\n",
            "['George Brinton McClellan']\n",
            "\n",
            "['Elisha Hunt Rhodes']\n",
            "['Ralph Thomas Dudgeon']\n",
            "\"['Yan Sun'\n",
            "['John Jackman']\n",
            " Part IV\"\n",
            "\"['Thomas P. Olivo'\n",
            "['Robert L Hartwig']\n",
            "\n",
            "\n",
            "['Emmanuelle Savignac']\n",
            "['Michael J. Verive']\n",
            " Justice\n",
            "\n",
            "\n",
            "\n",
            "['Institute of Ecology']\n",
            "\n",
            "\n",
            "\"['Drackle Dorle'\n",
            "['Margaret Khalakdina']\n",
            "['Tim Ingold']\n",
            "['Paul E. Minnis']\n",
            "\"['Charles F. Keyes'\n",
            "['Judith Okely']\n",
            " the Nation State\n",
            "['Drackle Edgar']\n",
            "['Sarah Pink']\n",
            "['University of Hawaii at Manoa. Southeast Asian Studies Program']\n",
            "\n",
            "\n",
            " Phrases\n",
            "['United States. Dept. of the Army']\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "['Association of Collegiate Schools of Architecture']\n",
            "\"['Bert Coules'\n",
            "['Verner Crane']\n",
            "['United States. Congress. Senate. Committee on Interior and Insular Affairs. Subcommittee on Public Lands']\n",
            "['United States. Congress. Senate. Interior and Insular Affairs']\n",
            "['Iowa Archeological Society']\n",
            "\"['Edgar Galindo'\n",
            "['Arthur Frederic Loveday']\n",
            " 1670-1732\"\n",
            " first published ... by R. Percivale ... Now enlarged ... by J. Minsheu ... Hereunto ... is annexed an ample English Dictionarie\n",
            "\n",
            "\n",
            "\"['National Academy of Sciences'\n",
            "['United States. Congress. Senate']\n",
            "['United States. Congress. Senate']\n",
            "\n",
            "['United States. Congress. Senate']\n",
            "['Frances Parkinson Keyes']\n",
            "\"['Joseph M. Flora'\n",
            "['Frances Parkinson Keyes']\n",
            "['Frances Parkinson Keyes']\n",
            "['Frances Parkinson Keyes']\n",
            "['Don Thornton']\n",
            "['Holstein-Friesian Association of America']\n",
            "['Canada. Dept. of Agriculture. Production Service']\n",
            " Containing a Record of All Holstein-Friesian Cattle ...\"\n",
            "['Holstein-Friesian Association of Canada']\n",
            "\n",
            "['Cecil M. Robeck']\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "['Guy Hart-Davis']\n",
            "\n",
            "\n",
            "['Birgit Pfitzmann']\n",
            "\n",
            "['Air Force Human Resources Laboratory']\n",
            "['Stephan Riedl']\n",
            "['Deanna J. Sands']\n",
            "\n",
            "['Netta Weinstein']\n",
            "['Chicago (Ill.). Board of Education']\n",
            "['Richard Watson Hantke']\n",
            "['William Bernhardt']\n",
            "['Roger Keyes']\n",
            "\"['Alain Silver'\n",
            "['Allan Michael Hoffman']\n",
            "['Sir Edward Louis Spears (bart.)']\n",
            "['Sir Edward Spears']\n",
            "\n",
            "['John L. Myers']\n",
            "\n",
            "\n",
            "['Kansas Geological Survey']\n",
            "['Kansas Geological Survey']\n",
            "['Kansas Geological Survey']\n",
            "['Erasmus Haworth']\n",
            "['Kansas Geological Survey']\n",
            "\n",
            "\n",
            "['Rubin Jernudd']\n",
            "['Richard Watson Hantke']\n",
            "['David Konstan']\n",
            "['Lance Flavell']\n",
            "\"['Philip R. Bunker'\n",
            "\n",
            "\"['Akādīmīyah al-ʻArabīyah lil-ʻUlūm wa-al-Tiknūlūjiyā (Egypt)'\n",
            "['Michael Charles Keyes']\n",
            "\n",
            "\n",
            "['Arieh Iserles']\n",
            "['Roy Axford']\n",
            " Fifth Edition\"\n",
            "\"['Laruie Kennedy-Malone'\n",
            "['Mark Powell']\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "['Marian Keyes']\n",
            "['Dana Luciano']\n",
            "\"['Howard R. Winokuer\n",
            "['Fenton Keyes']\n",
            " Happiness and Well-Being\"\n",
            " November 1860-September 1861\"\n",
            "['Russell H. Beatie']\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "['Yasunari Kawabata']\n",
            "['Marian Keyes']\n",
            "\n",
            "['Siobhán Lankford']\n",
            "['United States. Congress. Senate. Committee on Agriculture and Forestry']\n",
            "\"['Katherine Brickell'\n",
            "['United States. Congress. Senate. Committee on Appropriations']\n",
            "['United States. Congress. Senate. Committee on Appropriations']\n",
            "\n",
            " Hearings Before ... 72-1\n",
            "\n",
            "\n",
            " for the Fiscal Year Ending ...\"\n",
            " Diliman\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "['California. Governor']\n",
            "['California. Governor']\n",
            "\n",
            "\n",
            "\n",
            "['Terry Puett']\n",
            "['International Monetary Fund']\n",
            "\n",
            "['ديفيد هاوكينز']\n",
            "['طالب عبد الرحمن']\n",
            "['Eliot Gregory']\n",
            "['الحريري']\n",
            "['Robert Stevenson']\n",
            "['Michael Pluess']\n",
            "['Eric C.R. Reeve']\n",
            "\"['John Kuspira'\n",
            "['John Thomas Patterson']\n",
            "['W. Noel Keyes']\n",
            "['Wendelin Van Draanen']\n",
            "\"['Donna K. Arnett\n",
            "\n",
            " Gender and the Body\"\n",
            "['Stephen S. Wachtel']\n",
            " 1933 and 1934\"\n",
            " 1925\"\n",
            "\n",
            "['Leo Lyon Zagami']\n",
            "['Donald Tyson']\n",
            "['Carlos E. Cortes']\n",
            "\"['S. Guy Endore'\n",
            "['United Fresh Fruit and Vegetable Association']\n",
            "\n",
            "['Anthony R. Walker']\n",
            "['Elizabeth Mayne']\n",
            "['William E. Cain']\n",
            "['Mary Keyes Burgess']\n",
            "\n",
            "['Jack Hyles']\n",
            "['Wendelin Van Draanen']\n",
            "['Sarah Rivett']\n",
            "['John Cotton']\n",
            "['Wendelin Van Draanen']\n",
            "\"['Jean Gibran'\n",
            "\"['David Fontana'\n",
            "['Don F. Selby']\n",
            "['Brooke Schedneck']\n",
            "\"['Sidney Keyes'\n",
            "\"['Gavin Frost'\n",
            "['Monica Lindberg Falk']\n",
            "['Stephan Bodian']\n",
            " written by the kings maiestie for the benefit of all his subjects\n",
            "['Alice Bailey']\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"['Neil Pearson'\n",
            "\n",
            "\n",
            "\n",
            "['David Coulter']\n",
            "\n",
            "\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "\n",
            "['Paul Shakespeare']\n",
            "['Alexis Van Hurkman']\n",
            "['Ron Brinkmann']\n",
            "\n",
            " Okla.\n",
            "['United States. National Labor Relations Board']\n",
            "\n",
            "\n",
            " Dealers and Garagemen\"\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "\n",
            "\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "\"['East India Association (London\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "['United States. Patent Office']\n",
            "\n",
            "\n",
            "\n",
            "['New York (State). Legislature. Assembly']\n",
            " Second District\"\n",
            "['New York (State). Public Service Commission. 2d District']\n",
            "['New York (State). Legislature. Senate']\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "['Trivion Books']\n",
            " volume 4: 2008Art Book News Annual\n",
            "['United States Information Agency']\n",
            "['Dennis Bloodworth']\n",
            "\n",
            "\n",
            "\n",
            "\"['Jeffrey S Arpan'\n",
            "\n",
            "['Juvenal Londoño Angel']\n",
            "\"['Makoto Nakajima'\n",
            "['Toni Johnson-Woods']\n",
            " Manga\n",
            "['Christopher Hart']\n",
            "['Fayette Alexander Jones']\n",
            "\n",
            "\n",
            " Reading Manga\"\n",
            "['Adam L. Kern']\n",
            "\n",
            " Global Media\"\n",
            "\n",
            "\"['Chip Heath'\n",
            "['C. Lynch']\n",
            "['Lindsay Clare Matsumura']\n",
            "\"['Chip Heath'\n",
            " 2007\"\n",
            "\n",
            "\n",
            "['Marian Keyes']\n",
            "['Melissa Ferguson']\n",
            "\"['Zigurd Mednieks'\n",
            "['Gokhan Kurt']\n",
            "['David F. Dufty']\n",
            "\"['W. Frank Ableson'\n",
            "['Robert A. Heinlein']\n",
            "['Jeremy Reed']\n",
            "['Jessica Keyes']\n",
            "['William F. Nolan']\n",
            "['Daniel Keyes']\n",
            "['David Pogue']\n",
            "\"['Jack Nutting'\n",
            "\"['Kim Topley'\n",
            "\"['Molly Maskrey'\n",
            "['Molly K. Maskrey']\n",
            "\"['David Mark'\n",
            "\"['David Mark'\n",
            "['Andy Ihnatko']\n",
            "\"['Scott Kelby'\n",
            "['Scott Wittenburg']\n",
            "['Rene Djurup']\n",
            "['René Djurup']\n",
            "['Steve Schwartz']\n",
            "['Steve Schwartz']\n",
            "\"['Eric Butow'\n",
            "['Guy Hart-Davis']\n",
            "['Mark Edward Soper']\n",
            "['Guy Hart-Davis']\n",
            "\"['Paul McFedries'\n",
            " Home Remedies\n",
            "\"['Dorothy L. Espelage'\n",
            "['Berkshire County (Mass.)']\n",
            "\"['Stanley Kunitz'\n",
            "\n",
            "['Anna Keyes']\n",
            "\"['Gary R. Burleson'\n",
            "\n",
            "['Maine. Legislature. Senate']\n",
            " American and Foreign\"\n",
            "\n",
            "\"['Nicole Rafter'\n",
            "['Daniel Veirs']\n",
            "['Janice D. Hamlet']\n",
            "['Elizabeth Leese']\n",
            "\n",
            "\n",
            "['Janet Clark']\n",
            "['Wendelin Van Draanen']\n",
            "\"['Bert Coules'\n",
            "['Greg Keyes']\n",
            " American Film Realist\"\n",
            "['Nick Smedley']\n",
            "['Janice D. Hamlet']\n",
            "['Jon Tuska']\n",
            "['Frank Northen Magill']\n",
            "['John Howard Reid']\n",
            "['R. Barton Palmer']\n",
            "['Kristi McKim']\n",
            "['John Howard Reid']\n",
            "\"['Dominique Mainon'\n",
            "['James W. Cortada']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Rolf Oppliger']\n",
            "\n",
            "['Liang-Jie Zhang']\n",
            "['Susan C. Awe']\n",
            "['Jessica Keyes']\n",
            "\n",
            "['Kyle Keyes']\n",
            "['Sadao Nakajima']\n",
            "\"['S Nakajima'\n",
            "['Kyle Keyes']\n",
            "['Alexandra Boldyreva']\n",
            "['Aspen Publishers']\n",
            " Keyed to Crandall and Whaley\"\n",
            "['Martin C. Gutzwiller']\n",
            "\"['Yehuda B. Band'\n",
            "\"['Pierre Grivet'\n",
            "\"['University of California\n",
            "['National Research Council (U.S.). Division of Chemistry and Chemical Technology']\n",
            " key to better living\"\n",
            "['National Science Foundation (U.S.)']\n",
            "\"['University of California\n",
            "\"['James Wei'\n",
            "\n",
            "['Stefaan J. R. Simons']\n",
            "['Blanche Harris Dalton']\n",
            "\n",
            "['United States. Bureau of Animal Industry']\n",
            "\"['Catriona Mortimer-Sandilands'\n",
            " Focused\n",
            "\n",
            "\n",
            "\n",
            " California\"\n",
            "\n",
            "\n",
            "\n",
            "['Mary Keyes']\n",
            "['Ted Fishman']\n",
            "['Adele Sarkissian']\n",
            "\"['Dennis La Beau'\n",
            "['Adele Sarkissian']\n",
            "['Methodist Episcopal Church. East Ohio Conference']\n",
            "\"['J. Fabian'\n",
            " Work\n",
            "\"['Julio Alvarez-Builla'\n",
            "\n",
            "['Colleen E. Kriger']\n",
            "['الفرد هيسيل/شعبات عبد العزيز خليفة/-']\n",
            "['Susan Wise Bauer']\n",
            "\"['كارول ج. بلاند'\n",
            "['Irfan Habib']\n",
            " al-munʻaqidah bi-Jāmiʻat Ḥalab min 5-12 Rabīʻ al-Thānī 1396 al-muwāfiq li 5-12 Nīsān (Abrīl) 1976\"\n",
            "\n",
            "\n",
            "['Irfan Habib']\n",
            "['Fuʼād Afrām Bustānī']\n",
            " 1500-1965\"\n",
            "\n",
            "['Kiri Paramore']\n",
            "['Robert Pruter']\n",
            "\n",
            "['Jay Riley Case']\n",
            "['James Jay Carafano']\n",
            "['Allan Michael Hoffman']\n",
            "['Elisha W 1828-1910 Keyes']\n",
            "['Sheldon Garon']\n",
            "['Ken Lunde']\n",
            "\n",
            "['Wendelin Van Draanen']\n",
            "['Adobe Systems']\n",
            "['Adobe Creative Team']\n",
            "\"['Adobe Creative Team'\n",
            "\"['Andrew Faulkner'\n",
            "['Adobe Systems']\n",
            "['Dan Sacharow']\n",
            "\"['Michael Dinowitz'\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "['Practising Law Institute']\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "['Wendelin Van Draanen']\n",
            "['John Cotton']\n",
            "['Derek Hart']\n",
            "['William White']\n",
            "['William L. Benoit']\n",
            "\n",
            "['William E. Cain']\n",
            "['Alfred Hulse Brooks']\n",
            "\n",
            "['Walter Romig']\n",
            "\"['Gale\n",
            "['William Cookson']\n",
            "\"['Ciara Ní Bhroin'\n",
            " linguistics and philology\"\n",
            "['Sylvia Hadjetian']\n",
            " 1929-1939\"\n",
            "\n",
            "\"['Russell M. Lawson'\n",
            " 1925\"\n",
            "\n",
            "['Alan Lee Keyes']\n",
            "\n",
            "['Charles Rollin Keyes']\n",
            "['David Evans']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n01Rlq6wee8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f836eb9d-f561-4244-98a5-c413a89b5cf7"
      },
      "source": [
        "for row in rows.take(10):\n",
        "  print(row[1])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "authors\n",
            "['Wendelin Van Draanen']\n",
            "\n",
            "\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvdfQzEXjDq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ca8eda-b90d-40cc-d823-712fabf45eb2"
      },
      "source": [
        "# filter() - Creating a new RDD by returning only the elements that satisfy the search filter.\n",
        "rows.filter(lambda line: \"Inward Journey\" in line).collect()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Inward Journey',\n",
              "  '',\n",
              "  'en',\n",
              "  \"['Medical']\",\n",
              "  '',\n",
              "  'NOT_MATURE',\n",
              "  'Open Court Publishing Company',\n",
              "  '1983-01',\n",
              "  '133']]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKguXKc1lAtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "210346ab-1819-49bb-97fe-21d8b83382d3"
      },
      "source": [
        "# groupByKey() The following groups all titles to their publisher. Operates on value pairs\n",
        "rows = book_names.map(lambda line: line.split(\",\"))\n",
        "titleToPublisher = rows.map(lambda n: (str(n[0]),str(n[6]) )).groupByKey()\n",
        "titleToPublisher.map(lambda x : {x[0]: list(x[1])}).take(5)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': ['publisher']},\n",
              " {'The Boston Directory ...': ['']},\n",
              " {\"The CIO's Guide to Oracle Products and Solutions\": ['CRC Press']},\n",
              " {'Implementing the IT Balanced Scorecard': ['CRC Press',\n",
              "   'CRC Press',\n",
              "   'CRC Press']},\n",
              " {'Social Software Engineering': ['CRC Press']}]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "# @title Select the False Statement: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"flatMap function always produces a single value as output for each input value\" #@param [\"\", \"Map transforms an RDD of length N into another RDD of length N\",\"flatMap function always produces a single value as output for each input value\",\"The reduce operation shuffles and reduces the output obtained from the map\"]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good, But Not Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"Perfect for practice\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73b3b54-be0e-408c-9755-1bfda8bc2c12"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 6281\n",
            "Date of submission:  06 Aug 2023\n",
            "Time of submission:  15:45:34\n",
            "View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}